<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Liang Xu">


    <meta name="subtitle" content="This is Liang's blog for life and work.">




<title>Partial least square regression | Liang Xu</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    


<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Liang Xu</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/research">Research</a>
                
                    <a class="menu-item" href="/publication">Publications</a>
                
                    <a class="menu-item" href="/packages">Packages</a>
                
                    <a class="menu-item" href="/archives">Blog</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Liang Xu</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/research">Research</a>
                
                    <a class="menu-item" href="/publication">Publications</a>
                
                    <a class="menu-item" href="/packages">Packages</a>
                
                    <a class="menu-item" href="/archives">Blog</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6; // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function() {
            tocbot.refresh(obj_merge(tocbot_default_config, {
                hasInnerContainers: true
            }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function() {
        tocbot.init(obj_merge(tocbot_default_config, {
            collapseDepth: 1
        }));
    });

    function expand_toc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, {
            collapseDepth: expanded ? 1 : DEPTH_MAX
        }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">Partial least square regression</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Liang Xu</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">October 25, 2018&nbsp;&nbsp;0:00:00</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/Research/">Research</a>
                            
                                <a href="/categories/Research/Numerical-method/">Numerical method</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <p>I found the following interpretation to partial least square regression is much better than mine. So I cited it as below:</p>
<blockquote>
<p>“Partial least squares regression (PLS regression) is a statistical method that bears some relation to principal components regression; instead of finding hyperplanes of maximum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the observable variables to a new space. Because both the X and Y data are projected to new spaces, the PLS family of methods are known as bilinear factor models. Partial least squares Discriminant Analysis (PLS-DA) is a variant used when the Y is categorical.”</p>
<p>—from <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Partial_least_squares_regression">Wikipedia</a></p>
</blockquote>
<span id="more"></span>

<h2 id="PLS-algorithm"><a href="#PLS-algorithm" class="headerlink" title="PLS algorithm"></a>PLS algorithm</h2><p>For record purpose, I am deriving the PLS algorithm in this post. </p>
<p>Consider a linear regression problem in a vector format as in Eq.[eq:1] given an observed data, which is denoted by a vector \(\boldsymbol{x}^{T}&#x3D;(x_{1},x_{2},\cdots,x_{n})\)</p>
<p>$$<br>\begin{align}<br>y	&amp;&#x3D;b_{1}x_{1}+\cdots+b_{n}x_{n}+e \\<br>    &amp;&#x3D;\boldsymbol{x}^{T}\boldsymbol{b}+e \tag{1}<br>\end{align}<br>$$</p>
<p>We are aiming to work out the unknown regression coefficients vector \(\boldsymbol{b}\) for future prediction of \(y\) once we have the observed data \(\boldsymbol{x}\). To keep the prediction as accurate as possible, we will want to make the residual error e as small as possible. This is simply the whole idea of the linear regression method. So one solution is to use training samples, that is given m observations \(\boldsymbol{y}^{T}&#x3D;(y_{1},\cdots,y_{m}),X&#x3D;[x_{ij}],i&#x3D;1,\cdots,m,j&#x3D;1,\cdots,n \)<br>to solve \(\boldsymbol{b}\) out in terms of keeping the residual error sufficiently small</p>
<p>$$<br>\begin{pmatrix}<br>y_{1}\\<br>y_{2}\\<br>\vdots\\<br>y_{m}<br>\end{pmatrix}&#x3D;\begin{pmatrix}<br>x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1n}\\<br>x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2n}\\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\<br>x_{m1} &amp; x_{m2} &amp; \cdots &amp; x_{mn}<br>\end{pmatrix}\begin{pmatrix}<br>b_{1}\\<br>b_{2}\\<br>\vdots\\<br>b_{n}<br>\end{pmatrix}+\begin{pmatrix}<br>e_{1}\\<br>e_{2}\\<br>\vdots\\<br>e_{m}<br>\end{pmatrix}\tag{2}<br>$$</p>
<p>For comparability among different data, all the observations are assumed to be centered and normalized in advance. Now the problem is translated to an optimization problem, that is to find a least square solution for the regression coefficients such that the sum of the square errors of the residuals is minimized. For simplicity, I would like to use the matrix form for Eq.[2]</p>
<p>$$<br>Y&#x3D;X\boldsymbol{b}+\boldsymbol{e}. \tag{3}<br>$$</p>
<p>We want to find out \(\boldsymbol{b}\) such that</p>
<p>$$<br>min\{\boldsymbol{e}^{T}\boldsymbol{e}\}.<br>$$</p>
<p>The least square solution for \(\boldsymbol{b}\) is given by</p>
<p>$$<br>\boldsymbol{b}&#x3D;(X^{T}X)^{-1}X^{T}Y. \tag{4}<br>$$</p>
<p>The derivation is simply as follows:</p>
<p>$$<br>F_{\boldsymbol{b}}&#x3D;\frac{1}{2}(X\boldsymbol{b}-Y)^{T}(X\boldsymbol{b}-Y) \tag{5}<br>$$</p>
<p>So the derivative of F_{\boldsymbol{b}} yields</p>
<p>$$<br>\begin{align}<br>\nabla_{\boldsymbol{b}}F_{\boldsymbol{b}}	<br>&amp;&#x3D;\frac{1}{2}\nabla_{\boldsymbol{b}}(X\boldsymbol{b}-Y)^{T}(X\boldsymbol{b}-Y)\\<br>    &amp;&#x3D;\frac{1}{2}\nabla_{\boldsymbol{b}}(\boldsymbol{b}^{T}X^{T}-Y^{T})(X\boldsymbol{b}-Y)\\<br>    &amp;&#x3D;\frac{1}{2}\nabla_{\boldsymbol{b}}(\boldsymbol{b}^{T}X^{T}X\boldsymbol{b}-\boldsymbol{b}^{T}X^{T}Y-Y^{T}X\boldsymbol{b}+Y^{T}Y)\\<br>    &amp;&#x3D;\frac{1}{2}\nabla_{\boldsymbol{b}}tr(\boldsymbol{b}^{T}X^{T}X\boldsymbol{b}-\boldsymbol{b}^{T}X^{T}Y-Y^{T}X\boldsymbol{b}+Y^{T}Y)\\<br>    &amp;&#x3D;\frac{1}{2}\nabla_{\boldsymbol{b}}\left(tr(\boldsymbol{b}^{T}X^{T}X\boldsymbol{b})-2tr(Y^{T}X\boldsymbol{b})+tr(Y^{T}Y)\right)\\<br>    &amp;&#x3D;X^{T}X\boldsymbol{b}-X^{T}Y.<br>\end{align}<br>$$</p>
<p>By setting the derivative to be 0, solution Eq.[4] is obtained.<br>However, this least square solution may have problem when the training sample is not enough, that is m&lt;n. In that case, the matrix \(X^{T}X\) doesn’t have full rank which means it is nonsingular. </p>
<p>To solve this problem, we can project each measurement \(\boldsymbol{x_{i}}&#x3D;(x_{i1},\cdots,x_{in})\) into a lower-dimensional subspace spanned by data \(T&#x3D;XW\). We can think of this as forming a smaller set of features, each being the linear combination of the original set of features. These new features are also called “latent” variables. Therefore, the linear regression can be written as a linear regression system on the new latent variables</p>
<p>$$<br>\begin{align}<br>Y &amp;&#x3D;TQ^{T}+F \tag{6} \\<br>X &amp;&#x3D;TP^{T}+E \tag{7}<br>\end{align}<br>$$</p>
<p>where \(P,Q\) are the coefficient matrices and E,F are matrices of errors. As in partial least squares regression (PLSR), the weight matrix \(W\) reflects the covariance structure between the predictor and response variables. Hence, maximizing the covariance of the latent variables and the response variables \(Cov(T,Y)\) gives us the weight matrix \(W\). Once obtaining \(W\) and then constructing \(T\), \(Q^{T}\) is solved by the least squares solution Eq.[6]:</p>
<p>$$<br>Q^{T}&#x3D;(T^{T}T)^{-1}T^{T}Y.<br>$$</p>
<p>Plug Eq.[eq:latent] into the regression equation Eq.[6], we obtain the solution of the matrix \(\boldsymbol{b}\) of the coefficient in the model Eq.[3]</p>
<p>$$<br>\boldsymbol{b}&#x3D;W(T^{T}T)^{-1}T^{T}Y.<br>$$</p>
<p>So this is the whole idea of PLS. A commonly used algorithm to compute PLSR is the nonlinear iterative partial least square (NIPALS) method. The steps are summarized as follows:</p>
<p><strong>NIPALS</strong></p>
<ol>
<li>Normalize \(m\) training samples by</li>
</ol>
<p>$$<br>\begin{align}<br>x_{ij} &amp;&#x3D;\left(x_{ij}-\bar{x} <em>{\cdot\ j}\right)&#x2F; \sigma <em>{x</em>{\cdot\ j}} \\<br>y</em>{i} &amp;&#x3D;\left(y_{i}-\bar{y}\right)&#x2F; \sigma _{y}\ \text{for }i&#x3D;1,\cdots,m,j&#x3D;1,\cdots,n<br>\end{align}<br>$$</p>
<p><strong>Compute the scores</strong></p>
<ol start="2">
<li>Compute the dominant eigenvector of \(X_{0}^{T}Y_{0}Y_{0}^{T}X_{0}\) and assign it to \(w_{1}\) (the step of maximizing the covariance). Normalize \(w_{1}\). The scores \(t_{1}\) of \(X_{0}\) yields</li>
</ol>
<p>$$<br>t_{1}&#x3D;X_{0}w_{1}.<br>$$</p>
<p>Do the same to \(Y_{0}\) and obtain a normalized vector \(c_{1}\). The scores \(v_{1}\) of \(Y_{0}\) yields</p>
<p>$$<br>v_{1}&#x3D;Y_{0}c_{1}.<br>$$</p>
<p><strong>Compute the loadings</strong></p>
<ol start="3">
<li>Based on the regression equations</li>
</ol>
<p>$$<br>\begin{align}<br>X	&amp;&#x3D;t_{1}p_{1}^{T}+E \\<br>Y	&amp;&#x3D;t_{1}q_{1}^{T}+F,<br>\end{align}<br>$$</p>
<p>the least square method gives us </p>
<p>$$<br>\begin{align}<br>p_{1} &amp;&#x3D;\frac{X^{T}t_{1}}{t_{1}t_{1}^{T}} \\<br>q_{1} &amp;&#x3D;\frac{Y^{T}t_{1}}{t_{1}t_{1}^{T}}.<br>\end{align}<br>$$</p>
<ol start="4">
<li><p>Check if \(F\) is sufficiently small. Otherwise, set \(E\) as \(X_{1}\), \(F\) as \(Y_{1}\). Repeat step 2-3. </p>
</li>
<li><p>Finally, we obtain</p>
</li>
</ol>
<p>$$<br>\begin{align}<br>X &amp;&#x3D;t_{1}p_{1}^{T}+t_{2}p_{2}^{T}+\cdots+t_{n}p_{n}^{T}+E \\<br>Y &amp;&#x3D;t_{1}q_{1}^{T}+t_{2}q_{2}^{T}+\cdots+t_{n}q_{n}^{T}+F<br>\end{align}<br>$$</p>
<p>which in a matrix format is as follows:</p>
<p>$$<br>\begin{align}<br>X &amp;&#x3D;TP^{T}+E \\<br>Y &amp;&#x3D;TQ^{T}+F \\<br>    &amp;&#x3D;XWQ^{T}+F \\<br>    &amp;:&#x3D;XB+F.<br>\end{align}<br>$$</p>
<p>So, when we have a new data \(x_{new}\), to define the PLS components in our ABC-MCMC algorithm, we only need to compute </p>
<p>$$<br>t_{1}&#x3D;x_{new}^{T}p_{1},t_{2}&#x3D;x_{new}^{T}p_{2},\cdots,t_{n}&#x3D;x_{new}^{T}p_{n}<br>$$</p>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Liang Xu</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://xl0418.github.io/2018/10/25/2018-10-25-PLSR/">http://xl0418.github.io/2018/10/25/2018-10-25-PLSR/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>Do you believe in <strong>DESTINY</strong>?</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/math/"># math</a>
                    
                        <a href="/tags/regression/"># regression</a>
                    
                        <a href="/tags/least-square-method/"># least square method</a>
                    
                        <a href="/tags/partial-least-square/"># partial least square</a>
                    
                        <a href="/tags/machine-learning/"># machine learning</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2018/10/29/2018-10-29-sed/">Sed:a fast way to extract information from data</a>
            
            
            <a class="next" rel="next" href="/2018/10/24/2018-10-24-derivingFPequ/">Deriving the Fokker-Planck equation from a stochastic differential equation</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Liang Xu | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>

</html>