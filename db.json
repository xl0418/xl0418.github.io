{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/about/Liang_CV.pdf","path":"about/Liang_CV.pdf","modified":0,"renderable":0},{"_id":"themes/next-reloaded/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/favicon.ico","path":"images/favicon.ico","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/favicon16.ico","path":"images/favicon16.ico","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/clipboard-use.js","path":"js/src/clipboard-use.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/custom.js","path":"js/src/custom.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/clipboard/clipboard-action.js","path":"lib/clipboard/clipboard-action.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/clipboard/clipboard.js","path":"lib/clipboard/clipboard.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/zclip/clipboard.min.js","path":"lib/zclip/clipboard.min.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1}],"Cache":[{"_id":"themes/next-reloaded/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1540213352000},{"_id":"themes/next-reloaded/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1540213352000},{"_id":"themes/next-reloaded/.eslintrc.json","hash":"cc5f297f0322672fe3f684f823bc4659e4a54c41","modified":1540213352000},{"_id":"themes/next-reloaded/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1540213352000},{"_id":"themes/next-reloaded/.gitignore","hash":"a18c2e83bb20991b899b58e6aeadcb87dd8aa16e","modified":1540213352000},{"_id":"themes/next-reloaded/.travis.yml","hash":"3d1dc928c4a97933e64379cfde749dedf62f252c","modified":1540213352000},{"_id":"themes/next-reloaded/LICENSE.md","hash":"fc7227c508af3351120181cbf2f9b99dc41f063e","modified":1540213352000},{"_id":"themes/next-reloaded/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1540213352000},{"_id":"themes/next-reloaded/bower.json","hash":"23379fec9b4f70bc2611433ac3770445a8ca18d9","modified":1540213352000},{"_id":"themes/next-reloaded/README.md","hash":"140f4ece6670327a7d33b293947d958de80b44da","modified":1540213352000},{"_id":"themes/next-reloaded/_config.yml","hash":"8bd892403a547ed11d49295c1108b87b1ca7b56d","modified":1540406282000},{"_id":"themes/next-reloaded/crowdin.yml","hash":"e026078448c77dcdd9ef50256bb6635a8f83dca6","modified":1540213352000},{"_id":"themes/next-reloaded/gulpfile.coffee","hash":"48d2f9fa88a4210308fc41cc7d3f6d53989f71b7","modified":1540213352000},{"_id":"themes/next-reloaded/package.json","hash":"42186cf60771f5547b3a68cf3865f102011930d0","modified":1540213352000},{"_id":"source/categories/index.md","hash":"51fef65140eadfca025b4892d2537e9c1985f40a","modified":1601897459304},{"_id":"source/about/index.md","hash":"82f0686deaf83ee7be2256c898d372b63e7bd5f5","modified":1601897459303},{"_id":"source/_posts/2018-10-23-Blogging tech.md","hash":"f686ef8371db90d94431e4fabb385be0e5f48321","modified":1601897458211},{"_id":"source/_posts/2018-10-22-IntrotoPro1.md","hash":"ceb4c4f7d012818e596eef5372f52d769a85c716","modified":1601897458200},{"_id":"source/_posts/2018-10-24-derivingFPequ.md","hash":"86917f76b4734523900205c4d1923a429c92baf6","modified":1601897458211},{"_id":"source/_posts/2018-10-24-phylo2Lfunction.md","hash":"a9b8ab6f0fe689d7c0da950d192d8b85b678ed94","modified":1601897458212},{"_id":"source/_posts/2018-10-24-pruneLfunction.md","hash":"d54ebae92e0778507b0ee31e7f7ec53ef488b95f","modified":1601897458212},{"_id":"source/_posts/2018-10-25-PLSR.md","hash":"802db24f14d3c8cd445febdf666f5c404d688171","modified":1601897458212},{"_id":"source/_posts/2018-10-29-sed.md","hash":"31e2e6b02bef4a26ad0b89dc9b6096635836cba9","modified":1601897458213},{"_id":"source/_posts/2018-11-26-Rupdateallpackages.md","hash":"d7cb2861a857849d76b4d8b7338ee36d85554ac1","modified":1601897458214},{"_id":"source/_posts/2018-11-29-color.md","hash":"b3c40ca9d109e00cb7f50c56be5c3f04c7ae79e0","modified":1601897458215},{"_id":"source/_posts/2018-11-30-SMCplots.md","hash":"ef01fcaa4bc901b28e0a6bd5a4cba4179a365ac8","modified":1601897458216},{"_id":"source/_posts/2018-12-07-ggradar2helpdocument.md","hash":"62825597d2df2c75a6ef4a64e244722fac7b9fc3","modified":1601897458220},{"_id":"source/_posts/2018-12-05-ggradar2.md","hash":"1c9980774018192d779b7a10cf4df51b610b1121","modified":1601897458216},{"_id":"source/_posts/2019-01-04-PyCUDAseries1.md","hash":"c6585a21319a8e95359e87b8682f3d92edf3a4a6","modified":1601897458221},{"_id":"source/_posts/2019-01-10sed2.md","hash":"a5b042112fe9a0906864f4d34c7ac01c5cbb89f1","modified":1601897458224},{"_id":"source/_posts/2019-01-14-PyCUDAseries2.md","hash":"791ecd892c703421deb9ae9b6df5dc0e9d6dda1f","modified":1601897458225},{"_id":"source/_posts/2019-01-21-PyCUDAseries3.md","hash":"aa4307b8e221a2b3462453dbb31dff30a94befdb","modified":1601897458233},{"_id":"source/_posts/2019-02-08-Machinelearningseries2.md","hash":"a2443c149a59c22cfc19decf5990689b5fda90a9","modified":1601897458242},{"_id":"source/_posts/2019-02-01-Machinelearningseries1.md","hash":"cedc25ff16385843af5545f31240460d1041a4a4","modified":1601897458238},{"_id":"source/_posts/2019-03-20-readdatadon.md","hash":"2df66734f6893b6f5d72c30b7b8df7712af295aa","modified":1601897458305},{"_id":"source/_posts/2019-06-21-ABCalgorithm.md","hash":"5d45859774ad22eb3c4fac4db03b97e873e84be6","modified":1601897458307},{"_id":"source/_posts/2020-03-09-GUI-traitevolution.md","hash":"1f344cb5c15a8453f592da381fb329321fbb7985","modified":1601897459185},{"_id":"source/_posts/2019-06-21-TPmodel.md","hash":"3d926fd87743da935628d51cd1910e9a57cbadca","modified":1601897458481},{"_id":"source/_posts/2020-03-18-generalABC.md","hash":"e184e026436774db16c65c1a55436c6f43559f0c","modified":1601897459189},{"_id":"source/_posts/2020-04-02-government&pandemic.md","hash":"f2048aa8144efc19c5069fdc0c801bd34c624d2f","modified":1601897459283},{"_id":"source/_posts/2020-04-25-datavisulization_shinyapp.md","hash":"348d1701a1a7b4f75895262da9616d083beb4756","modified":1601897459301},{"_id":"source/_posts/hello-world.md","hash":"36237693e04d6eee2637613c9f0b0cfde107bd31","modified":1601897459302},{"_id":"source/tags/index.md","hash":"8613d7db500f47c01e158aa88faae8037537933b","modified":1601897459304},{"_id":"themes/next-reloaded/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1540213352000},{"_id":"themes/next-reloaded/.git/config","hash":"e6396f4650bd14040df3e97a6e598b61349b261e","modified":1540213352000},{"_id":"themes/next-reloaded/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1540213350000},{"_id":"themes/next-reloaded/.git/index","hash":"cf4703c9a0c9661796248b722368c5c4bbf772cb","modified":1540213352000},{"_id":"themes/next-reloaded/.git/packed-refs","hash":"77659971ba079b5708a11649cc0fc041be706653","modified":1540213352000},{"_id":"themes/next-reloaded/.github/CODE_OF_CONDUCT.md","hash":"b63696d41f022525e40d7e7870c3785b6bc7536b","modified":1540213352000},{"_id":"themes/next-reloaded/.github/CONTRIBUTING.md","hash":"a5335a99377069ae76fd993d488bc3eaf48f3a05","modified":1540213352000},{"_id":"themes/next-reloaded/.github/ISSUE_TEMPLATE.md","hash":"00c25366764e6b9ccb40b877c60dc13b2916bbf7","modified":1540213352000},{"_id":"themes/next-reloaded/.github/PULL_REQUEST_TEMPLATE.md","hash":"7abbb4c8a29b2c14e576a00f53dbc0b4f5669c13","modified":1540213352000},{"_id":"themes/next-reloaded/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1540213352000},{"_id":"themes/next-reloaded/.github/stale.yml","hash":"fd0856f6745db8bd0228079ccb92a662830cc4fb","modified":1540213352000},{"_id":"themes/next-reloaded/docs/ALGOLIA-SEARCH.md","hash":"141e989844d0b5ae2e09fb162a280715afb39b0d","modified":1540213352000},{"_id":"themes/next-reloaded/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1540213352000},{"_id":"themes/next-reloaded/docs/AUTHORS.md","hash":"7b24be2891167bdedb9284a682c2344ec63e50b5","modified":1540213352000},{"_id":"themes/next-reloaded/docs/DATA-FILES.md","hash":"8e1962dd3e1b700169b3ae5bba43992f100651ce","modified":1540213352000},{"_id":"themes/next-reloaded/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"120750c03ec30ccaa470b113bbe39f3d423c67f0","modified":1540213352000},{"_id":"themes/next-reloaded/docs/INSTALLATION.md","hash":"2bbdd6c1751b2b42ce9b9335da420c6026a483e9","modified":1540213352000},{"_id":"themes/next-reloaded/docs/LICENSE","hash":"fe607fe22fc9308f6434b892a7f2d2c5514b8f0d","modified":1540213352000},{"_id":"themes/next-reloaded/docs/UPDATE-FROM-5.1.X.md","hash":"ad57c168d12ba01cf144a1ea0627b2ffd1847d3e","modified":1540213352000},{"_id":"themes/next-reloaded/docs/MATH.md","hash":"e6023505dcccaef0b856102543585a13fc6af0b1","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_layout.swig","hash":"4bc10f6dd0217edca00d677eea4827b0e3f09290","modified":1540542688000},{"_id":"themes/next-reloaded/layout/archive.swig","hash":"2b6450c6b6d2bcbcd123ad9f59922a5e323d77a5","modified":1540213352000},{"_id":"themes/next-reloaded/layout/category.swig","hash":"5d955284a42f802a48560b4452c80906a5d1da02","modified":1540213352000},{"_id":"themes/next-reloaded/layout/index.swig","hash":"c2a3896c64e96790edc10426ef586b6186a87f46","modified":1540213352000},{"_id":"themes/next-reloaded/layout/schedule.swig","hash":"3e9cba5313bf3b98a38ccb6ef78b56ffa11d66ee","modified":1540213352000},{"_id":"themes/next-reloaded/layout/post.swig","hash":"318249db246a57e9422875a2457c6acfce974ba5","modified":1540213352000},{"_id":"themes/next-reloaded/layout/tag.swig","hash":"ba402ce8fd55e80b240e019e8d8c48949b194373","modified":1540213352000},{"_id":"themes/next-reloaded/layout/page.swig","hash":"ff06fb87b51c286799fb58443373a486bdb0bd0d","modified":1540302256000},{"_id":"themes/next-reloaded/languages/default.yml","hash":"c540c3a0d7db2d4239293c8783881962640b6c34","modified":1540213352000},{"_id":"themes/next-reloaded/languages/en.yml","hash":"c540c3a0d7db2d4239293c8783881962640b6c34","modified":1540213352000},{"_id":"themes/next-reloaded/languages/fr.yml","hash":"0162a85ae4175e66882a9ead1249fedb89200467","modified":1540213352000},{"_id":"themes/next-reloaded/languages/it.yml","hash":"62ef41d0a9a3816939cb4d93a524e6930ab9c517","modified":1540213352000},{"_id":"themes/next-reloaded/languages/de.yml","hash":"fb478c5040a4e58a4c1ad5fb52a91e5983d65a3a","modified":1540213352000},{"_id":"themes/next-reloaded/languages/id.yml","hash":"e7fb582e117a0785036dcdbb853a6551263d6aa6","modified":1540213352000},{"_id":"themes/next-reloaded/languages/nl.yml","hash":"bb9ce8adfa5ee94bc6b5fac6ad24ba4605d180d3","modified":1540213352000},{"_id":"themes/next-reloaded/languages/ko.yml","hash":"fae155018ae0efdf68669b2c7dd3f959c2e45cc9","modified":1540213352000},{"_id":"themes/next-reloaded/languages/ja.yml","hash":"e331b15b1fda0f2285d25853f834682ab8dc3c39","modified":1540213352000},{"_id":"themes/next-reloaded/languages/pt-BR.yml","hash":"bfc80c8a363fa2e8dde38ea2bc85cd19e15ab653","modified":1540213352000},{"_id":"themes/next-reloaded/languages/pt.yml","hash":"3cb51937d13ff12fcce747f972ccb664840a9ef3","modified":1540213352000},{"_id":"themes/next-reloaded/languages/tr.yml","hash":"c5f0c20743b1dd52ccb256050b1397d023e6bcd9","modified":1540213352000},{"_id":"themes/next-reloaded/languages/ru.yml","hash":"db0644e738d2306ac38567aa183ca3e859a3980f","modified":1540213352000},{"_id":"themes/next-reloaded/languages/zh-CN.yml","hash":"fbbf3a0b664ae8e927c700b0a813692b94345156","modified":1540213352000},{"_id":"themes/next-reloaded/languages/vi.yml","hash":"8da921dd8335dd676efce31bf75fdd4af7ce6448","modified":1540213352000},{"_id":"themes/next-reloaded/languages/zh-HK.yml","hash":"7903b96912c605e630fb695534012501b2fad805","modified":1540213352000},{"_id":"themes/next-reloaded/languages/zh-TW.yml","hash":"6e6d2cd8f4244cb1b349b94904cb4770935acefd","modified":1540213352000},{"_id":"themes/next-reloaded/scripts/helpers.js","hash":"a70bfad3efda76738dab12e28e8b75e3989ee3da","modified":1540213352000},{"_id":"themes/next-reloaded/scripts/merge-configs.js","hash":"33afe97284d34542015d358a720823feeebef120","modified":1540213352000},{"_id":"themes/next-reloaded/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1540213352000},{"_id":"themes/next-reloaded/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1540213352000},{"_id":"themes/next-reloaded/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1540213352000},{"_id":"themes/next-reloaded/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1540213352000},{"_id":"source/about/Liang_CV.pdf","hash":"3e1a7c3b61f72b6ce450db175b0d9f7dae0653d8","modified":1601897459303},{"_id":"themes/next-reloaded/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1540213352000},{"_id":"source/_posts/2018-10-29-sed/d.png","hash":"6bdc800119bc8de7461548b9d94b5f776c43a345","modified":1601897458213},{"_id":"source/_posts/2018-10-29-sed/ds.png","hash":"965d5418e7f621023a969770a36e2702a117ed90","modified":1601897458213},{"_id":"source/_posts/2018-10-29-sed/rawdata.png","hash":"504d3898506b182f7ffb5b6cb19618dde1154b0e","modified":1601897458214},{"_id":"source/_posts/2018-11-29-color/col1.png","hash":"cbf10523cc8e07e62bef59195000b0f541cd581f","modified":1601897458215},{"_id":"source/_posts/2018-11-29-color/col2.png","hash":"3dd942d4972d112beb80a25fc3aef5c905a92b54","modified":1601897458215},{"_id":"source/_posts/2018-11-29-color/col3.png","hash":"1d83794811a02b4bdadab17f78c8275ad87f131f","modified":1601897458215},{"_id":"source/_posts/2018-11-30-SMCplots/Rplot1.png","hash":"7b3ed1bb1002079cb8719e12f7d0e01a4098ae75","modified":1601897458216},{"_id":"source/_posts/2018-11-29-color/col4.png","hash":"8e015d04d4090c8debb5eed1964595de0f368a5b","modified":1601897458215},{"_id":"source/_posts/2018-11-30-SMCplots/Rplot2.png","hash":"a4ace9d48b797039ce52da319ebf313b339b2e1e","modified":1601897458216},{"_id":"source/_posts/2018-12-05-ggradar2/fullscore.png","hash":"5c50cfa94f69d90edfb9561abf66f229c83eee50","modified":1601897458217},{"_id":"source/_posts/2018-12-05-ggradar2/lux.png","hash":"9ddc42a286d090bbe82e25d58cbd92c1d7347079","modified":1601897458217},{"_id":"source/_posts/2018-12-05-ggradar2/mini.png","hash":"18b7ec1f51059e5dd8e748c53d68a806daec38ea","modified":1601897458218},{"_id":"source/_posts/2018-12-05-ggradar2/multipleplots.png","hash":"2c0d8c8213f7775d680657810a17ecb3e48ed38d","modified":1601897458218},{"_id":"source/_posts/2018-12-05-ggradar2/nolegend.png","hash":"338d96bd8f0ccd7d26c8dd4e341b39362c64a04e","modified":1601897458218},{"_id":"source/_posts/2018-12-05-ggradar2/multipleplotsbig.png","hash":"739c484223aa914606da7a43e93446af8e163388","modified":1601897458218},{"_id":"source/_posts/2018-12-05-ggradar2/roundnofill.png","hash":"31bfb3f19b550723abcc32b6b7177982128c8830","modified":1601897458219},{"_id":"source/_posts/2018-12-05-ggradar2/roundfill.png","hash":"7a6539ae5da9c90b8a62b37eed031cff190d7c9c","modified":1601897458219},{"_id":"source/_posts/2018-12-05-ggradar2/straightfill.png","hash":"f0f6bfe7a10e8856cd45cff454aad0655692b18f","modified":1601897458219},{"_id":"source/_posts/2018-12-05-ggradar2/straightnofill.png","hash":"efa92f19754af10d469cf6675eb7a44e08e70eda","modified":1601897458219},{"_id":"source/_posts/2018-12-05-ggradar2/trend.png","hash":"580a0ab7b4449330b9425f722f196fffb82c9134","modified":1601897458219},{"_id":"source/_posts/2019-01-10sed2/mfiles.png","hash":"45a7ad5fb9fe1ab18105f988b18272269c0e549b","modified":1601897458224},{"_id":"source/_posts/2019-01-14-PyCUDAseries2/speedtest.png","hash":"c48054dc2ef38a8857227c55428dc68e185f841d","modified":1601897458227},{"_id":"source/_posts/2019-02-01-Machinelearningseries1/modelselection1.png","hash":"c119feddb57b38b5d364ce35082001d7d62dbd74","modified":1601897458238},{"_id":"source/_posts/2019-02-01-Machinelearningseries1/prediction_sample.png","hash":"cd4ff39cfafd41d0711a4f11a77df2c0277ce7f9","modified":1601897458240},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/1.png","hash":"08f6c16b529bc03ecf83c55de5d28d2ec737aad7","modified":1601897458242},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/10.png","hash":"b2656495f031e2eefcc8734ee589b1ab555f1c6e","modified":1601897458243},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/11.png","hash":"a92bd5f59fee6cf4c18e4c7778bb53c8853c9b27","modified":1601897458243},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/12.png","hash":"6b4b7c01e0a107e349be33d327ff4a828de3ad41","modified":1601897458243},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/13-15.png","hash":"4e6540b7dbbc1cecadc83b89205c4e249207f2d8","modified":1601897458244},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/17.png","hash":"58c3578da213026a6faf5236cef70d0de0df18e7","modified":1601897458246},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/16.png","hash":"a79170ae6a15e5fb2342f6c373121e17d76eed33","modified":1601897458244},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/19-1.png","hash":"9a35d6438ea32257c73165fa5764e7f1555390ad","modified":1601897458247},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/18.png","hash":"fe7df21d74b9ac39988417271f71df0da3e14d46","modified":1601897458247},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/19.png","hash":"747d05b0ea5814c72e57861034a59c4a63211a60","modified":1601897458247},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/2.png","hash":"2ad49fbdeb4f563edd0797d9a4d972a8839c9d55","modified":1601897458248},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/20.png","hash":"85329614886a4b196eb845f13bb3a0fffabb1489","modified":1601897458248},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/21.png","hash":"94ce999c4c85fa14d85ae219eb00f9fbc6c94c36","modified":1601897458249},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/22.png","hash":"07b7bd1f8775d6e41253f8a4b0ae8e696bbc6502","modified":1601897458249},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/23.png","hash":"839ecc43535c8c589a64aa7cc947ec88441f871a","modified":1601897458249},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/25.png","hash":"cb29499e4d9341922f9d1e234068966d0b28fb73","modified":1601897458250},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/24.png","hash":"81ae1d4ec941ea54cf9457840d97557d0852b2e5","modified":1601897458249},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/3.png","hash":"82ca237383cd0b4c6c69991218d5243fe95411dd","modified":1601897458250},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/4.png","hash":"f53c3a372ae9ae0a6e992e176ef35ccf8fd1160a","modified":1601897458252},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/5.png","hash":"4445bfdd3d0690432257608a790062f4d4ecfccf","modified":1601897458252},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/78.png","hash":"1a55ed4d935acf4d99d095bd183364b02bb7182b","modified":1601897458252},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/6.png","hash":"d28b9b8aed0df282c8921d57715c719b924e3a37","modified":1601897458252},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/9-1.png","hash":"dd5ccd15eab9ebd82ffb81322a9b543f28326dff","modified":1601897458253},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/9.png","hash":"21858569c261d689790e470dda6647388bbf6828","modified":1601897458253},{"_id":"source/_posts/2019-03-20-readdatadon/d.png","hash":"6bdc800119bc8de7461548b9d94b5f776c43a345","modified":1601897458305},{"_id":"source/_posts/2019-06-21-ABCalgorithm/Rplot2.png","hash":"a4ace9d48b797039ce52da319ebf313b339b2e1e","modified":1601897458463},{"_id":"source/_posts/2019-03-20-readdatadon/rewrittendata.png","hash":"52a13bcda911b3879916779e798250de59cd01ba","modified":1601897458306},{"_id":"source/_posts/2020-03-09-GUI-traitevolution/p1.png","hash":"51eee44eff5793b37c8e4206438a97cff6b47603","modified":1601897459186},{"_id":"source/_posts/2020-03-09-GUI-traitevolution/p2.png","hash":"74d01c3c8e6cb746289469f2602c303afa15857a","modified":1601897459186},{"_id":"source/_posts/2020-03-09-GUI-traitevolution/p3.png","hash":"2ce44cf24deb5e9d34abd78b7c93eea8e43cfc72","modified":1601897459186},{"_id":"source/_posts/2020-03-09-GUI-traitevolution/p4.png","hash":"c8d1a197ba7983f534e5f7818b3063c086c85eed","modified":1601897459188},{"_id":"source/_posts/2020-03-18-generalABC/Predict1.png","hash":"8fbdb67a9331ad9f7d467a4a486d039dd2f5e016","modified":1601897459190},{"_id":"source/_posts/2020-03-18-generalABC/Predict2.png","hash":"ac88d57a8405d71db8a98580cb7951bd55dcfb29","modified":1601897459190},{"_id":"source/_posts/2020-03-18-generalABC/data.png","hash":"2f1482534ce247a1e5ece239b01f6f5690d0b5cf","modified":1601897459191},{"_id":"themes/next-reloaded/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1540213350000},{"_id":"themes/next-reloaded/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1540213350000},{"_id":"themes/next-reloaded/.git/hooks/fsmonitor-watchman.sample","hash":"f7c0aa40cb0d620ff0bca3efe3521ec79e5d7156","modified":1540213350000},{"_id":"themes/next-reloaded/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1540213350000},{"_id":"themes/next-reloaded/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1540213350000},{"_id":"themes/next-reloaded/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1540213350000},{"_id":"themes/next-reloaded/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1540213350000},{"_id":"themes/next-reloaded/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1540213350000},{"_id":"themes/next-reloaded/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1540213350000},{"_id":"themes/next-reloaded/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1540213350000},{"_id":"themes/next-reloaded/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1540213350000},{"_id":"themes/next-reloaded/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1540213350000},{"_id":"themes/next-reloaded/.git/logs/HEAD","hash":"f3dadd37a03ed8e677a43f4e117c013bcacf2e53","modified":1540213352000},{"_id":"themes/next-reloaded/docs/ru/DATA-FILES.md","hash":"d6d20f60f77a76c77f8e65d0c9adbd79d0274557","modified":1540213352000},{"_id":"themes/next-reloaded/docs/ru/UPDATE-FROM-5.1.X.md","hash":"b1dd18d9b890b21718883ea1832e7e02a773104a","modified":1540213352000},{"_id":"themes/next-reloaded/docs/ru/README.md","hash":"c54e256ed11a84ee38f755d6f35a3e6e29a91dbc","modified":1540213352000},{"_id":"themes/next-reloaded/docs/ru/INSTALLATION.md","hash":"6c5d69e94961c793da156217ecf1179e868d7ba1","modified":1540213352000},{"_id":"themes/next-reloaded/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"6855402e2ef59aae307e8bd2a990647d3a605eb8","modified":1540213352000},{"_id":"themes/next-reloaded/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"a45a791b49954331390d548ac34169d573ea5922","modified":1540213352000},{"_id":"themes/next-reloaded/docs/zh-CN/CONTRIBUTING.md","hash":"bd2c955d9b7b1b45bd74a4536717d547e03fcde3","modified":1540213352000},{"_id":"themes/next-reloaded/docs/zh-CN/INSTALLATION.md","hash":"b19a6e0ae96eb7c756fb5b1ba03934c7f9cbb3c3","modified":1540213352000},{"_id":"themes/next-reloaded/docs/zh-CN/DATA-FILES.md","hash":"f3eec572a7d83542e2710a7404082014aaa1a5e7","modified":1540213352000},{"_id":"themes/next-reloaded/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"24cf2618d164440b047bb9396263de83bee5b993","modified":1540213352000},{"_id":"themes/next-reloaded/docs/zh-CN/README.md","hash":"aa6808f4f587c1a97205fa9427ba96a366bcb288","modified":1540213352000},{"_id":"themes/next-reloaded/docs/zh-CN/MATH.md","hash":"8ac2f5d2a023211d8d8ea626cbf6b8dea67ac201","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_custom/custom.swig","hash":"de90263747f0aad53afd13ae91507d31fb181752","modified":1540827150000},{"_id":"themes/next-reloaded/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"5da70d7fa0c988a66a469b9795d33d471a4a4433","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_custom/head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_macro/passage-end-tag.swig","hash":"031db18e316c587c31ce651900bb75d6c18932c8","modified":1540306914000},{"_id":"themes/next-reloaded/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_macro/post-copyright.swig","hash":"0790ddbc349508d7ece45a9a4391d0a1cd7263cc","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_macro/post-related.swig","hash":"08fe30ce8909b920540231e36c97e28cfbce62b6","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_macro/post.swig","hash":"d7466a1236759bef56bcf82d4d8b5a35e9b79c98","modified":1587218420000},{"_id":"themes/next-reloaded/layout/_macro/reward.swig","hash":"bd5778d509c51f4b1d8da3a2bc35462929f08c75","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_macro/sidebar.swig","hash":"8ad419441656055d2653cfa45c91357f0cccf671","modified":1540300174000},{"_id":"themes/next-reloaded/layout/_macro/wechat-subscriber.swig","hash":"a9e1346b83cf99e06bed59a53fc069279751e52a","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_partials/breadcrumb.swig","hash":"6994d891e064f10607bce23f6e2997db7994010e","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_partials/comments.swig","hash":"89cd8f05425b0b452bc555a17aa89ba57a0f1300","modified":1540294088000},{"_id":"themes/next-reloaded/layout/_partials/footer.swig","hash":"bac30f1a30e536ef61136adf9a3c1d5352264103","modified":1540287668000},{"_id":"themes/next-reloaded/layout/_partials/page-header.swig","hash":"1aaf32bed57b976c4c1913fd801be34d4838cc72","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_partials/pagination.swig","hash":"dbe321bcf3cf45917cc11a3e3f50d8572bac2c70","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_scripts/boostrap.swig","hash":"0a0129e926c27fffc6e7ef87fe370016bc7a4564","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_scripts/noscript.swig","hash":"ac3ad2c0eccdf16edaa48816d111aaf51200a54b","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_scripts/commons.swig","hash":"6fc63d5da49cb6157b8792f39c7305b55a0d1593","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_scripts/vendors.swig","hash":"e0bdc723d1dc858b41fd66e44e2786e6519f259f","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/bookmark.swig","hash":"60001c8e08b21bf3a7afaf029839e1455340e95d","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/copy-code.swig","hash":"a8ab2035654dd06d94faf11a35750529e922d719","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/exturl.swig","hash":"f532ce257fca6108e84b8f35329c53f272c2ce84","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/github-banner.swig","hash":"cabd9640dc3027a0b3ac06f5ebce777e50754065","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/needsharebutton.swig","hash":"927f19160ae14e7030df306fc7114ba777476282","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/pangu.swig","hash":"6b75c5fd76ae7cf0a7b04024510bd5221607eab3","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/scroll-cookie.swig","hash":"b0ca46e0d1ff4c08cb0a3a8c1994f20d0260cef9","modified":1540213352000},{"_id":"themes/next-reloaded/scripts/tags/button.js","hash":"4b12c376bea894d23cca0f9fcb3d6518b6db279d","modified":1540213352000},{"_id":"themes/next-reloaded/scripts/tags/center-quote.js","hash":"f13430d9d1c9773b390787c2f046bb1f12a79878","modified":1540213352000},{"_id":"themes/next-reloaded/scripts/tags/exturl.js","hash":"1412ce2ef59fa4137b697a507fd759ff067a2398","modified":1540213352000},{"_id":"themes/next-reloaded/scripts/tags/full-image.js","hash":"e282bf5a7c70b3d354001e8f66d3bef1a4fbb79e","modified":1540213352000},{"_id":"themes/next-reloaded/scripts/tags/group-pictures.js","hash":"981e01aaf45a1f0f23ce0796d03134f9e437aaca","modified":1540213352000},{"_id":"themes/next-reloaded/scripts/tags/include-raw.js","hash":"5db59d56f4f4082382bf1c16722e6c383892b0c5","modified":1540213352000},{"_id":"themes/next-reloaded/scripts/tags/label.js","hash":"f0ecd3b5773b19a6bd93a819dfe0c49ee418e4de","modified":1540213352000},{"_id":"themes/next-reloaded/scripts/tags/note.js","hash":"adb945ba93ac487d46b969ca4e59d3681b8f8d1c","modified":1540213352000},{"_id":"themes/next-reloaded/scripts/tags/tabs.js","hash":"e37761253d68a29593fe9ed2fe403f49b6e971de","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/main.styl","hash":"c26ca6e7b5bd910b9046d6722c8e00be672890e0","modified":1540213352000},{"_id":"themes/next-reloaded/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1540213352000},{"_id":"themes/next-reloaded/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1540213352000},{"_id":"themes/next-reloaded/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1540213352000},{"_id":"themes/next-reloaded/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1540213352000},{"_id":"themes/next-reloaded/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1540213352000},{"_id":"themes/next-reloaded/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1540213352000},{"_id":"themes/next-reloaded/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1540213352000},{"_id":"themes/next-reloaded/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1540213352000},{"_id":"themes/next-reloaded/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1540213352000},{"_id":"themes/next-reloaded/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1540213352000},{"_id":"themes/next-reloaded/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1540213352000},{"_id":"themes/next-reloaded/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1540213352000},{"_id":"themes/next-reloaded/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1540213352000},{"_id":"themes/next-reloaded/source/images/favicon.ico","hash":"dbad95a76f53c8edcf8c452d2ee9bd27fbfc74ad","modified":1540292530000},{"_id":"themes/next-reloaded/source/images/favicon16.ico","hash":"01a6788c4ccc31027fa9f836b9c1573d25bd7472","modified":1540292536000},{"_id":"themes/next-reloaded/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1540213352000},{"_id":"themes/next-reloaded/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1540213352000},{"_id":"themes/next-reloaded/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1540213352000},{"_id":"themes/next-reloaded/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1540213352000},{"_id":"themes/next-reloaded/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1540213352000},{"_id":"source/_posts/2018-10-22-IntrotoPro1/Powertable.jpg","hash":"08373cf6112778618fd1a22a51e735a27a49c679","modified":1601897458206},{"_id":"source/_posts/2019-01-04-PyCUDAseries1/win10SDK.png","hash":"255648cb736a724d3623bf51d04bbdce05778edd","modified":1601897458223},{"_id":"source/_posts/2019-01-10sed2/rdatafiles.png","hash":"7c87f5b7820eb89deaf7a4250e942ef94b1bb53b","modified":1601897458225},{"_id":"source/_posts/2019-01-14-PyCUDAseries2/gridblockthread.png","hash":"31ad63d0cf3f0411ae70f29a71261f3e6642020e","modified":1601897458226},{"_id":"source/_posts/2020-03-18-generalABC/abc_scheme.png","hash":"462844b99697e9d935b97ec985a78007fa3da4f8","modified":1601897459191},{"_id":"themes/next-reloaded/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1540213352000},{"_id":"source/_posts/2019-01-04-PyCUDAseries1/vsinstaller.png","hash":"00caf651a329fb3185dc12e1a67b0eabf91875be","modified":1601897458222},{"_id":"themes/next-reloaded/.git/refs/heads/master","hash":"c2f3a9849a5ad71b785665beb36649cb97bde2c1","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_macro/menu/menu-badge.swig","hash":"65c5e585982dae7ae1542cada71858b4ea1f73d6","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_macro/menu/menu-item.swig","hash":"d1b73c926109145e52605929b75914cc8b60fb89","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_partials/head/head-unique.swig","hash":"a7e376b087ae77f2e2a61ba6af81cde5af693174","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_partials/head/head.swig","hash":"00bf33b3c557b8f7e9faf49b226ea6ff7df5cda0","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_partials/header/brand.swig","hash":"fd780171713aada5eb4f4ffed8e714617c8ae6be","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_partials/header/index.swig","hash":"2082f5077551123e695e8afec471c9c44b436acb","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_partials/header/menu.swig","hash":"3db735d0cd2d449edf2674310ac1e7c0043cb357","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_partials/header/sub-menu.swig","hash":"88b4b6051592d26bff59788acb76346ce4e398c2","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_partials/search/index.swig","hash":"a33b29ccbdc2248aedff23b04e0627f435824406","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_scripts/pages/post-details.swig","hash":"cc865af4a3cb6d25a0be171b7fc919ade306bb50","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_scripts/schemes/gemini.swig","hash":"ea03fe9c98ddcfcc0ecfdbe5a2b622f9cde3b3a1","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_scripts/schemes/pisces.swig","hash":"ea03fe9c98ddcfcc0ecfdbe5a2b622f9cde3b3a1","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/busuanzi-counter.swig","hash":"07307f1f0e0e9858f2c7143cbdfcb2a9a92149ab","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/firestore.swig","hash":"fae69a0e1a1d42f7bb44e594a29857d94594698b","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/google-analytics.swig","hash":"beb53371c035b62e1a2c7bb76c63afbb595fe6e5","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/lean-analytics.swig","hash":"c28f3f4aa31d7f996d26a97df6cd7ffa9bfd2cec","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/comments/disqus.swig","hash":"03ef008bc95e8e83232e5464a6c63d6157d33a5e","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/comments/gitment.swig","hash":"fe8177e4698df764e470354b6acde8292a3515e0","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/comments/gitalk.swig","hash":"a7aaa4c2729df8d0e6c4e37d544047c5b48dee6b","modified":1540294032000},{"_id":"themes/next-reloaded/layout/_third-party/comments/index.swig","hash":"9809d63a4234c5f268ff0b489579149b849728fc","modified":1540294114000},{"_id":"themes/next-reloaded/layout/_third-party/comments/livere.swig","hash":"2c74a96dd314e804d801f8773ac1b2e0a970fce3","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/comments/valine.swig","hash":"34421679cae6581697cd3ab7c3729eb220e3e3f5","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/math/index.swig","hash":"a6fc00ec7f5642aabd66aa1cf51c6acc5b10e012","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/math/mathjax.swig","hash":"9b9ff4cc6d5474ab03f09835a2be80e0dba9fe89","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/math/katex.swig","hash":"97dbc2035bcb5aa7eafb80a4202dc827cce34983","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/search/localsearch.swig","hash":"b15e10abe85b4270860a56c970b559baa258b2a8","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1540542352000},{"_id":"themes/next-reloaded/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_variables/Gemini.styl","hash":"e1f6f59ad6e562dfe640ee4ed5d1ac9b6aba4114","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_mixins/Pisces.styl","hash":"2640a54fa63bdd4c547eab7ce2fc1192cf0ccec8","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_mixins/base.styl","hash":"59961fb806a39c367fd19ad37268eee112be6729","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_variables/Pisces.styl","hash":"c167eeb6b736f7b021fba98c38c2c21032ee1255","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_variables/base.styl","hash":"f9b83d0385529e52ce7ba95ed5ed6b3d4e2419bb","modified":1540213352000},{"_id":"themes/next-reloaded/source/js/src/affix.js","hash":"a2aab233d99297435a5274bf512c3c753fe08e80","modified":1540213352000},{"_id":"themes/next-reloaded/source/js/src/algolia-search.js","hash":"1f7f10c579e7703d0f6acb8b73f3d78a07d0c623","modified":1540213352000},{"_id":"themes/next-reloaded/source/js/src/bootstrap.js","hash":"1c41508b83cb0c4512e64b4d63afa1be954ce8ef","modified":1540213352000},{"_id":"themes/next-reloaded/source/js/src/clipboard-use.js","hash":"58121dfdf82b66277454872635567d25ff8b1bca","modified":1540480704000},{"_id":"themes/next-reloaded/source/js/src/custom.js","hash":"0aaaa015344f39954d2d25072c85b0f9ef603784","modified":1540813986000},{"_id":"themes/next-reloaded/source/js/src/exturl.js","hash":"54825acc8de4793feac415be227b965428f4e97d","modified":1540213352000},{"_id":"themes/next-reloaded/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1540213352000},{"_id":"themes/next-reloaded/source/js/src/motion.js","hash":"b45d2c0d48f2c8e6a0621b8063845f76b89476cc","modified":1540213352000},{"_id":"themes/next-reloaded/source/js/src/post-details.js","hash":"0dde5e6d4547587662a3256317a9d5d1db507692","modified":1540213352000},{"_id":"themes/next-reloaded/source/js/src/scrollspy.js","hash":"fa3c92968bcdbcb8d95a1729f7659d9753cbd077","modified":1540213352000},{"_id":"themes/next-reloaded/source/js/src/utils.js","hash":"66f2ac658d6110f70a86f784d0c5d891a97c14bd","modified":1540213352000},{"_id":"themes/next-reloaded/source/js/src/scroll-cookie.js","hash":"d07b3776708d4ae79ed2037c4c7391d5c9b06b19","modified":1540213352000},{"_id":"themes/next-reloaded/source/lib/clipboard/clipboard-action.js","hash":"8129a822aec2c10d043f423a9350998c8e35eb35","modified":1525012872000},{"_id":"themes/next-reloaded/source/lib/clipboard/clipboard.js","hash":"58de9e9688c6d4bd796d1eb88b6df3b7eb3d0182","modified":1525012872000},{"_id":"themes/next-reloaded/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1540213352000},{"_id":"themes/next-reloaded/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1540213352000},{"_id":"themes/next-reloaded/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1540213352000},{"_id":"themes/next-reloaded/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1540213352000},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1540213352000},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1540213352000},{"_id":"themes/next-reloaded/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1540213352000},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1540213352000},{"_id":"themes/next-reloaded/source/lib/zclip/clipboard.min.js","hash":"eb41c0e88206dda6f0fd8dfbbeefdc0829a9d13d","modified":1525012872000},{"_id":"source/_posts/2018-10-22-IntrotoPro1/Est_S2VS.jpg","hash":"8aa9ca8c8eff24b2a06ba205d1af7c90c2434a47","modified":1601897458203},{"_id":"themes/next-reloaded/.git/objects/pack/pack-309590525463f01225d07409039f0121a5567753.idx","hash":"e89f42ec01441f1765d6a0447c559c360213ee75","modified":1540213352000},{"_id":"themes/next-reloaded/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1540213352000},{"_id":"source/_posts/2019-06-21-ABCalgorithm/modelseleSMC.png","hash":"084d79b8a6540c7aabcd7d285808b1340be9bc77","modified":1601897458478},{"_id":"themes/next-reloaded/.git/logs/refs/heads/master","hash":"f3dadd37a03ed8e677a43f4e117c013bcacf2e53","modified":1540213352000},{"_id":"themes/next-reloaded/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/search/algolia-search/assets.swig","hash":"6958a97fde63e03983ec2394a4f8e408860fb42b","modified":1540213352000},{"_id":"themes/next-reloaded/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/back-to-top-sidebar.styl","hash":"b4a2f1d031fe44452cf55ded8211cf018235073a","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/buttons.styl","hash":"7e509c7c28c59f905b847304dd3d14d94b6f3b8e","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/outline/outline.styl","hash":"aebbd86500d819c4532ab290c62b6f432bc2f878","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/scaffolding/base.styl","hash":"b75256fe3768b1a37b6ff6dd7f9f0ff135a42067","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/scaffolding/mobile.styl","hash":"efc40a32487e0ac7b94b1ca81bdbdcc4ec8f2924","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/scaffolding/tables.styl","hash":"02d138ed65060e98f20bc5b1dd59a791222b7156","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_schemes/Gemini/index.styl","hash":"665b1813a1d6fbc3c5549a76e4f26cd62a804dde","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/_base.styl","hash":"0bef9f0dc134215bc4d0984ba3a16a1a0b6f87ec","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/_menu.styl","hash":"f43c821ea272f80703862260b140932fe4aa0e1f","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/_posts-expanded.styl","hash":"2212511ae14258d93bec57993c0385e5ffbb382b","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/index.styl","hash":"5e12572b18846250e016a872a738026478ceef37","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_schemes/Muse/_menu.styl","hash":"35f093fe4c1861661ac1542d6e8ea5a9bbfeb659","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_schemes/Muse/index.styl","hash":"d5e8ea6336bc2e237d501ed0d5bbcbbfe296c832","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_schemes/Pisces/_layout.styl","hash":"876b5d99061025cf485a3cac440624ded5734319","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_schemes/Pisces/_menu.styl","hash":"05a5abf02e84ba8f639b6f9533418359f0ae4ecb","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_schemes/Pisces/_sidebar.styl","hash":"41f9cdafa00e256561c50ae0b97ab7fcd7c1d6a2","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_schemes/Pisces/_sub-menu.styl","hash":"ffa870c3fa37a48b01dc6f967e66f5df508d02bf","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_schemes/Pisces/index.styl","hash":"5779cc8086b1cfde9bc4f1afdd85223bdc45f0a0","modified":1540213352000},{"_id":"themes/next-reloaded/source/js/src/schemes/pisces.js","hash":"89267bd16ecbedd1958af7f0fb3f4f654d24fffa","modified":1540213352000},{"_id":"themes/next-reloaded/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1540213352000},{"_id":"themes/next-reloaded/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1540213352000},{"_id":"themes/next-reloaded/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1540213352000},{"_id":"themes/next-reloaded/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1540213352000},{"_id":"themes/next-reloaded/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1540213352000},{"_id":"themes/next-reloaded/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1540213352000},{"_id":"themes/next-reloaded/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1540213352000},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1540213352000},{"_id":"themes/next-reloaded/.git/logs/refs/remotes/origin/HEAD","hash":"f3dadd37a03ed8e677a43f4e117c013bcacf2e53","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/footer/footer.styl","hash":"39dee82d481dd9d44e33658960ec63e47cd0a715","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/highlight/highlight.styl","hash":"a6dc3c7eb81ef5117c28fa2245fff1adc02d0292","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/header/github-banner.styl","hash":"ee37e6c465b9b2a7e39175fccfcbed14f2db039b","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/header/header.styl","hash":"7cc3f36222494c9a1325c5347d7eb9ae53755a32","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/pages/breadcrumb.styl","hash":"7dd9a0378ccff3e4a2003f486b1a34e74c20dac6","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/pages/pages.styl","hash":"fb451dc4cc0355b57849c27d3eb110c73562f794","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-expand.styl","hash":"ca89b167d368eac50a4f808fa53ba67e69cbef94","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-meta.styl","hash":"417f05ff12a2aaca6ceeac8b7e7eb26e9440c4c3","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-reading_progress.styl","hash":"f4e9f870baa56eae423a123062f00e24cc780be1","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-reward.styl","hash":"549a8a0b5301d32acd86a97f17340cdfcd46fb63","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-title.styl","hash":"fcbbf06b546c366d70b7d2ba5880b0be3ca1e8ea","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-widgets.styl","hash":"578bb2d5f24cad39205bbafb4c39c7e9962b9fa9","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post.styl","hash":"6089cbf4c907fe198b6501e40dc937480d0be175","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"35c0350096921dd8e2222ec41b6c17a4ea6b44f2","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-author.styl","hash":"e18b90c97aaff027e795f5a0cb10476a71bf1c3a","modified":1540306456000},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"4427ed3250483ed5b7baad74fa93474bd1eda729","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar.styl","hash":"43bc58daa8d35d5d515dc787ceb21dd77633fe49","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/tags/full-image.styl","hash":"6ec8ea7b11a146777b6b8da0f71f0cc1dbd129df","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/tags/tags.styl","hash":"5e340ee2407a4e39cd708794cfcc718a5f398d7b","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/algolia-search.styl","hash":"10e9bb3392826a5a8f4cabfc14c6d81645f33fe6","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/gitalk.styl","hash":"a01484e350ad5fc9b1fdfbfafb2ddd9687ad4d20","modified":1540294204000},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/needsharebutton.styl","hash":"a5e3e6b4b4b814a9fe40b34d784fed67d6d977fa","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/related-posts.styl","hash":"76937db9702053d772f6758d9cea4088c2a6e2a3","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/third-party.styl","hash":"85e048a5829cdd0235c97e594765ec325076775b","modified":1540294226000},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1540213352000},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1540213352000},{"_id":"source/_posts/2018-10-22-IntrotoPro1/Trees_S2.jpg","hash":"1b861d7249e51042282ca283eaaca2a7ca6290ba","modified":1601897458209},{"_id":"themes/next-reloaded/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1540213352000},{"_id":"source/_posts/2020-03-18-generalABC/setup_model1.gif","hash":"41faa38bfa1cf77516dc7d2021d9a1b280fdb05c","modified":1601897459196},{"_id":"source/_posts/2020-03-18-generalABC/setup_model2.gif","hash":"cfe5c89a2b130ee3022944b95bf48a184bf855bb","modified":1601897459199},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/step2.png","hash":"f083cc62bdbceaa7841d58b5ca6391b98c75b67d","modified":1601897458264},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/step4.png","hash":"711f5160a7362eccb039cf22c6f0f6247732db15","modified":1601897458277},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/step3.png","hash":"649ae940377bf729b5997e82435eca95462b108e","modified":1601897458270},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/step6.png","hash":"35a833df81cbb285569edf65deeede0394bd749b","modified":1601897458290},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/step7.png","hash":"8e86e2988358ba7ca17eff5b97c27560b8d73e31","modified":1601897458296},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/step8.png","hash":"3c17f96e1ae34c613775cfa48b88d2347ce32143","modified":1601897458303},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/step1.png","hash":"0651aeec6e745d56e82cbce37c772860e19b958f","modified":1601897458259},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/step5.png","hash":"a63831c50ca8613ffa826fc6e07a6f3b75eda72c","modified":1601897458283},{"_id":"themes/next-reloaded/.git/objects/pack/pack-309590525463f01225d07409039f0121a5567753.pack","hash":"b14b82f8d602008dc7266f09f2db0a47721357db","modified":1540213352000},{"_id":"source/_posts/2019-06-21-TPmodel/guixianren1.jpg","hash":"ed62f2db9f2ba7b6257feaabca909a4560b5b4b1","modified":1601897458531},{"_id":"source/_posts/2019-06-21-TPmodel/guixianren_s.png","hash":"ac23a18f63625f176858319f7aa82cd470b7b97e","modified":1601897458597},{"_id":"source/_posts/2019-06-21-ABCalgorithm/MCMC3chains_test3.mp4","hash":"15749ae0ca8e6e6541d54680466d0976454fd81e","modified":1601897458446},{"_id":"source/_posts/2019-06-21-TPmodel/singlespecies5.mp4","hash":"8df799aec096d3ac68b64302b0124567dd866084","modified":1601897459177},{"_id":"source/_posts/2019-06-21-TPmodel/multi6species15.mp4","hash":"2361444126185c5a4cad48a95c6acef6415f4fc5","modified":1601897458781},{"_id":"public/search.xml","hash":"f1e8a9a1716a51f239ca70dbbb6a4226ba60bb83","modified":1601898407600},{"_id":"public/categories/index.html","hash":"a6e69f14791f9e705b36dc760dad35076102eabd","modified":1601898407722},{"_id":"public/about/index.html","hash":"0be3ced9f96b152a035fadb0f9761ac825ac4c11","modified":1601898407722},{"_id":"public/tags/index.html","hash":"5b729d639782fa0df2203f8401687bf863e760e4","modified":1601898407722},{"_id":"public/2020/10/05/2020-04-25-datavisulization_shinyapp/index.html","hash":"0d621002c17a53f4d47ff87bf66d76a7b0f527d4","modified":1601898407722},{"_id":"public/2020/10/05/hello-world/index.html","hash":"8e7af1c57a524b0b744174fc365d9ac5ada18736","modified":1601898407722},{"_id":"public/2020/10/05/2020-04-02-government&pandemic/index.html","hash":"72356de4bab72eecb6807b708baeb8e25a35fe97","modified":1601898407722},{"_id":"public/2020/10/05/2020-03-18-generalABC/index.html","hash":"5a67e032e1b6cb0d8f3085f5e982fdcddcd35bb5","modified":1601898407722},{"_id":"public/2020/10/05/2019-06-21-TPmodel/index.html","hash":"2f79b0f968cebe47fa08c1782ba289191ced2c6b","modified":1601898407722},{"_id":"public/2020/10/05/2020-03-09-GUI-traitevolution/index.html","hash":"6b0f924765cc428b3a967803fe99331eea8dacd9","modified":1601898407722},{"_id":"public/2020/10/05/2019-06-21-ABCalgorithm/index.html","hash":"fe248c74c497ed30ff828eadad813b7be5947740","modified":1601898407722},{"_id":"public/2020/10/05/2019-03-20-readdatadon/index.html","hash":"7448b74b09585d6fbcd71c27ca64b46a68229050","modified":1601898407723},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/index.html","hash":"b4433de6d39ef9358d797cea0aa3c4a40b1a2640","modified":1601898407723},{"_id":"public/2020/10/05/2019-02-01-Machinelearningseries1/index.html","hash":"8a154c9115b5896ee0b97d1408e2c2c78d5c255e","modified":1601898407723},{"_id":"public/2020/10/05/2019-01-21-PyCUDAseries3/index.html","hash":"cc06329da92746a180ebd24b907915dff08e9063","modified":1601898407723},{"_id":"public/2020/10/05/2019-01-14-PyCUDAseries2/index.html","hash":"5e74006970d99d56045a1c1c9ac40d11a49b0373","modified":1601898407724},{"_id":"public/2020/10/05/2019-01-10sed2/index.html","hash":"c1eb2f61a191870988cb049ee664efd1c57ccb40","modified":1601898407724},{"_id":"public/2020/10/05/2018-12-07-ggradar2helpdocument/index.html","hash":"673c7dcff14a7126c7b08ed0587a92d27bd22cb3","modified":1601898407724},{"_id":"public/2020/10/05/2019-01-04-PyCUDAseries1/index.html","hash":"ab799ff9fc0c95fabb97813591168d5ccd48a6a8","modified":1601898407724},{"_id":"public/2020/10/05/2018-12-05-ggradar2/index.html","hash":"1e7ceed942eba5bc4c4a50f7079738c34dfd56d7","modified":1601898407724},{"_id":"public/2020/10/05/2018-11-30-SMCplots/index.html","hash":"aef7e456e80b224e9c825aa33442b1149689aa19","modified":1601898407724},{"_id":"public/2020/10/05/2018-11-26-Rupdateallpackages/index.html","hash":"9f271038bd3a933c195bccf8c5750ef672314ba8","modified":1601898407724},{"_id":"public/2020/10/05/2018-11-29-color/index.html","hash":"5cda1e312c29283effcdbe65bfc7bda0a48db340","modified":1601898407724},{"_id":"public/2020/10/05/2018-10-29-sed/index.html","hash":"2aedd6b23a2ca281bd05d952e6ea1baac40399b0","modified":1601898407724},{"_id":"public/2020/10/05/2018-10-24-pruneLfunction/index.html","hash":"059d47eeebf06b3f8b40ae1255e74fa4a08cf97a","modified":1601898407724},{"_id":"public/2020/10/05/2018-10-25-PLSR/index.html","hash":"c9566c711d7bf7e540da64ecbf8bb9198ffbda13","modified":1601898407724},{"_id":"public/2020/10/05/2018-10-24-derivingFPequ/index.html","hash":"49d40ccf1ded0be1b1ae8419505fef73f5237b2a","modified":1601898407724},{"_id":"public/2020/10/05/2018-10-24-phylo2Lfunction/index.html","hash":"ae5d83dda981c55d972cacc553eff7f81b9362f0","modified":1601898407724},{"_id":"public/2020/10/05/2018-10-23-Blogging tech/index.html","hash":"f0a239397944475dc2545bff5595c85d72154c95","modified":1601898407725},{"_id":"public/2020/10/05/2018-10-22-IntrotoPro1/index.html","hash":"46f17402c3d99dbc5e7410a02a89964252e30ecc","modified":1601898407725},{"_id":"public/archives/index.html","hash":"012fb1149688c97bb8282f998ad15c998db9ea52","modified":1601898407725},{"_id":"public/archives/page/2/index.html","hash":"f54e06cb66c8f9b35c6159557c359fcb3404e2e0","modified":1601898407725},{"_id":"public/archives/page/3/index.html","hash":"ab711c7babf346ca3ac2b2fd1a39425dbc6da7e3","modified":1601898407725},{"_id":"public/archives/2020/index.html","hash":"2749accba6597dbc60c3941ac1e16dcdd979c014","modified":1601898407725},{"_id":"public/archives/2020/page/2/index.html","hash":"5d590ef8021161cb0ebfca40c05e43adef1eb6c9","modified":1601898407725},{"_id":"public/archives/2020/page/3/index.html","hash":"e7f598ce963c8497e1b3032c297d0070e70545b8","modified":1601898407725},{"_id":"public/archives/2020/10/index.html","hash":"918ba843352b88faeec593b2bc3cc5ac65aed736","modified":1601898407725},{"_id":"public/archives/2020/10/page/2/index.html","hash":"c4cfee557f365390d4a212eebcd4ad712646bb52","modified":1601898407725},{"_id":"public/archives/2020/10/page/3/index.html","hash":"af1c15880b60db62896c90fb2f9e994cd8051cae","modified":1601898407725},{"_id":"public/categories/Research/index.html","hash":"45178736c18ef16ee2db4d92cf6e611089c8e24f","modified":1601898407725},{"_id":"public/categories/Research/page/2/index.html","hash":"5091d12aca4e8c8559065d428447f38b859bb977","modified":1601898407725},{"_id":"public/categories/Research/page/3/index.html","hash":"59ac063cb2846842d3bec95ccc04bd5a46e8992c","modified":1601898407725},{"_id":"public/categories/Blogging/index.html","hash":"50db03f258511282a041a5f195156ae77d225b58","modified":1601898407725},{"_id":"public/categories/R/index.html","hash":"c844481127cc7ad1cbe0c4b540070ba13153dd45","modified":1601898407725},{"_id":"public/categories/Research/Phd-projects/index.html","hash":"d805cbf7c0142727765a0c8d7494e9c61a9bd751","modified":1601898407725},{"_id":"public/categories/Research/R/index.html","hash":"14e039d83bec56cd8491666494dc20dda1768b6d","modified":1601898407725},{"_id":"public/categories/Research/Numerical-method/index.html","hash":"8ad07b94db1e6b126cc5cccfc33b5cd7500b0869","modified":1601898407725},{"_id":"public/categories/Blogging/Blogging-tech/index.html","hash":"874ead97fa221ff099bb6d095353311e6afc0aaf","modified":1601898407725},{"_id":"public/categories/Research/Bash/index.html","hash":"54e824daada476989444ddfd63cbf8e18d9f710b","modified":1601898407725},{"_id":"public/categories/Research/Stochastic-differential-equations/index.html","hash":"6b69bc5983b06207bd8e9efceea10bff308d917e","modified":1601898407725},{"_id":"public/categories/Research/Data-visualization/index.html","hash":"e643e76069b13bc16c7c41b13469fd0561063bd9","modified":1601898407725},{"_id":"public/categories/R/phylogeny-function/index.html","hash":"b4e6a2d33cf6d578059f07282289cceabdc41e42","modified":1601898407725},{"_id":"public/categories/Research/GPU-programming/index.html","hash":"af4a8629ac1acd9633f695df68d415df25d16fb9","modified":1601898407725},{"_id":"public/categories/Research/R/phylogeny-function/index.html","hash":"3f7038d95510178b1e27a0bbecfab6af27971ccf","modified":1601898407725},{"_id":"public/categories/Research/Machine-learning/index.html","hash":"b47db0be4579b59b230f137f77b69719cb948ace","modified":1601898407725},{"_id":"public/categories/Research/Algorithm/index.html","hash":"4aca8133b87cf0f9500e7ee2b10f8a5d2b06a0ef","modified":1601898407726},{"_id":"public/categories/Research/Python/index.html","hash":"a1257b721155c0d7fa879408f40c8311411aee16","modified":1601898407726},{"_id":"public/categories/Research/Bash/sed/index.html","hash":"797a06950a2abb871d5b8131cb8180a66a75fbdf","modified":1601898407726},{"_id":"public/categories/Research/Phd-projects/trait-population-model/index.html","hash":"aace1713682516196a6320d125e4f0777ca7b7be","modified":1601898407726},{"_id":"public/categories/Research/Modeling/index.html","hash":"62679f48c4fbabd4971e2f790a7694ee339744fd","modified":1601898407726},{"_id":"public/categories/BB-life/index.html","hash":"ba4ca02fd920abd63812c3fd869fcb03d8cda3d8","modified":1601898407726},{"_id":"public/categories/Research/Data-visualization/R/index.html","hash":"f9e2a4b13dfce6e08992f86f867c520c74b755ad","modified":1601898407726},{"_id":"public/categories/Research/GPU-programming/pyCUDA/index.html","hash":"c73767ef2b6af10cad289a6b30893ca7370855f0","modified":1601898407726},{"_id":"public/categories/Research/Machine-learning/Deep-learning/index.html","hash":"d8f562925a684a3b5f49faa5e1af01b87e5349bb","modified":1601898407726},{"_id":"public/categories/Research/Algorithm/ABC/index.html","hash":"fb4ca1154d559328e95155ef906af9dfd98cad2d","modified":1601898407726},{"_id":"public/categories/Research/Python/GUI/index.html","hash":"02af268b964301c2c0c88cd82ade557c97c129d9","modified":1601898407726},{"_id":"public/categories/Research/Data-visualization/R/ggradar2/index.html","hash":"6f076f87e3114481b88782b17f8cce684c1991d0","modified":1601898407726},{"_id":"public/index.html","hash":"a06803624c69a381e5d6608f95debe3109c844c6","modified":1601898407726},{"_id":"public/page/2/index.html","hash":"cb3d00284785bc7d4cd832d5971c811b9d42e9a6","modified":1601898407726},{"_id":"public/page/3/index.html","hash":"c2f391cb1e75e1724c245f45ac07e9481e1d94a5","modified":1601898407726},{"_id":"public/tags/evolution/index.html","hash":"a2cf3859109ba1d0af291677ea0c85e2b2ab04a0","modified":1601898407726},{"_id":"public/tags/mathematical-modeling/index.html","hash":"32d51780853beb504d9475150fc6dc0bf8d2bd23","modified":1601898407726},{"_id":"public/tags/diversity-dependence/index.html","hash":"cc5eeda20b2f98d723a9e4eff10b0a2c7700928c","modified":1601898407726},{"_id":"public/tags/Tech/index.html","hash":"7643a96a867d05f337071aa26cbac5ad8254e502","modified":1601898407726},{"_id":"public/tags/the-Fokker-Planck-equation/index.html","hash":"e0b4537ee77cf139c9f6a21fa38562c04897454d","modified":1601898407726},{"_id":"public/tags/stochastic-differential-equation/index.html","hash":"68700cb12d1a74e6f255ec559cafca9e385644a0","modified":1601898407726},{"_id":"public/tags/math/index.html","hash":"80f3bcd02acae2157c16267c96631d0100a8b0c5","modified":1601898407726},{"_id":"public/tags/phylogeny/index.html","hash":"76a88c614901548953d68d2a4367f8555a1aeb63","modified":1601898407726},{"_id":"public/tags/phylo-class/index.html","hash":"86191c7c90efb67e5912513cde4f5cd24b570842","modified":1601898407726},{"_id":"public/tags/L-table/index.html","hash":"5218fa3a5e0426bace59d8aa64a53db95ddb4f94","modified":1601898407726},{"_id":"public/tags/DDD-package/index.html","hash":"0d618a3db934a0000add0c06de9d5d703ed80cdf","modified":1601898407727},{"_id":"public/tags/pruneL/index.html","hash":"2058c3d76121438e8ed5f1f43e550bdd2a06bd38","modified":1601898407727},{"_id":"public/tags/phylo2L/index.html","hash":"10e25066fbcd27ed8b69a82c63c5298c473ef68f","modified":1601898407727},{"_id":"public/tags/regression/index.html","hash":"1b9996110b83b905c06c2cfba62e465b42b7bd60","modified":1601898407727},{"_id":"public/tags/least-square-method/index.html","hash":"36f466d08bb80621ea021e0cc5270e167cb00b6a","modified":1601898407727},{"_id":"public/tags/partial-least-square/index.html","hash":"7ea83becc36459fc07fa35b4460874bfb795c1f9","modified":1601898407727},{"_id":"public/tags/machine-learning/index.html","hash":"693fe6ca25a2b6398c95c83364e0df3e723704e1","modified":1601898407727},{"_id":"public/tags/project-3/index.html","hash":"1f5c3ec39f58df5189aa02d9131a1b2acc612d7c","modified":1601898407727},{"_id":"public/tags/bash/index.html","hash":"2ce9857885c8e7b5a4035f71297a3b2df2315acc","modified":1601898407727},{"_id":"public/tags/mega-data/index.html","hash":"9cf57327f927b6421e15d6654ad61c53259d0331","modified":1601898407727},{"_id":"public/tags/extract-information/index.html","hash":"ce4b477c6892d0295d68f010d7ee3d400f0c50dd","modified":1601898407727},{"_id":"public/tags/R/index.html","hash":"bac358d38bbc987ce010fe87ea8c188f54b061c7","modified":1601898407727},{"_id":"public/tags/update/index.html","hash":"b2f3842e782699095e3fa8e8610a301ba9bbf23e","modified":1601898407727},{"_id":"public/tags/packages/index.html","hash":"8d7f935fb73c4b5a3df000940cb8e5ad3b2aa78e","modified":1601898407727},{"_id":"public/tags/color/index.html","hash":"58f181042dc613990ccafae11ad3a80152c4e5fe","modified":1601898407727},{"_id":"public/tags/python/index.html","hash":"621b6bba2041d345b830f2ef78b7c81a85c88648","modified":1601898407727},{"_id":"public/tags/plot/index.html","hash":"db8b3ca3205e3c57ea9853a5d9569aafd40ee1b1","modified":1601898407727},{"_id":"public/tags/ggplot/index.html","hash":"a5521493c06e60b67ed275e981e80f079e362697","modified":1601898407727},{"_id":"public/tags/SMC/index.html","hash":"1e2143d04ce36b403759432e59c62cb930c67fcf","modified":1601898407727},{"_id":"public/tags/Data-visualization/index.html","hash":"e74cc68bd936998ab1b0e5899a765addc5e66271","modified":1601898407727},{"_id":"public/tags/ggradar2/index.html","hash":"6d1fa4e08297e02d518025f9e87e46affd83eb23","modified":1601898407727},{"_id":"public/tags/help-document/index.html","hash":"1b5014a800d1fd62936502fea0253dc480e8318b","modified":1601898407727},{"_id":"public/tags/Python/index.html","hash":"ec6b43d3936eab2fb18747686f0511a005dca318","modified":1601898407727},{"_id":"public/tags/CUDA/index.html","hash":"c97a2123ecfc37250997a65c66c1eb472729a21e","modified":1601898407727},{"_id":"public/tags/GPU-programming/index.html","hash":"a5747d76006e42bf7d9d174025ef4b08b5d1107c","modified":1601898407727},{"_id":"public/tags/Parallel-computation/index.html","hash":"8b769fa1bc6e5bf59fc75a00e754aaa89c655e49","modified":1601898407727},{"_id":"public/tags/Machine-learning/index.html","hash":"b2e345125ea70abfad94f0c45d2dccb8d0e25189","modified":1601898407728},{"_id":"public/tags/neural-networks/index.html","hash":"ff090bffaa2cb8972a4ff71f36d57a19bdba594e","modified":1601898407728},{"_id":"public/tags/Backward-propagation/index.html","hash":"0a565f4814b122b3fc35b295d2b9441524ba30ee","modified":1601898407728},{"_id":"public/tags/gradient-descent/index.html","hash":"7b7edf19671a88e15b33238ff83d19249f8cf0b6","modified":1601898407728},{"_id":"public/tags/data-analysis/index.html","hash":"575687b6414fa46f19227b7a095bed9bef4caea4","modified":1601898407728},{"_id":"public/tags/ABC/index.html","hash":"79c627db4fcd5e4bae5e7dbbbfed71b87f86717a","modified":1601898407728},{"_id":"public/tags/Animation/index.html","hash":"3c6ab309f020af19f11c2aae2abf4ecf4598de2d","modified":1601898407728},{"_id":"public/tags/GUI/index.html","hash":"bd56f6ca41b3a4c454069c35362d6590a74d154b","modified":1601898407728},{"_id":"public/tags/models/index.html","hash":"c267df2442924e863a3c68aac85fb3a22877a0c8","modified":1601898407728},{"_id":"public/tags/ecology/index.html","hash":"6938d2ac8a6526e8298482f9f230443ae3f6f81d","modified":1601898407728},{"_id":"public/tags/algorithm/index.html","hash":"bbfb48e63628ab52cf48e3c851a356fd24b2f741","modified":1601898407728},{"_id":"public/tags/simulation/index.html","hash":"fe4f32b80b49a057d514d83112447795d616568e","modified":1601898407728},{"_id":"public/tags/coronavirus/index.html","hash":"cb138b6a227df6cfae5d01352b8c60f16bb661cd","modified":1601898407728},{"_id":"public/tags/pandemic/index.html","hash":"2cd71feba4bedb1ca073cca19b04c851ffe96945","modified":1601898407728},{"_id":"public/tags/government-measure/index.html","hash":"1b2f13ff77764d73852cc3b27a8611275a6dfa3d","modified":1601898407728},{"_id":"public/tags/data-visualization/index.html","hash":"06a6a412824c5e6c626948a3390be210614b2481","modified":1601898407728},{"_id":"public/tags/info/index.html","hash":"ce5241acfc5a3eda75f8f09d367f0057c7eef19e","modified":1601898407728},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1601898407750},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1601898407750},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1601898407750},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1601898407750},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1601898407750},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1601898407750},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1601898407750},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1601898407750},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1601898407750},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1601898407750},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1601898407750},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1601898407750},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1601898407750},{"_id":"public/images/favicon.ico","hash":"dbad95a76f53c8edcf8c452d2ee9bd27fbfc74ad","modified":1601898407750},{"_id":"public/images/favicon16.ico","hash":"01a6788c4ccc31027fa9f836b9c1573d25bd7472","modified":1601898407750},{"_id":"public/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1601898407750},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1601898407750},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1601898407750},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1601898407750},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1601898407750},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1601898407750},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1601898407750},{"_id":"public/2020/10/05/2018-11-30-SMCplots/Rplot1.png","hash":"7b3ed1bb1002079cb8719e12f7d0e01a4098ae75","modified":1601898407750},{"_id":"public/2020/10/05/2018-11-30-SMCplots/Rplot2.png","hash":"a4ace9d48b797039ce52da319ebf313b339b2e1e","modified":1601898407751},{"_id":"public/2020/10/05/2019-01-10sed2/mfiles.png","hash":"45a7ad5fb9fe1ab18105f988b18272269c0e549b","modified":1601898407751},{"_id":"public/2020/10/05/2019-01-14-PyCUDAseries2/speedtest.png","hash":"c48054dc2ef38a8857227c55428dc68e185f841d","modified":1601898407751},{"_id":"public/2020/10/05/2019-02-01-Machinelearningseries1/modelselection1.png","hash":"c119feddb57b38b5d364ce35082001d7d62dbd74","modified":1601898407751},{"_id":"public/2020/10/05/2019-02-01-Machinelearningseries1/prediction_sample.png","hash":"cd4ff39cfafd41d0711a4f11a77df2c0277ce7f9","modified":1601898407751},{"_id":"public/2020/10/05/2019-03-20-readdatadon/d.png","hash":"6bdc800119bc8de7461548b9d94b5f776c43a345","modified":1601898407751},{"_id":"public/2020/10/05/2019-03-20-readdatadon/rewrittendata.png","hash":"52a13bcda911b3879916779e798250de59cd01ba","modified":1601898407751},{"_id":"public/2020/10/05/2018-10-29-sed/d.png","hash":"6bdc800119bc8de7461548b9d94b5f776c43a345","modified":1601898407751},{"_id":"public/2020/10/05/2018-10-29-sed/ds.png","hash":"965d5418e7f621023a969770a36e2702a117ed90","modified":1601898407751},{"_id":"public/2020/10/05/2018-10-29-sed/rawdata.png","hash":"504d3898506b182f7ffb5b6cb19618dde1154b0e","modified":1601898407751},{"_id":"public/2020/10/05/2019-06-21-ABCalgorithm/Rplot2.png","hash":"a4ace9d48b797039ce52da319ebf313b339b2e1e","modified":1601898407751},{"_id":"public/2020/10/05/2018-11-29-color/col1.png","hash":"cbf10523cc8e07e62bef59195000b0f541cd581f","modified":1601898407751},{"_id":"public/2020/10/05/2018-11-29-color/col2.png","hash":"3dd942d4972d112beb80a25fc3aef5c905a92b54","modified":1601898407751},{"_id":"public/2020/10/05/2018-11-29-color/col3.png","hash":"1d83794811a02b4bdadab17f78c8275ad87f131f","modified":1601898407751},{"_id":"public/2020/10/05/2018-11-29-color/col4.png","hash":"8e015d04d4090c8debb5eed1964595de0f368a5b","modified":1601898407751},{"_id":"public/2020/10/05/2020-03-09-GUI-traitevolution/p1.png","hash":"51eee44eff5793b37c8e4206438a97cff6b47603","modified":1601898407751},{"_id":"public/2020/10/05/2020-03-09-GUI-traitevolution/p2.png","hash":"74d01c3c8e6cb746289469f2602c303afa15857a","modified":1601898407751},{"_id":"public/2020/10/05/2020-03-09-GUI-traitevolution/p3.png","hash":"2ce44cf24deb5e9d34abd78b7c93eea8e43cfc72","modified":1601898407751},{"_id":"public/2020/10/05/2020-03-09-GUI-traitevolution/p4.png","hash":"c8d1a197ba7983f534e5f7818b3063c086c85eed","modified":1601898407751},{"_id":"public/2020/10/05/2020-03-18-generalABC/Predict1.png","hash":"8fbdb67a9331ad9f7d467a4a486d039dd2f5e016","modified":1601898407751},{"_id":"public/2020/10/05/2020-03-18-generalABC/Predict2.png","hash":"ac88d57a8405d71db8a98580cb7951bd55dcfb29","modified":1601898407751},{"_id":"public/2020/10/05/2020-03-18-generalABC/data.png","hash":"2f1482534ce247a1e5ece239b01f6f5690d0b5cf","modified":1601898407751},{"_id":"public/2020/10/05/2018-12-05-ggradar2/fullscore.png","hash":"5c50cfa94f69d90edfb9561abf66f229c83eee50","modified":1601898407751},{"_id":"public/2020/10/05/2018-12-05-ggradar2/lux.png","hash":"9ddc42a286d090bbe82e25d58cbd92c1d7347079","modified":1601898407751},{"_id":"public/2020/10/05/2018-12-05-ggradar2/mini.png","hash":"18b7ec1f51059e5dd8e748c53d68a806daec38ea","modified":1601898407751},{"_id":"public/2020/10/05/2018-12-05-ggradar2/multipleplots.png","hash":"2c0d8c8213f7775d680657810a17ecb3e48ed38d","modified":1601898407751},{"_id":"public/2020/10/05/2018-12-05-ggradar2/multipleplotsbig.png","hash":"739c484223aa914606da7a43e93446af8e163388","modified":1601898407752},{"_id":"public/2020/10/05/2018-12-05-ggradar2/nolegend.png","hash":"338d96bd8f0ccd7d26c8dd4e341b39362c64a04e","modified":1601898407752},{"_id":"public/2020/10/05/2018-12-05-ggradar2/roundfill.png","hash":"7a6539ae5da9c90b8a62b37eed031cff190d7c9c","modified":1601898407752},{"_id":"public/2020/10/05/2018-12-05-ggradar2/roundnofill.png","hash":"31bfb3f19b550723abcc32b6b7177982128c8830","modified":1601898407752},{"_id":"public/2020/10/05/2018-12-05-ggradar2/straightfill.png","hash":"f0f6bfe7a10e8856cd45cff454aad0655692b18f","modified":1601898407752},{"_id":"public/2020/10/05/2018-12-05-ggradar2/straightnofill.png","hash":"efa92f19754af10d469cf6675eb7a44e08e70eda","modified":1601898407752},{"_id":"public/2020/10/05/2018-12-05-ggradar2/trend.png","hash":"580a0ab7b4449330b9425f722f196fffb82c9134","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/1.png","hash":"08f6c16b529bc03ecf83c55de5d28d2ec737aad7","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/10.png","hash":"b2656495f031e2eefcc8734ee589b1ab555f1c6e","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/11.png","hash":"a92bd5f59fee6cf4c18e4c7778bb53c8853c9b27","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/12.png","hash":"6b4b7c01e0a107e349be33d327ff4a828de3ad41","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/13-15.png","hash":"4e6540b7dbbc1cecadc83b89205c4e249207f2d8","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/16.png","hash":"a79170ae6a15e5fb2342f6c373121e17d76eed33","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/17.png","hash":"58c3578da213026a6faf5236cef70d0de0df18e7","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/18.png","hash":"fe7df21d74b9ac39988417271f71df0da3e14d46","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/19-1.png","hash":"9a35d6438ea32257c73165fa5764e7f1555390ad","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/19.png","hash":"747d05b0ea5814c72e57861034a59c4a63211a60","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/2.png","hash":"2ad49fbdeb4f563edd0797d9a4d972a8839c9d55","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/20.png","hash":"85329614886a4b196eb845f13bb3a0fffabb1489","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/21.png","hash":"94ce999c4c85fa14d85ae219eb00f9fbc6c94c36","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/22.png","hash":"07b7bd1f8775d6e41253f8a4b0ae8e696bbc6502","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/23.png","hash":"839ecc43535c8c589a64aa7cc947ec88441f871a","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/24.png","hash":"81ae1d4ec941ea54cf9457840d97557d0852b2e5","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/25.png","hash":"cb29499e4d9341922f9d1e234068966d0b28fb73","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/3.png","hash":"82ca237383cd0b4c6c69991218d5243fe95411dd","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/4.png","hash":"f53c3a372ae9ae0a6e992e176ef35ccf8fd1160a","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/5.png","hash":"4445bfdd3d0690432257608a790062f4d4ecfccf","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/6.png","hash":"d28b9b8aed0df282c8921d57715c719b924e3a37","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/78.png","hash":"1a55ed4d935acf4d99d095bd183364b02bb7182b","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/9-1.png","hash":"dd5ccd15eab9ebd82ffb81322a9b543f28326dff","modified":1601898407752},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/9.png","hash":"21858569c261d689790e470dda6647388bbf6828","modified":1601898407752},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1601898408667},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1601898408668},{"_id":"public/2020/10/05/2019-01-04-PyCUDAseries1/win10SDK.png","hash":"255648cb736a724d3623bf51d04bbdce05778edd","modified":1601898408669},{"_id":"public/2020/10/05/2019-01-10sed2/rdatafiles.png","hash":"7c87f5b7820eb89deaf7a4250e942ef94b1bb53b","modified":1601898408669},{"_id":"public/2020/10/05/2019-01-14-PyCUDAseries2/gridblockthread.png","hash":"31ad63d0cf3f0411ae70f29a71261f3e6642020e","modified":1601898408669},{"_id":"public/2020/10/05/2018-10-22-IntrotoPro1/Powertable.jpg","hash":"08373cf6112778618fd1a22a51e735a27a49c679","modified":1601898408670},{"_id":"public/2020/10/05/2020-03-18-generalABC/abc_scheme.png","hash":"462844b99697e9d935b97ec985a78007fa3da4f8","modified":1601898408670},{"_id":"public/css/main.css","hash":"327759d0ddf46f861815407534c76ace8b1a99b7","modified":1601898408702},{"_id":"public/js/src/affix.js","hash":"a2aab233d99297435a5274bf512c3c753fe08e80","modified":1601898408702},{"_id":"public/js/src/algolia-search.js","hash":"1f7f10c579e7703d0f6acb8b73f3d78a07d0c623","modified":1601898408702},{"_id":"public/js/src/bootstrap.js","hash":"1c41508b83cb0c4512e64b4d63afa1be954ce8ef","modified":1601898408702},{"_id":"public/js/src/clipboard-use.js","hash":"f7121e23b59dafd2056edcb45658f9beb7433e89","modified":1601898408702},{"_id":"public/js/src/custom.js","hash":"90b139a071ae7af9b71d8cd9c34929d8f4349a7b","modified":1601898408702},{"_id":"public/js/src/exturl.js","hash":"54825acc8de4793feac415be227b965428f4e97d","modified":1601898408702},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1601898408702},{"_id":"public/js/src/motion.js","hash":"b45d2c0d48f2c8e6a0621b8063845f76b89476cc","modified":1601898408702},{"_id":"public/js/src/scrollspy.js","hash":"fa3c92968bcdbcb8d95a1729f7659d9753cbd077","modified":1601898408702},{"_id":"public/js/src/post-details.js","hash":"0dde5e6d4547587662a3256317a9d5d1db507692","modified":1601898408702},{"_id":"public/js/src/utils.js","hash":"66f2ac658d6110f70a86f784d0c5d891a97c14bd","modified":1601898408702},{"_id":"public/js/src/scroll-cookie.js","hash":"d07b3776708d4ae79ed2037c4c7391d5c9b06b19","modified":1601898408702},{"_id":"public/lib/clipboard/clipboard-action.js","hash":"8129a822aec2c10d043f423a9350998c8e35eb35","modified":1601898408702},{"_id":"public/lib/clipboard/clipboard.js","hash":"58de9e9688c6d4bd796d1eb88b6df3b7eb3d0182","modified":1601898408702},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1601898408702},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1601898408702},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1601898408702},{"_id":"public/lib/zclip/clipboard.min.js","hash":"eb41c0e88206dda6f0fd8dfbbeefdc0829a9d13d","modified":1601898408702},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1601898408703},{"_id":"public/js/src/schemes/pisces.js","hash":"89267bd16ecbedd1958af7f0fb3f4f654d24fffa","modified":1601898408703},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1601898408703},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1601898408703},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1601898408703},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1601898408703},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1601898408703},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1601898408703},{"_id":"public/about/Liang_CV.pdf","hash":"3e1a7c3b61f72b6ce450db175b0d9f7dae0653d8","modified":1601898408703},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1601898408703},{"_id":"public/2020/10/05/2018-10-22-IntrotoPro1/Est_S2VS.jpg","hash":"8aa9ca8c8eff24b2a06ba205d1af7c90c2434a47","modified":1601898408703},{"_id":"public/2020/10/05/2019-01-04-PyCUDAseries1/vsinstaller.png","hash":"00caf651a329fb3185dc12e1a67b0eabf91875be","modified":1601898408703},{"_id":"public/2020/10/05/2019-06-21-ABCalgorithm/modelseleSMC.png","hash":"084d79b8a6540c7aabcd7d285808b1340be9bc77","modified":1601898408770},{"_id":"public/2020/10/05/2018-10-22-IntrotoPro1/Trees_S2.jpg","hash":"1b861d7249e51042282ca283eaaca2a7ca6290ba","modified":1601898408818},{"_id":"public/2020/10/05/2020-03-18-generalABC/setup_model1.gif","hash":"41faa38bfa1cf77516dc7d2021d9a1b280fdb05c","modified":1601898408846},{"_id":"public/2020/10/05/2020-03-18-generalABC/setup_model2.gif","hash":"cfe5c89a2b130ee3022944b95bf48a184bf855bb","modified":1601898408847},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/step2.png","hash":"f083cc62bdbceaa7841d58b5ca6391b98c75b67d","modified":1601898408878},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/step1.png","hash":"0651aeec6e745d56e82cbce37c772860e19b958f","modified":1601898408878},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/step4.png","hash":"711f5160a7362eccb039cf22c6f0f6247732db15","modified":1601898408878},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/step5.png","hash":"a63831c50ca8613ffa826fc6e07a6f3b75eda72c","modified":1601898408884},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/step3.png","hash":"649ae940377bf729b5997e82435eca95462b108e","modified":1601898408885},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/step6.png","hash":"35a833df81cbb285569edf65deeede0394bd749b","modified":1601898408885},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/step8.png","hash":"3c17f96e1ae34c613775cfa48b88d2347ce32143","modified":1601898408885},{"_id":"public/2020/10/05/2019-02-08-Machinelearningseries2/step7.png","hash":"8e86e2988358ba7ca17eff5b97c27560b8d73e31","modified":1601898408886},{"_id":"public/2020/10/05/2019-06-21-TPmodel/guixianren1.jpg","hash":"ed62f2db9f2ba7b6257feaabca909a4560b5b4b1","modified":1601898408911},{"_id":"public/2020/10/05/2019-06-21-TPmodel/guixianren_s.png","hash":"ac23a18f63625f176858319f7aa82cd470b7b97e","modified":1601898408935},{"_id":"public/2020/10/05/2019-06-21-ABCalgorithm/MCMC3chains_test3.mp4","hash":"15749ae0ca8e6e6541d54680466d0976454fd81e","modified":1601898409136},{"_id":"public/2020/10/05/2019-06-21-TPmodel/singlespecies5.mp4","hash":"8df799aec096d3ac68b64302b0124567dd866084","modified":1601898409388},{"_id":"public/2020/10/05/2019-06-21-TPmodel/multi6species15.mp4","hash":"2361444126185c5a4cad48a95c6acef6415f4fc5","modified":1601898409396}],"Category":[{"name":"Research","_id":"ckfwgy1170004nj392l3if9wh"},{"name":"Blogging","_id":"ckfwgy126000anj39l4uxd8x9"},{"name":"R","_id":"ckfwgy12h000inj39o8utf2iw"},{"name":"Phd projects","parent":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy12p000onj39zidcws5k"},{"name":"R","parent":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy12s000unj39ro1mki1q"},{"name":"Numerical method","parent":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy1320010nj39bqek7v5a"},{"name":"Blogging tech","parent":"ckfwgy126000anj39l4uxd8x9","_id":"ckfwgy13d0015nj39hkl6e4nu"},{"name":"Bash","parent":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy13i001anj39x8mbvsho"},{"name":"Stochastic differential equations","parent":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy13o001pnj39ikcpum8n"},{"name":"Data visualization","parent":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy13q001unj39lo7rxww5"},{"name":"phylogeny function","parent":"ckfwgy12h000inj39o8utf2iw","_id":"ckfwgy13t0021nj39co4qiqpm"},{"name":"GPU programming","parent":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy13x002cnj39q8p82qxm"},{"name":"phylogeny function","parent":"ckfwgy12s000unj39ro1mki1q","_id":"ckfwgy142002lnj39gnga7fmz"},{"name":"Machine learning","parent":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy144002rnj39buy4e0ew"},{"name":"Algorithm","parent":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy1460034nj3935gafyil"},{"name":"Python","parent":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy1470036nj39sed0irko"},{"name":"sed","parent":"ckfwgy13i001anj39x8mbvsho","_id":"ckfwgy14a0039nj39pc8en4p6"},{"name":"trait-population model","parent":"ckfwgy12p000onj39zidcws5k","_id":"ckfwgy14i003enj39dn74je9i"},{"name":"Modeling","parent":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy15h003mnj39jg0i8309"},{"name":"BB life","_id":"ckfwgy15o003wnj39xvk5gpdg"},{"name":"R","parent":"ckfwgy13q001unj39lo7rxww5","_id":"ckfwgy15p0042nj393daq1n7k"},{"name":"pyCUDA","parent":"ckfwgy13x002cnj39q8p82qxm","_id":"ckfwgy191004jnj39fgeibe1h"},{"name":"Deep learning","parent":"ckfwgy144002rnj39buy4e0ew","_id":"ckfwgy1980051nj39p5r87mkh"},{"name":"ABC","parent":"ckfwgy1460034nj3935gafyil","_id":"ckfwgy19e005inj39fcdzwrg3"},{"name":"GUI","parent":"ckfwgy1470036nj39sed0irko","_id":"ckfwgy19f005mnj394qtqgcvt"},{"name":"ggradar2","parent":"ckfwgy15p0042nj393daq1n7k","_id":"ckfwgy19i005unj390s2ncq21"}],"Data":[],"Page":[{"title":"categories","date":"2018-10-22T15:11:12.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2018-10-22 17:11:12\ntype: \"categories\"\n---\n","updated":"2020-10-05T11:30:59.304Z","path":"categories/index.html","comments":1,"layout":"page","_id":"ckfwgy10x0000nj396u8hwhwi","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"About me","date":"2018-10-22T15:08:14.000Z","_content":"I am Liang Xu, a PhD student in theoretical biology in University of Groningen. I am focusing on modeling species interactions in macro eco-evolutionary science. I am interested in mathematical models as well as deep learning. You can find my CV [**HERE**](./Liang_CV.pdf).\n\n## Research Interest\n A major challenge in ecology is the need for a better theoretical framework for understanding how species assemblages (ecological communities) arise, why some are species-rich and others species-poor, and why some species are present or dominant whereas others are not. I am interested in how to construct theoretical models to mimic the evolutionary processes and invent efficient tools with explanatory power to the evolutionary issues. This field involves mathematical modeling, Eco-evolutionary science, \n\n## Education\n**2015 - 2020**  University of Groningen. PhD in Eco-evolution; \n                      **Dissertation thesis**: [Modelling species interactions in macroevolution and macroecology](https://doi.org/10.33612/diss.125954510).\n\n**2008 - 2010** Hong Kong University. Master of Philosophy in Mathematics; Epidemic models of virus infection\n\n**2004 - 2008** Beijing Normal University. Bachelor in Mathematics\n\n## Work Experience\n**2010 - 2015** Chongqing University of Technology & Sciences. Lecturer in Department of Mathematics and Physics.\n\n## PhD Projects \n* **Inferring local diversity-dependence**. \nIt is still hotly debated that whether there exists ecological limit to diversity. A diversity-dependent diversification model has been developed to infer diversity-dependent signal. However, the model ignores local information. In this project, we aim to model the evolutionary processes incorporating the local details and check that if we can still detect the local diversity-dependence. \n* **Inferring the effect of species interactions on trait evolution**. \n  Ecology and evolution jointly help to form the pattern of traits of species. We construct an eco-evolutionary framework combing both ecological interaction and evolutionary history to describe how traits of species evolve under environmental stabilizing selection and species interactions. \n* **A spatial phylogenetic Jancen-Connell extension to the neutral theory of species diversity**.\n  The neutral theory of species diversity opens a new window to explain species assembly. However, the neutral assumption that all changes in distribution and abundance occur because of purely random variation in births, deaths, migration and speciation violates the recognition of the importance in species differences. Here, we focus on tree species and develop a spatial phylogenetic Jansen-Connell extension to the neutral theory. We aim to explore to what extent the additional mechanism indeed affects species assembly.\n\n## Publication\n**Xu, L.**, & Etienne, R. S. (2018). Detecting local diversity-dependence in diversification. Evolution, 72(6), 1294-1305. DOI: 10.1111/evo.13482 \n\n**Xu, L.**, S. van Doorn, H. Hildenbrandt and R. S. Etienne (2020). Inferring the effect of species interactions on trait evolution. Systematic Biology. Under review. \n\n**Xu, L.**, H. Hildenbrandt and R. S. Etienne (2020). Incorporating eco-evolutionary interactions into a spatially explicit phylogenetic Janzen-Connell model predicts realistic macroecological and macroevolutionary patterns. To be submitted.","source":"about/index.md","raw":"---\ntitle: About me\ndate: 2018-10-22 17:08:14\n---\nI am Liang Xu, a PhD student in theoretical biology in University of Groningen. I am focusing on modeling species interactions in macro eco-evolutionary science. I am interested in mathematical models as well as deep learning. You can find my CV [**HERE**](./Liang_CV.pdf).\n\n## Research Interest\n A major challenge in ecology is the need for a better theoretical framework for understanding how species assemblages (ecological communities) arise, why some are species-rich and others species-poor, and why some species are present or dominant whereas others are not. I am interested in how to construct theoretical models to mimic the evolutionary processes and invent efficient tools with explanatory power to the evolutionary issues. This field involves mathematical modeling, Eco-evolutionary science, \n\n## Education\n**2015 - 2020**  University of Groningen. PhD in Eco-evolution; \n                      **Dissertation thesis**: [Modelling species interactions in macroevolution and macroecology](https://doi.org/10.33612/diss.125954510).\n\n**2008 - 2010** Hong Kong University. Master of Philosophy in Mathematics; Epidemic models of virus infection\n\n**2004 - 2008** Beijing Normal University. Bachelor in Mathematics\n\n## Work Experience\n**2010 - 2015** Chongqing University of Technology & Sciences. Lecturer in Department of Mathematics and Physics.\n\n## PhD Projects \n* **Inferring local diversity-dependence**. \nIt is still hotly debated that whether there exists ecological limit to diversity. A diversity-dependent diversification model has been developed to infer diversity-dependent signal. However, the model ignores local information. In this project, we aim to model the evolutionary processes incorporating the local details and check that if we can still detect the local diversity-dependence. \n* **Inferring the effect of species interactions on trait evolution**. \n  Ecology and evolution jointly help to form the pattern of traits of species. We construct an eco-evolutionary framework combing both ecological interaction and evolutionary history to describe how traits of species evolve under environmental stabilizing selection and species interactions. \n* **A spatial phylogenetic Jancen-Connell extension to the neutral theory of species diversity**.\n  The neutral theory of species diversity opens a new window to explain species assembly. However, the neutral assumption that all changes in distribution and abundance occur because of purely random variation in births, deaths, migration and speciation violates the recognition of the importance in species differences. Here, we focus on tree species and develop a spatial phylogenetic Jansen-Connell extension to the neutral theory. We aim to explore to what extent the additional mechanism indeed affects species assembly.\n\n## Publication\n**Xu, L.**, & Etienne, R. S. (2018). Detecting local diversity-dependence in diversification. Evolution, 72(6), 1294-1305. DOI: 10.1111/evo.13482 \n\n**Xu, L.**, S. van Doorn, H. Hildenbrandt and R. S. Etienne (2020). Inferring the effect of species interactions on trait evolution. Systematic Biology. Under review. \n\n**Xu, L.**, H. Hildenbrandt and R. S. Etienne (2020). Incorporating eco-evolutionary interactions into a spatially explicit phylogenetic Janzen-Connell model predicts realistic macroecological and macroevolutionary patterns. To be submitted.","updated":"2020-10-05T11:30:59.303Z","path":"about/index.html","comments":1,"layout":"page","_id":"ckfwgy1140002nj39zzgx9wsv","content":"<p>I am Liang Xu, a PhD student in theoretical biology in University of Groningen. I am focusing on modeling species interactions in macro eco-evolutionary science. I am interested in mathematical models as well as deep learning. You can find my CV <a href=\"./Liang_CV.pdf\"><strong>HERE</strong></a>.</p>\n<h2 id=\"Research-Interest\"><a href=\"#Research-Interest\" class=\"headerlink\" title=\"Research Interest\"></a>Research Interest</h2><p> A major challenge in ecology is the need for a better theoretical framework for understanding how species assemblages (ecological communities) arise, why some are species-rich and others species-poor, and why some species are present or dominant whereas others are not. I am interested in how to construct theoretical models to mimic the evolutionary processes and invent efficient tools with explanatory power to the evolutionary issues. This field involves mathematical modeling, Eco-evolutionary science, </p>\n<h2 id=\"Education\"><a href=\"#Education\" class=\"headerlink\" title=\"Education\"></a>Education</h2><p><strong>2015 - 2020</strong>  University of Groningen. PhD in Eco-evolution;<br>                      <strong>Dissertation thesis</strong>: <a href=\"https://doi.org/10.33612/diss.125954510\" target=\"_blank\" rel=\"noopener\">Modelling species interactions in macroevolution and macroecology</a>.</p>\n<p><strong>2008 - 2010</strong> Hong Kong University. Master of Philosophy in Mathematics; Epidemic models of virus infection</p>\n<p><strong>2004 - 2008</strong> Beijing Normal University. Bachelor in Mathematics</p>\n<h2 id=\"Work-Experience\"><a href=\"#Work-Experience\" class=\"headerlink\" title=\"Work Experience\"></a>Work Experience</h2><p><strong>2010 - 2015</strong> Chongqing University of Technology &amp; Sciences. Lecturer in Department of Mathematics and Physics.</p>\n<h2 id=\"PhD-Projects\"><a href=\"#PhD-Projects\" class=\"headerlink\" title=\"PhD Projects\"></a>PhD Projects</h2><ul>\n<li><strong>Inferring local diversity-dependence</strong>.<br>It is still hotly debated that whether there exists ecological limit to diversity. A diversity-dependent diversification model has been developed to infer diversity-dependent signal. However, the model ignores local information. In this project, we aim to model the evolutionary processes incorporating the local details and check that if we can still detect the local diversity-dependence. </li>\n<li><strong>Inferring the effect of species interactions on trait evolution</strong>.<br>Ecology and evolution jointly help to form the pattern of traits of species. We construct an eco-evolutionary framework combing both ecological interaction and evolutionary history to describe how traits of species evolve under environmental stabilizing selection and species interactions. </li>\n<li><strong>A spatial phylogenetic Jancen-Connell extension to the neutral theory of species diversity</strong>.<br>The neutral theory of species diversity opens a new window to explain species assembly. However, the neutral assumption that all changes in distribution and abundance occur because of purely random variation in births, deaths, migration and speciation violates the recognition of the importance in species differences. Here, we focus on tree species and develop a spatial phylogenetic Jansen-Connell extension to the neutral theory. We aim to explore to what extent the additional mechanism indeed affects species assembly.</li>\n</ul>\n<h2 id=\"Publication\"><a href=\"#Publication\" class=\"headerlink\" title=\"Publication\"></a>Publication</h2><p><strong>Xu, L.</strong>, &amp; Etienne, R. S. (2018). Detecting local diversity-dependence in diversification. Evolution, 72(6), 1294-1305. DOI: 10.1111/evo.13482 </p>\n<p><strong>Xu, L.</strong>, S. van Doorn, H. Hildenbrandt and R. S. Etienne (2020). Inferring the effect of species interactions on trait evolution. Systematic Biology. Under review. </p>\n<p><strong>Xu, L.</strong>, H. Hildenbrandt and R. S. Etienne (2020). Incorporating eco-evolutionary interactions into a spatially explicit phylogenetic Janzen-Connell model predicts realistic macroecological and macroevolutionary patterns. To be submitted.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>I am Liang Xu, a PhD student in theoretical biology in University of Groningen. I am focusing on modeling species interactions in macro eco-evolutionary science. I am interested in mathematical models as well as deep learning. You can find my CV <a href=\"./Liang_CV.pdf\"><strong>HERE</strong></a>.</p>\n<h2 id=\"Research-Interest\"><a href=\"#Research-Interest\" class=\"headerlink\" title=\"Research Interest\"></a>Research Interest</h2><p> A major challenge in ecology is the need for a better theoretical framework for understanding how species assemblages (ecological communities) arise, why some are species-rich and others species-poor, and why some species are present or dominant whereas others are not. I am interested in how to construct theoretical models to mimic the evolutionary processes and invent efficient tools with explanatory power to the evolutionary issues. This field involves mathematical modeling, Eco-evolutionary science, </p>\n<h2 id=\"Education\"><a href=\"#Education\" class=\"headerlink\" title=\"Education\"></a>Education</h2><p><strong>2015 - 2020</strong>  University of Groningen. PhD in Eco-evolution;<br>                      <strong>Dissertation thesis</strong>: <a href=\"https://doi.org/10.33612/diss.125954510\" target=\"_blank\" rel=\"noopener\">Modelling species interactions in macroevolution and macroecology</a>.</p>\n<p><strong>2008 - 2010</strong> Hong Kong University. Master of Philosophy in Mathematics; Epidemic models of virus infection</p>\n<p><strong>2004 - 2008</strong> Beijing Normal University. Bachelor in Mathematics</p>\n<h2 id=\"Work-Experience\"><a href=\"#Work-Experience\" class=\"headerlink\" title=\"Work Experience\"></a>Work Experience</h2><p><strong>2010 - 2015</strong> Chongqing University of Technology &amp; Sciences. Lecturer in Department of Mathematics and Physics.</p>\n<h2 id=\"PhD-Projects\"><a href=\"#PhD-Projects\" class=\"headerlink\" title=\"PhD Projects\"></a>PhD Projects</h2><ul>\n<li><strong>Inferring local diversity-dependence</strong>.<br>It is still hotly debated that whether there exists ecological limit to diversity. A diversity-dependent diversification model has been developed to infer diversity-dependent signal. However, the model ignores local information. In this project, we aim to model the evolutionary processes incorporating the local details and check that if we can still detect the local diversity-dependence. </li>\n<li><strong>Inferring the effect of species interactions on trait evolution</strong>.<br>Ecology and evolution jointly help to form the pattern of traits of species. We construct an eco-evolutionary framework combing both ecological interaction and evolutionary history to describe how traits of species evolve under environmental stabilizing selection and species interactions. </li>\n<li><strong>A spatial phylogenetic Jancen-Connell extension to the neutral theory of species diversity</strong>.<br>The neutral theory of species diversity opens a new window to explain species assembly. However, the neutral assumption that all changes in distribution and abundance occur because of purely random variation in births, deaths, migration and speciation violates the recognition of the importance in species differences. Here, we focus on tree species and develop a spatial phylogenetic Jansen-Connell extension to the neutral theory. We aim to explore to what extent the additional mechanism indeed affects species assembly.</li>\n</ul>\n<h2 id=\"Publication\"><a href=\"#Publication\" class=\"headerlink\" title=\"Publication\"></a>Publication</h2><p><strong>Xu, L.</strong>, &amp; Etienne, R. S. (2018). Detecting local diversity-dependence in diversification. Evolution, 72(6), 1294-1305. DOI: 10.1111/evo.13482 </p>\n<p><strong>Xu, L.</strong>, S. van Doorn, H. Hildenbrandt and R. S. Etienne (2020). Inferring the effect of species interactions on trait evolution. Systematic Biology. Under review. </p>\n<p><strong>Xu, L.</strong>, H. Hildenbrandt and R. S. Etienne (2020). Incorporating eco-evolutionary interactions into a spatially explicit phylogenetic Janzen-Connell model predicts realistic macroecological and macroevolutionary patterns. To be submitted.</p>\n"},{"title":"All tags","date":"2018-10-22T15:04:18.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: All tags\ndate: 2018-10-22 17:04:18\ntype: \"tags\"\n---\n","updated":"2020-10-05T11:30:59.304Z","path":"tags/index.html","comments":1,"layout":"page","_id":"ckfwgy11b0006nj398sccb0os","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"PhD Project 1-Detecting local diversity-dependence in diversification","_content":"\nWelcome to [Liang&apos;s blog](https://xl0418.github.io/)! This is my first PhD project since 2015 joining [Etienne&apos;s lab](https://www.rug.nl/staff/r.s.etienne/). You will find a brief introduction to this project. The code for the model can be found on my [GitHub](https://github.com/xl0418/code).\n\nThis my first formal research project starting from December of 2015 to February 2018 when the paper gets published. It seems to take quite a long time. Because I spent some time to switch from mathematics to evolutionary science and writing paper costs time as well. Anyway, to commemorate the first paper. \n\n<!--more-->\n## Abstract\nWhether there are ecological limits to species diversification is a hotly debated topic. Molecular phylogenies show slowdowns in lineage accumulation, suggesting that speciation rates decline with increasing diversity. A maximum likelihood method to detect diversity-dependent diversification from phylogenetic branching times exists, but it assumes that diversity-dependence is a global phenomenon and therefore ignores that the underlying species interactions are mostly local, and not all species in the phylogeny co-occur locally. Here, we explore whether this maximum likelihood method based on the non-spatial diversity-dependence model can detect local diversity-dependence, by applying it to phylogenies, simulated with a spatial stochastic model of local-diversity-dependent speciation, extinction and dispersal between two local communities. We find that type I errors (falsely detecting diversity-dependence) are low, and the power to detect diversity-dependence is high when dispersal rates are not too low. Interestingly, when dispersal is high the power to detect diversity-dependence is even higher than in the non-spatial model. Moreover, estimates of intrinsic speciation rate, extinction rate and ecological limit strongly depend on dispersal rate. We conclude that the non-spatial diversity-dependent approach can be used to detect diversity-dependence in clades of species that live in not too disconnected areas, but parameter estimates must be interpreted cautiously.\n\n## Model\nWe have developed a spatial diversity-dependent diversification model to explore if the global version of the diversity-dependence approach could detect the diversity-dependent signal on the spatial scenario. \n\n## Generating trees\nThe idea is simple. For simplicity, we build a two-location model and let species evolve (speciate: give birth to a new species/ extinction: one species goes extinct) in the regime. A pack of generated trees under different speciation rates and extinction rates and dispersal rates is like this (Scenario 2, see details in our paper):\n![Generating trees](2018-10-22-IntrotoPro1/Trees_S2.jpg)\n\n## Applying the bootstrapping analysis\nWe exploit the bootstrapping analysis to examine the power of the global approah to detect the signal of diversity-dependence on the spatial structered trees. The following tables show the power of the global tool for signal detection under different parameter combinations and scenarios.\n![Power of the method](2018-10-22-IntrotoPro1/Powertable.jpg)\n\n## Parameter inference\nAs due to the complexity of the model, an analytical likelihood function is not possible to achieve. Therefore, we test if the global likelihood approach can recover the parameters on the spatial model or show some patterns.\n![Parameter inference](2018-10-22-IntrotoPro1/Est_S2VS.jpg)\n\n## Conclusion\nFrom the results above, the conclusion is clear. Please see our paper:-)\n\n## Reference\nXu, L., & Etienne, R. S. (2018). Detecting local diversity-dependence in diversification. Evolution, 72(6), 1294-1305. DOI: 10.1111/evo.13482 \n  \nEtienne, R. S., Pigot, A. L., & Phillimore, A. B. (2016). How reliably can we infer diversity-dependent diversification from phylogenies? Methods in ecology and evolution, 7(9), 1092-1099. DOI: 10.1111/2041-210X.12565\n\nEtienne, R. S., Haegeman, B., Stadler, T., Aze, T., Pearson, P. N., Purvis, A., & Phillimore, A. B. (2012). Diversity-dependence brings molecular phylogenies closer to agreement with the fossil record. Proceedings of the Royal Society of London. Series B, Biological Sciences, 279(1732), 1300-1309. DOI: 10.1098/rspb.2011.1439\n","source":"_posts/2018-10-22-IntrotoPro1.md","raw":"---\ntitle: PhD Project 1-Detecting local diversity-dependence in diversification\ncategories: [Research,Phd projects]\ntags: [evolution,mathematical modeling,diversity-dependence]\n---\n\nWelcome to [Liang&apos;s blog](https://xl0418.github.io/)! This is my first PhD project since 2015 joining [Etienne&apos;s lab](https://www.rug.nl/staff/r.s.etienne/). You will find a brief introduction to this project. The code for the model can be found on my [GitHub](https://github.com/xl0418/code).\n\nThis my first formal research project starting from December of 2015 to February 2018 when the paper gets published. It seems to take quite a long time. Because I spent some time to switch from mathematics to evolutionary science and writing paper costs time as well. Anyway, to commemorate the first paper. \n\n<!--more-->\n## Abstract\nWhether there are ecological limits to species diversification is a hotly debated topic. Molecular phylogenies show slowdowns in lineage accumulation, suggesting that speciation rates decline with increasing diversity. A maximum likelihood method to detect diversity-dependent diversification from phylogenetic branching times exists, but it assumes that diversity-dependence is a global phenomenon and therefore ignores that the underlying species interactions are mostly local, and not all species in the phylogeny co-occur locally. Here, we explore whether this maximum likelihood method based on the non-spatial diversity-dependence model can detect local diversity-dependence, by applying it to phylogenies, simulated with a spatial stochastic model of local-diversity-dependent speciation, extinction and dispersal between two local communities. We find that type I errors (falsely detecting diversity-dependence) are low, and the power to detect diversity-dependence is high when dispersal rates are not too low. Interestingly, when dispersal is high the power to detect diversity-dependence is even higher than in the non-spatial model. Moreover, estimates of intrinsic speciation rate, extinction rate and ecological limit strongly depend on dispersal rate. We conclude that the non-spatial diversity-dependent approach can be used to detect diversity-dependence in clades of species that live in not too disconnected areas, but parameter estimates must be interpreted cautiously.\n\n## Model\nWe have developed a spatial diversity-dependent diversification model to explore if the global version of the diversity-dependence approach could detect the diversity-dependent signal on the spatial scenario. \n\n## Generating trees\nThe idea is simple. For simplicity, we build a two-location model and let species evolve (speciate: give birth to a new species/ extinction: one species goes extinct) in the regime. A pack of generated trees under different speciation rates and extinction rates and dispersal rates is like this (Scenario 2, see details in our paper):\n![Generating trees](2018-10-22-IntrotoPro1/Trees_S2.jpg)\n\n## Applying the bootstrapping analysis\nWe exploit the bootstrapping analysis to examine the power of the global approah to detect the signal of diversity-dependence on the spatial structered trees. The following tables show the power of the global tool for signal detection under different parameter combinations and scenarios.\n![Power of the method](2018-10-22-IntrotoPro1/Powertable.jpg)\n\n## Parameter inference\nAs due to the complexity of the model, an analytical likelihood function is not possible to achieve. Therefore, we test if the global likelihood approach can recover the parameters on the spatial model or show some patterns.\n![Parameter inference](2018-10-22-IntrotoPro1/Est_S2VS.jpg)\n\n## Conclusion\nFrom the results above, the conclusion is clear. Please see our paper:-)\n\n## Reference\nXu, L., & Etienne, R. S. (2018). Detecting local diversity-dependence in diversification. Evolution, 72(6), 1294-1305. DOI: 10.1111/evo.13482 \n  \nEtienne, R. S., Pigot, A. L., & Phillimore, A. B. (2016). How reliably can we infer diversity-dependent diversification from phylogenies? Methods in ecology and evolution, 7(9), 1092-1099. DOI: 10.1111/2041-210X.12565\n\nEtienne, R. S., Haegeman, B., Stadler, T., Aze, T., Pearson, P. N., Purvis, A., & Phillimore, A. B. (2012). Diversity-dependence brings molecular phylogenies closer to agreement with the fossil record. Proceedings of the Royal Society of London. Series B, Biological Sciences, 279(1732), 1300-1309. DOI: 10.1098/rspb.2011.1439\n","slug":"2018-10-22-IntrotoPro1","published":1,"date":"2020-10-05T11:30:58.197Z","updated":"2020-10-05T11:30:58.200Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy10z0001nj397pmdm7sw","content":"<p>Welcome to <a href=\"https://xl0418.github.io/\">Liang&apos;s blog</a>! This is my first PhD project since 2015 joining <a href=\"https://www.rug.nl/staff/r.s.etienne/\" target=\"_blank\" rel=\"noopener\">Etienne&apos;s lab</a>. You will find a brief introduction to this project. The code for the model can be found on my <a href=\"https://github.com/xl0418/code\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<p>This my first formal research project starting from December of 2015 to February 2018 when the paper gets published. It seems to take quite a long time. Because I spent some time to switch from mathematics to evolutionary science and writing paper costs time as well. Anyway, to commemorate the first paper. </p>\n<a id=\"more\"></a>\n<h2 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>Whether there are ecological limits to species diversification is a hotly debated topic. Molecular phylogenies show slowdowns in lineage accumulation, suggesting that speciation rates decline with increasing diversity. A maximum likelihood method to detect diversity-dependent diversification from phylogenetic branching times exists, but it assumes that diversity-dependence is a global phenomenon and therefore ignores that the underlying species interactions are mostly local, and not all species in the phylogeny co-occur locally. Here, we explore whether this maximum likelihood method based on the non-spatial diversity-dependence model can detect local diversity-dependence, by applying it to phylogenies, simulated with a spatial stochastic model of local-diversity-dependent speciation, extinction and dispersal between two local communities. We find that type I errors (falsely detecting diversity-dependence) are low, and the power to detect diversity-dependence is high when dispersal rates are not too low. Interestingly, when dispersal is high the power to detect diversity-dependence is even higher than in the non-spatial model. Moreover, estimates of intrinsic speciation rate, extinction rate and ecological limit strongly depend on dispersal rate. We conclude that the non-spatial diversity-dependent approach can be used to detect diversity-dependence in clades of species that live in not too disconnected areas, but parameter estimates must be interpreted cautiously.</p>\n<h2 id=\"Model\"><a href=\"#Model\" class=\"headerlink\" title=\"Model\"></a>Model</h2><p>We have developed a spatial diversity-dependent diversification model to explore if the global version of the diversity-dependence approach could detect the diversity-dependent signal on the spatial scenario. </p>\n<h2 id=\"Generating-trees\"><a href=\"#Generating-trees\" class=\"headerlink\" title=\"Generating trees\"></a>Generating trees</h2><p>The idea is simple. For simplicity, we build a two-location model and let species evolve (speciate: give birth to a new species/ extinction: one species goes extinct) in the regime. A pack of generated trees under different speciation rates and extinction rates and dispersal rates is like this (Scenario 2, see details in our paper):<br><img src=\"/2020/10/05/2018-10-22-IntrotoPro1/Trees_S2.jpg\" alt=\"Generating trees\"></p>\n<h2 id=\"Applying-the-bootstrapping-analysis\"><a href=\"#Applying-the-bootstrapping-analysis\" class=\"headerlink\" title=\"Applying the bootstrapping analysis\"></a>Applying the bootstrapping analysis</h2><p>We exploit the bootstrapping analysis to examine the power of the global approah to detect the signal of diversity-dependence on the spatial structered trees. The following tables show the power of the global tool for signal detection under different parameter combinations and scenarios.<br><img src=\"/2020/10/05/2018-10-22-IntrotoPro1/Powertable.jpg\" alt=\"Power of the method\"></p>\n<h2 id=\"Parameter-inference\"><a href=\"#Parameter-inference\" class=\"headerlink\" title=\"Parameter inference\"></a>Parameter inference</h2><p>As due to the complexity of the model, an analytical likelihood function is not possible to achieve. Therefore, we test if the global likelihood approach can recover the parameters on the spatial model or show some patterns.<br><img src=\"/2020/10/05/2018-10-22-IntrotoPro1/Est_S2VS.jpg\" alt=\"Parameter inference\"></p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>From the results above, the conclusion is clear. Please see our paper:-)</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p>Xu, L., &amp; Etienne, R. S. (2018). Detecting local diversity-dependence in diversification. Evolution, 72(6), 1294-1305. DOI: 10.1111/evo.13482 </p>\n<p>Etienne, R. S., Pigot, A. L., &amp; Phillimore, A. B. (2016). How reliably can we infer diversity-dependent diversification from phylogenies? Methods in ecology and evolution, 7(9), 1092-1099. DOI: 10.1111/2041-210X.12565</p>\n<p>Etienne, R. S., Haegeman, B., Stadler, T., Aze, T., Pearson, P. N., Purvis, A., &amp; Phillimore, A. B. (2012). Diversity-dependence brings molecular phylogenies closer to agreement with the fossil record. Proceedings of the Royal Society of London. Series B, Biological Sciences, 279(1732), 1300-1309. DOI: 10.1098/rspb.2011.1439</p>\n","site":{"data":{}},"excerpt":"<p>Welcome to <a href=\"https://xl0418.github.io/\">Liang&apos;s blog</a>! This is my first PhD project since 2015 joining <a href=\"https://www.rug.nl/staff/r.s.etienne/\" target=\"_blank\" rel=\"noopener\">Etienne&apos;s lab</a>. You will find a brief introduction to this project. The code for the model can be found on my <a href=\"https://github.com/xl0418/code\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<p>This my first formal research project starting from December of 2015 to February 2018 when the paper gets published. It seems to take quite a long time. Because I spent some time to switch from mathematics to evolutionary science and writing paper costs time as well. Anyway, to commemorate the first paper. </p>","more":"<h2 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>Whether there are ecological limits to species diversification is a hotly debated topic. Molecular phylogenies show slowdowns in lineage accumulation, suggesting that speciation rates decline with increasing diversity. A maximum likelihood method to detect diversity-dependent diversification from phylogenetic branching times exists, but it assumes that diversity-dependence is a global phenomenon and therefore ignores that the underlying species interactions are mostly local, and not all species in the phylogeny co-occur locally. Here, we explore whether this maximum likelihood method based on the non-spatial diversity-dependence model can detect local diversity-dependence, by applying it to phylogenies, simulated with a spatial stochastic model of local-diversity-dependent speciation, extinction and dispersal between two local communities. We find that type I errors (falsely detecting diversity-dependence) are low, and the power to detect diversity-dependence is high when dispersal rates are not too low. Interestingly, when dispersal is high the power to detect diversity-dependence is even higher than in the non-spatial model. Moreover, estimates of intrinsic speciation rate, extinction rate and ecological limit strongly depend on dispersal rate. We conclude that the non-spatial diversity-dependent approach can be used to detect diversity-dependence in clades of species that live in not too disconnected areas, but parameter estimates must be interpreted cautiously.</p>\n<h2 id=\"Model\"><a href=\"#Model\" class=\"headerlink\" title=\"Model\"></a>Model</h2><p>We have developed a spatial diversity-dependent diversification model to explore if the global version of the diversity-dependence approach could detect the diversity-dependent signal on the spatial scenario. </p>\n<h2 id=\"Generating-trees\"><a href=\"#Generating-trees\" class=\"headerlink\" title=\"Generating trees\"></a>Generating trees</h2><p>The idea is simple. For simplicity, we build a two-location model and let species evolve (speciate: give birth to a new species/ extinction: one species goes extinct) in the regime. A pack of generated trees under different speciation rates and extinction rates and dispersal rates is like this (Scenario 2, see details in our paper):<br><img src=\"/2020/10/05/2018-10-22-IntrotoPro1/Trees_S2.jpg\" alt=\"Generating trees\"></p>\n<h2 id=\"Applying-the-bootstrapping-analysis\"><a href=\"#Applying-the-bootstrapping-analysis\" class=\"headerlink\" title=\"Applying the bootstrapping analysis\"></a>Applying the bootstrapping analysis</h2><p>We exploit the bootstrapping analysis to examine the power of the global approah to detect the signal of diversity-dependence on the spatial structered trees. The following tables show the power of the global tool for signal detection under different parameter combinations and scenarios.<br><img src=\"/2020/10/05/2018-10-22-IntrotoPro1/Powertable.jpg\" alt=\"Power of the method\"></p>\n<h2 id=\"Parameter-inference\"><a href=\"#Parameter-inference\" class=\"headerlink\" title=\"Parameter inference\"></a>Parameter inference</h2><p>As due to the complexity of the model, an analytical likelihood function is not possible to achieve. Therefore, we test if the global likelihood approach can recover the parameters on the spatial model or show some patterns.<br><img src=\"/2020/10/05/2018-10-22-IntrotoPro1/Est_S2VS.jpg\" alt=\"Parameter inference\"></p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>From the results above, the conclusion is clear. Please see our paper:-)</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p>Xu, L., &amp; Etienne, R. S. (2018). Detecting local diversity-dependence in diversification. Evolution, 72(6), 1294-1305. DOI: 10.1111/evo.13482 </p>\n<p>Etienne, R. S., Pigot, A. L., &amp; Phillimore, A. B. (2016). How reliably can we infer diversity-dependent diversification from phylogenies? Methods in ecology and evolution, 7(9), 1092-1099. DOI: 10.1111/2041-210X.12565</p>\n<p>Etienne, R. S., Haegeman, B., Stadler, T., Aze, T., Pearson, P. N., Purvis, A., &amp; Phillimore, A. B. (2012). Diversity-dependence brings molecular phylogenies closer to agreement with the fossil record. Proceedings of the Royal Society of London. Series B, Biological Sciences, 279(1732), 1300-1309. DOI: 10.1098/rspb.2011.1439</p>"},{"title":"Blogging tech","_content":"\nI am using [hexo](https://hexo.io/zh-cn/index.html) for blogging. The blog website is built on my personal [Github Page](https://xl0418.github.io/). The theme of the blog is [next](https://theme-next.iissnan.com/) which is a powerful tool to customize the blog. Here I would like to share/archive some useful tech for future use. \n\n<!--more-->\n\n## Optimizing the theme\nTwo posts are recommended here on [zhihu](https://zhuanlan.zhihu.com/p/30836436) and [CSDN](https://blog.csdn.net/qq_33699981/article/details/72716951). They almost contain all the custom adjustments for the theme. Of course, the official website [next](https://theme-next.iissnan.com/) is a good choice as well. \nAdvance tech to make your blog prettier is on this [website](https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html). Huge amount of tips and tricks are shown. \n\n## Materials\nTo look for some materials like pictures, icons, you can find a lot form [easyicon](https://www.easyicon.net/) and [fontawesome](https://fontawesome.com/) **for free**.","source":"_posts/2018-10-23-Blogging tech.md","raw":"---\ntitle: Blogging tech\ncategories: [Blogging, Blogging tech]\ntags: [Tech]\n---\n\nI am using [hexo](https://hexo.io/zh-cn/index.html) for blogging. The blog website is built on my personal [Github Page](https://xl0418.github.io/). The theme of the blog is [next](https://theme-next.iissnan.com/) which is a powerful tool to customize the blog. Here I would like to share/archive some useful tech for future use. \n\n<!--more-->\n\n## Optimizing the theme\nTwo posts are recommended here on [zhihu](https://zhuanlan.zhihu.com/p/30836436) and [CSDN](https://blog.csdn.net/qq_33699981/article/details/72716951). They almost contain all the custom adjustments for the theme. Of course, the official website [next](https://theme-next.iissnan.com/) is a good choice as well. \nAdvance tech to make your blog prettier is on this [website](https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html). Huge amount of tips and tricks are shown. \n\n## Materials\nTo look for some materials like pictures, icons, you can find a lot form [easyicon](https://www.easyicon.net/) and [fontawesome](https://fontawesome.com/) **for free**.","slug":"2018-10-23-Blogging tech","published":1,"date":"2020-10-05T11:30:58.209Z","updated":"2020-10-05T11:30:58.211Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy1150003nj399b58fphg","content":"<p>I am using <a href=\"https://hexo.io/zh-cn/index.html\" target=\"_blank\" rel=\"noopener\">hexo</a> for blogging. The blog website is built on my personal <a href=\"https://xl0418.github.io/\">Github Page</a>. The theme of the blog is <a href=\"https://theme-next.iissnan.com/\" target=\"_blank\" rel=\"noopener\">next</a> which is a powerful tool to customize the blog. Here I would like to share/archive some useful tech for future use. </p>\n<a id=\"more\"></a>\n<h2 id=\"Optimizing-the-theme\"><a href=\"#Optimizing-the-theme\" class=\"headerlink\" title=\"Optimizing the theme\"></a>Optimizing the theme</h2><p>Two posts are recommended here on <a href=\"https://zhuanlan.zhihu.com/p/30836436\" target=\"_blank\" rel=\"noopener\">zhihu</a> and <a href=\"https://blog.csdn.net/qq_33699981/article/details/72716951\" target=\"_blank\" rel=\"noopener\">CSDN</a>. They almost contain all the custom adjustments for the theme. Of course, the official website <a href=\"https://theme-next.iissnan.com/\" target=\"_blank\" rel=\"noopener\">next</a> is a good choice as well.<br>Advance tech to make your blog prettier is on this <a href=\"https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html\" target=\"_blank\" rel=\"noopener\">website</a>. Huge amount of tips and tricks are shown. </p>\n<h2 id=\"Materials\"><a href=\"#Materials\" class=\"headerlink\" title=\"Materials\"></a>Materials</h2><p>To look for some materials like pictures, icons, you can find a lot form <a href=\"https://www.easyicon.net/\" target=\"_blank\" rel=\"noopener\">easyicon</a> and <a href=\"https://fontawesome.com/\" target=\"_blank\" rel=\"noopener\">fontawesome</a> <strong>for free</strong>.</p>\n","site":{"data":{}},"excerpt":"<p>I am using <a href=\"https://hexo.io/zh-cn/index.html\" target=\"_blank\" rel=\"noopener\">hexo</a> for blogging. The blog website is built on my personal <a href=\"https://xl0418.github.io/\">Github Page</a>. The theme of the blog is <a href=\"https://theme-next.iissnan.com/\" target=\"_blank\" rel=\"noopener\">next</a> which is a powerful tool to customize the blog. Here I would like to share/archive some useful tech for future use. </p>","more":"<h2 id=\"Optimizing-the-theme\"><a href=\"#Optimizing-the-theme\" class=\"headerlink\" title=\"Optimizing the theme\"></a>Optimizing the theme</h2><p>Two posts are recommended here on <a href=\"https://zhuanlan.zhihu.com/p/30836436\" target=\"_blank\" rel=\"noopener\">zhihu</a> and <a href=\"https://blog.csdn.net/qq_33699981/article/details/72716951\" target=\"_blank\" rel=\"noopener\">CSDN</a>. They almost contain all the custom adjustments for the theme. Of course, the official website <a href=\"https://theme-next.iissnan.com/\" target=\"_blank\" rel=\"noopener\">next</a> is a good choice as well.<br>Advance tech to make your blog prettier is on this <a href=\"https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html\" target=\"_blank\" rel=\"noopener\">website</a>. Huge amount of tips and tricks are shown. </p>\n<h2 id=\"Materials\"><a href=\"#Materials\" class=\"headerlink\" title=\"Materials\"></a>Materials</h2><p>To look for some materials like pictures, icons, you can find a lot form <a href=\"https://www.easyicon.net/\" target=\"_blank\" rel=\"noopener\">easyicon</a> and <a href=\"https://fontawesome.com/\" target=\"_blank\" rel=\"noopener\">fontawesome</a> <strong>for free</strong>.</p>"},{"title":"Deriving the Fokker-Planck equation from a stochastic differential equation","top":3,"_content":"\nWriting a periodic progress report is a good way to record the process that you gain knowledge. Reviewing my progress reports in the last 3 years, it recovers my memory about the knowledge and technologies that I almost forgot as I rarely use them. From this post on, I would like to give them a review and share on the blog. Hope it can help people who are interest in and one can help me if I am wrong at somewhere. \n\n<!--more-->\n\n## Derivation\nHere I briefly present the derivation for the Fokker-Planck equation from a stochastic differential equation.\n\nGiven the stochastic process \n\n$$\ndx=a(x,t)dt+b(x,t)dW_{t}\n$$\n\nwhere \\\\(W_{t}\\\\) is a Wiener process. By Ito lemma, for any twice-differentiable scalar function \\\\(f(x)\\\\) we have \n\n$$\ndf(x)=\\left\\(a(x,t)f'(x)+\\frac{1}{2}b^{2}(x,t)f''(x)\\right\\)dt+b(x,t)f'(x)dW_{t}\n$$\n\nThe expectation of \\\\(f(x,t)\\\\) yields\n\n$$\nE(f(x))=\\int f(x)p(x,t)dx\n$$\n\nand take the derivative\n\n$$\n\\frac{dE(f(x))}{dt}=\\frac{d\\int f(x)p(x,t)dx}{dt}=\\int f(x)\\frac{\\partial p(x,t)}{\\partial t}dx \\tag{1}\n$$\n\nAlso, we could plug Eq.[1] in the expectation of \\\\(f(x)\\\\) and take the derivative yields\n\n$$\n\\frac{dE(f(x))}{dt}=\\frac{E(df(x))}{dt}=E\\left\\(a(x,t)f'(x)+\\frac{1}{2}b^{2}(x,t)f''(x)\\right\\) \\tag{2}\n$$\n\nFrom that Eq.[1] and Eq.[2] are identical, we have\n\n$$\n\\begin{align}\n\\int f(x)\\frac{\\partial p(x,t)}{\\partial t}dx\t&=\\int\\left\\(a(x,t)f'(x)+\\frac{1}{2}b^{2}(x,t)f''(x)\\right\\)p(x,t)dx \\\\\\\\\n\t& =\\int a(x,t)f'(x)p(x,t)dx+\\frac{1}{2}\\int b^{2}(x,t)f''(x)p(x,t)dx  \\\\\\\\\n\t& =-\\int f(x)\\frac{\\partial a(x,t)p(x,t)}{\\partial x}dx+\\frac{1}{2}\\int f(x)\\frac{\\partial^{2}b^{2}(x,t)p(x,t)}{\\partial x^{2}}dx  \\\\\\\\\n\t& =\\int f(x)\\left\\(-\\frac{\\partial a(x,t)p(x,t)}{\\partial x}+\\frac{1}{2}\\frac{\\partial^{2}b^{2}(x,t)p(x,t)}{\\partial x^{2}}\\right\\)dx\n\\end{align}\n$$\n\nAs \\\\(f(x)\\\\) is arbitrary, we obtain the Fokker-Planck equation in one dimension\n\n$$\n\\frac{\\partial p(x,t)}{\\partial t}=-\\frac{\\partial a(x,t)p(x,t)}{\\partial x}+\\frac{1}{2}\\frac{\\partial^{2}b^{2}(x,t)p(x,t)}{\\partial x^{2}}\n$$\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>","source":"_posts/2018-10-24-derivingFPequ.md","raw":"---\ntitle: Deriving the Fokker-Planck equation from a stochastic differential equation\ntop: 3\ncategories: [Research, Stochastic differential equations]\ntags: [the Fokker-Planck equation,stochastic differential equation,mathematical modeling,math]\n---\n\nWriting a periodic progress report is a good way to record the process that you gain knowledge. Reviewing my progress reports in the last 3 years, it recovers my memory about the knowledge and technologies that I almost forgot as I rarely use them. From this post on, I would like to give them a review and share on the blog. Hope it can help people who are interest in and one can help me if I am wrong at somewhere. \n\n<!--more-->\n\n## Derivation\nHere I briefly present the derivation for the Fokker-Planck equation from a stochastic differential equation.\n\nGiven the stochastic process \n\n$$\ndx=a(x,t)dt+b(x,t)dW_{t}\n$$\n\nwhere \\\\(W_{t}\\\\) is a Wiener process. By Ito lemma, for any twice-differentiable scalar function \\\\(f(x)\\\\) we have \n\n$$\ndf(x)=\\left\\(a(x,t)f'(x)+\\frac{1}{2}b^{2}(x,t)f''(x)\\right\\)dt+b(x,t)f'(x)dW_{t}\n$$\n\nThe expectation of \\\\(f(x,t)\\\\) yields\n\n$$\nE(f(x))=\\int f(x)p(x,t)dx\n$$\n\nand take the derivative\n\n$$\n\\frac{dE(f(x))}{dt}=\\frac{d\\int f(x)p(x,t)dx}{dt}=\\int f(x)\\frac{\\partial p(x,t)}{\\partial t}dx \\tag{1}\n$$\n\nAlso, we could plug Eq.[1] in the expectation of \\\\(f(x)\\\\) and take the derivative yields\n\n$$\n\\frac{dE(f(x))}{dt}=\\frac{E(df(x))}{dt}=E\\left\\(a(x,t)f'(x)+\\frac{1}{2}b^{2}(x,t)f''(x)\\right\\) \\tag{2}\n$$\n\nFrom that Eq.[1] and Eq.[2] are identical, we have\n\n$$\n\\begin{align}\n\\int f(x)\\frac{\\partial p(x,t)}{\\partial t}dx\t&=\\int\\left\\(a(x,t)f'(x)+\\frac{1}{2}b^{2}(x,t)f''(x)\\right\\)p(x,t)dx \\\\\\\\\n\t& =\\int a(x,t)f'(x)p(x,t)dx+\\frac{1}{2}\\int b^{2}(x,t)f''(x)p(x,t)dx  \\\\\\\\\n\t& =-\\int f(x)\\frac{\\partial a(x,t)p(x,t)}{\\partial x}dx+\\frac{1}{2}\\int f(x)\\frac{\\partial^{2}b^{2}(x,t)p(x,t)}{\\partial x^{2}}dx  \\\\\\\\\n\t& =\\int f(x)\\left\\(-\\frac{\\partial a(x,t)p(x,t)}{\\partial x}+\\frac{1}{2}\\frac{\\partial^{2}b^{2}(x,t)p(x,t)}{\\partial x^{2}}\\right\\)dx\n\\end{align}\n$$\n\nAs \\\\(f(x)\\\\) is arbitrary, we obtain the Fokker-Planck equation in one dimension\n\n$$\n\\frac{\\partial p(x,t)}{\\partial t}=-\\frac{\\partial a(x,t)p(x,t)}{\\partial x}+\\frac{1}{2}\\frac{\\partial^{2}b^{2}(x,t)p(x,t)}{\\partial x^{2}}\n$$\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>","slug":"2018-10-24-derivingFPequ","published":1,"date":"2020-10-05T11:30:58.211Z","updated":"2020-10-05T11:30:58.211Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy11c0007nj39jit1ttbs","content":"<p>Writing a periodic progress report is a good way to record the process that you gain knowledge. Reviewing my progress reports in the last 3 years, it recovers my memory about the knowledge and technologies that I almost forgot as I rarely use them. From this post on, I would like to give them a review and share on the blog. Hope it can help people who are interest in and one can help me if I am wrong at somewhere. </p>\n<a id=\"more\"></a>\n<h2 id=\"Derivation\"><a href=\"#Derivation\" class=\"headerlink\" title=\"Derivation\"></a>Derivation</h2><p>Here I briefly present the derivation for the Fokker-Planck equation from a stochastic differential equation.</p>\n<p>Given the stochastic process </p>\n<p>$$<br>dx=a(x,t)dt+b(x,t)dW_{t}<br>$$</p>\n<p>where \\(W_{t}\\) is a Wiener process. By Ito lemma, for any twice-differentiable scalar function \\(f(x)\\) we have </p>\n<p>$$<br>df(x)=\\left(a(x,t)f’(x)+\\frac{1}{2}b^{2}(x,t)f’’(x)\\right)dt+b(x,t)f’(x)dW_{t}<br>$$</p>\n<p>The expectation of \\(f(x,t)\\) yields</p>\n<p>$$<br>E(f(x))=\\int f(x)p(x,t)dx<br>$$</p>\n<p>and take the derivative</p>\n<p>$$<br>\\frac{dE(f(x))}{dt}=\\frac{d\\int f(x)p(x,t)dx}{dt}=\\int f(x)\\frac{\\partial p(x,t)}{\\partial t}dx \\tag{1}<br>$$</p>\n<p>Also, we could plug Eq.[1] in the expectation of \\(f(x)\\) and take the derivative yields</p>\n<p>$$<br>\\frac{dE(f(x))}{dt}=\\frac{E(df(x))}{dt}=E\\left(a(x,t)f’(x)+\\frac{1}{2}b^{2}(x,t)f’’(x)\\right) \\tag{2}<br>$$</p>\n<p>From that Eq.[1] and Eq.[2] are identical, we have</p>\n<p>$$<br>\\begin{align}<br>\\int f(x)\\frac{\\partial p(x,t)}{\\partial t}dx    &amp;=\\int\\left(a(x,t)f’(x)+\\frac{1}{2}b^{2}(x,t)f’’(x)\\right)p(x,t)dx \\\\<br>    &amp; =\\int a(x,t)f’(x)p(x,t)dx+\\frac{1}{2}\\int b^{2}(x,t)f’’(x)p(x,t)dx  \\\\<br>    &amp; =-\\int f(x)\\frac{\\partial a(x,t)p(x,t)}{\\partial x}dx+\\frac{1}{2}\\int f(x)\\frac{\\partial^{2}b^{2}(x,t)p(x,t)}{\\partial x^{2}}dx  \\\\<br>    &amp; =\\int f(x)\\left(-\\frac{\\partial a(x,t)p(x,t)}{\\partial x}+\\frac{1}{2}\\frac{\\partial^{2}b^{2}(x,t)p(x,t)}{\\partial x^{2}}\\right)dx<br>\\end{align}<br>$$</p>\n<p>As \\(f(x)\\) is arbitrary, we obtain the Fokker-Planck equation in one dimension</p>\n<p>$$<br>\\frac{\\partial p(x,t)}{\\partial t}=-\\frac{\\partial a(x,t)p(x,t)}{\\partial x}+\\frac{1}{2}\\frac{\\partial^{2}b^{2}(x,t)p(x,t)}{\\partial x^{2}}<br>$$</p>\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>","site":{"data":{}},"excerpt":"<p>Writing a periodic progress report is a good way to record the process that you gain knowledge. Reviewing my progress reports in the last 3 years, it recovers my memory about the knowledge and technologies that I almost forgot as I rarely use them. From this post on, I would like to give them a review and share on the blog. Hope it can help people who are interest in and one can help me if I am wrong at somewhere. </p>","more":"<h2 id=\"Derivation\"><a href=\"#Derivation\" class=\"headerlink\" title=\"Derivation\"></a>Derivation</h2><p>Here I briefly present the derivation for the Fokker-Planck equation from a stochastic differential equation.</p>\n<p>Given the stochastic process </p>\n<p>$$<br>dx=a(x,t)dt+b(x,t)dW_{t}<br>$$</p>\n<p>where \\(W_{t}\\) is a Wiener process. By Ito lemma, for any twice-differentiable scalar function \\(f(x)\\) we have </p>\n<p>$$<br>df(x)=\\left(a(x,t)f’(x)+\\frac{1}{2}b^{2}(x,t)f’’(x)\\right)dt+b(x,t)f’(x)dW_{t}<br>$$</p>\n<p>The expectation of \\(f(x,t)\\) yields</p>\n<p>$$<br>E(f(x))=\\int f(x)p(x,t)dx<br>$$</p>\n<p>and take the derivative</p>\n<p>$$<br>\\frac{dE(f(x))}{dt}=\\frac{d\\int f(x)p(x,t)dx}{dt}=\\int f(x)\\frac{\\partial p(x,t)}{\\partial t}dx \\tag{1}<br>$$</p>\n<p>Also, we could plug Eq.[1] in the expectation of \\(f(x)\\) and take the derivative yields</p>\n<p>$$<br>\\frac{dE(f(x))}{dt}=\\frac{E(df(x))}{dt}=E\\left(a(x,t)f’(x)+\\frac{1}{2}b^{2}(x,t)f’’(x)\\right) \\tag{2}<br>$$</p>\n<p>From that Eq.[1] and Eq.[2] are identical, we have</p>\n<p>$$<br>\\begin{align}<br>\\int f(x)\\frac{\\partial p(x,t)}{\\partial t}dx    &amp;=\\int\\left(a(x,t)f’(x)+\\frac{1}{2}b^{2}(x,t)f’’(x)\\right)p(x,t)dx \\\\<br>    &amp; =\\int a(x,t)f’(x)p(x,t)dx+\\frac{1}{2}\\int b^{2}(x,t)f’’(x)p(x,t)dx  \\\\<br>    &amp; =-\\int f(x)\\frac{\\partial a(x,t)p(x,t)}{\\partial x}dx+\\frac{1}{2}\\int f(x)\\frac{\\partial^{2}b^{2}(x,t)p(x,t)}{\\partial x^{2}}dx  \\\\<br>    &amp; =\\int f(x)\\left(-\\frac{\\partial a(x,t)p(x,t)}{\\partial x}+\\frac{1}{2}\\frac{\\partial^{2}b^{2}(x,t)p(x,t)}{\\partial x^{2}}\\right)dx<br>\\end{align}<br>$$</p>\n<p>As \\(f(x)\\) is arbitrary, we obtain the Fokker-Planck equation in one dimension</p>\n<p>$$<br>\\frac{\\partial p(x,t)}{\\partial t}=-\\frac{\\partial a(x,t)p(x,t)}{\\partial x}+\\frac{1}{2}\\frac{\\partial^{2}b^{2}(x,t)p(x,t)}{\\partial x^{2}}<br>$$</p>\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>"},{"title":"phylo2L function","_content":"\nI guess this function is specially useful to [our group](https://www.rug.nl/staff/r.s.etienne/) in which we play with L table. \n\nL table is an alternative way to a phylo class for phylogenetic information storage. The function `L2phylo` has been implemented in the [DDD package](https://cran.r-project.org/web/packages/DDD/index.html) that converts an L table to a phylo class. This function `phylo2L` does the conversion the other way around. Thus, if you want to apply your model to an empirical data. This may be useful to you.\n\n<!--more-->\n\n`phylo2L` function can be found [here](https://github.com/xl0418/Code/blob/99133e6e5744be7382c038edc5701cd494d8e76c/Pro2/R_p2/phylo2L.R) if you want to improve the function. I have verified it by examining if \n\n```R  \nL=phylo2L(L2phylo(L))\n``` \n\nNotice that `phylo2L` doesn't have the argument `dropextinct` as what `L2phylo` has. Because to my perspective L table should be consistent with the given phylo class. But if you have a full tree on hand and want to prune it, you can do it like this\n\n```R \nprune_phylo = L2phylo(phylo2L(full tree), dropextinct = TRUE)\nprune_L = phylo2L(prune_phylo)\n``` \n\nOr you can use [`pruneL`](https://github.com/xl0418/Code/blob/f4dfd4acc15af6855572fb4659f396cea14bb83b/Pro2/R_p2/pruneL.R) function that I have developed to prune an L table. More details on `pruneL` function can be found in this [post](https://xl0418.github.io/2018/10/24/2018-10-24-pruneLfunction/).","source":"_posts/2018-10-24-phylo2Lfunction.md","raw":"---\ntitle: phylo2L function\ncategories: [,R,phylogeny function]\ntags: [phylogeny,phylo class,L table,DDD package,pruneL,phylo2L]\n---\n\nI guess this function is specially useful to [our group](https://www.rug.nl/staff/r.s.etienne/) in which we play with L table. \n\nL table is an alternative way to a phylo class for phylogenetic information storage. The function `L2phylo` has been implemented in the [DDD package](https://cran.r-project.org/web/packages/DDD/index.html) that converts an L table to a phylo class. This function `phylo2L` does the conversion the other way around. Thus, if you want to apply your model to an empirical data. This may be useful to you.\n\n<!--more-->\n\n`phylo2L` function can be found [here](https://github.com/xl0418/Code/blob/99133e6e5744be7382c038edc5701cd494d8e76c/Pro2/R_p2/phylo2L.R) if you want to improve the function. I have verified it by examining if \n\n```R  \nL=phylo2L(L2phylo(L))\n``` \n\nNotice that `phylo2L` doesn't have the argument `dropextinct` as what `L2phylo` has. Because to my perspective L table should be consistent with the given phylo class. But if you have a full tree on hand and want to prune it, you can do it like this\n\n```R \nprune_phylo = L2phylo(phylo2L(full tree), dropextinct = TRUE)\nprune_L = phylo2L(prune_phylo)\n``` \n\nOr you can use [`pruneL`](https://github.com/xl0418/Code/blob/f4dfd4acc15af6855572fb4659f396cea14bb83b/Pro2/R_p2/pruneL.R) function that I have developed to prune an L table. More details on `pruneL` function can be found in this [post](https://xl0418.github.io/2018/10/24/2018-10-24-pruneLfunction/).","slug":"2018-10-24-phylo2Lfunction","published":1,"date":"2020-10-05T11:30:58.211Z","updated":"2020-10-05T11:30:58.212Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy11f0008nj39m0ge4p77","content":"<p>I guess this function is specially useful to <a href=\"https://www.rug.nl/staff/r.s.etienne/\" target=\"_blank\" rel=\"noopener\">our group</a> in which we play with L table. </p>\n<p>L table is an alternative way to a phylo class for phylogenetic information storage. The function <code>L2phylo</code> has been implemented in the <a href=\"https://cran.r-project.org/web/packages/DDD/index.html\" target=\"_blank\" rel=\"noopener\">DDD package</a> that converts an L table to a phylo class. This function <code>phylo2L</code> does the conversion the other way around. Thus, if you want to apply your model to an empirical data. This may be useful to you.</p>\n<a id=\"more\"></a>\n<p><code>phylo2L</code> function can be found <a href=\"https://github.com/xl0418/Code/blob/99133e6e5744be7382c038edc5701cd494d8e76c/Pro2/R_p2/phylo2L.R\" target=\"_blank\" rel=\"noopener\">here</a> if you want to improve the function. I have verified it by examining if </p>\n<pre><code class=\"R\">L=phylo2L(L2phylo(L))\n</code></pre>\n<p>Notice that <code>phylo2L</code> doesn’t have the argument <code>dropextinct</code> as what <code>L2phylo</code> has. Because to my perspective L table should be consistent with the given phylo class. But if you have a full tree on hand and want to prune it, you can do it like this</p>\n<pre><code class=\"R\">prune_phylo = L2phylo(phylo2L(full tree), dropextinct = <span class=\"literal\">TRUE</span>)\nprune_L = phylo2L(prune_phylo)\n</code></pre>\n<p>Or you can use <a href=\"https://github.com/xl0418/Code/blob/f4dfd4acc15af6855572fb4659f396cea14bb83b/Pro2/R_p2/pruneL.R\" target=\"_blank\" rel=\"noopener\"><code>pruneL</code></a> function that I have developed to prune an L table. More details on <code>pruneL</code> function can be found in this <a href=\"https://xl0418.github.io/2018/10/24/2018-10-24-pruneLfunction/\">post</a>.</p>\n","site":{"data":{}},"excerpt":"<p>I guess this function is specially useful to <a href=\"https://www.rug.nl/staff/r.s.etienne/\" target=\"_blank\" rel=\"noopener\">our group</a> in which we play with L table. </p>\n<p>L table is an alternative way to a phylo class for phylogenetic information storage. The function <code>L2phylo</code> has been implemented in the <a href=\"https://cran.r-project.org/web/packages/DDD/index.html\" target=\"_blank\" rel=\"noopener\">DDD package</a> that converts an L table to a phylo class. This function <code>phylo2L</code> does the conversion the other way around. Thus, if you want to apply your model to an empirical data. This may be useful to you.</p>","more":"<p><code>phylo2L</code> function can be found <a href=\"https://github.com/xl0418/Code/blob/99133e6e5744be7382c038edc5701cd494d8e76c/Pro2/R_p2/phylo2L.R\" target=\"_blank\" rel=\"noopener\">here</a> if you want to improve the function. I have verified it by examining if </p>\n<pre><code class=\"R\">L=phylo2L(L2phylo(L))\n</code></pre>\n<p>Notice that <code>phylo2L</code> doesn’t have the argument <code>dropextinct</code> as what <code>L2phylo</code> has. Because to my perspective L table should be consistent with the given phylo class. But if you have a full tree on hand and want to prune it, you can do it like this</p>\n<pre><code class=\"R\">prune_phylo = L2phylo(phylo2L(full tree), dropextinct = <span class=\"literal\">TRUE</span>)\nprune_L = phylo2L(prune_phylo)\n</code></pre>\n<p>Or you can use <a href=\"https://github.com/xl0418/Code/blob/f4dfd4acc15af6855572fb4659f396cea14bb83b/Pro2/R_p2/pruneL.R\" target=\"_blank\" rel=\"noopener\"><code>pruneL</code></a> function that I have developed to prune an L table. More details on <code>pruneL</code> function can be found in this <a href=\"https://xl0418.github.io/2018/10/24/2018-10-24-pruneLfunction/\">post</a>.</p>"},{"title":"pruneL function","_content":"\nI guess this function is specially useful to [our group](https://www.rug.nl/staff/r.s.etienne/) in which we play with L table. But might be less useful than `phylo2L` function :-)\n\nFollowing last post, this function `pruneL` prunes an L table by removing the extinct lineages.\n\n<!--more-->\n\n`pruneL` function can be found [here](https://github.com/xl0418/Code/blob/f4dfd4acc15af6855572fb4659f396cea14bb83b/Pro2/R_p2/pruneL.R) if you want to improve the function. There is a huge room for improvement. This version is only the first draft of the function. And I guess I should remove the argument `dropextinct`. It doesn't make any sense. What do you think?","source":"_posts/2018-10-24-pruneLfunction.md","raw":"---\ntitle: pruneL function\ncategories: [Research,R,phylogeny function]\ntags: [phylogeny,phylo class,L table,DDD package,pruneL,phylo2L]\n---\n\nI guess this function is specially useful to [our group](https://www.rug.nl/staff/r.s.etienne/) in which we play with L table. But might be less useful than `phylo2L` function :-)\n\nFollowing last post, this function `pruneL` prunes an L table by removing the extinct lineages.\n\n<!--more-->\n\n`pruneL` function can be found [here](https://github.com/xl0418/Code/blob/f4dfd4acc15af6855572fb4659f396cea14bb83b/Pro2/R_p2/pruneL.R) if you want to improve the function. There is a huge room for improvement. This version is only the first draft of the function. And I guess I should remove the argument `dropextinct`. It doesn't make any sense. What do you think?","slug":"2018-10-24-pruneLfunction","published":1,"date":"2020-10-05T11:30:58.212Z","updated":"2020-10-05T11:30:58.212Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy11h0009nj39efn5zhxl","content":"<p>I guess this function is specially useful to <a href=\"https://www.rug.nl/staff/r.s.etienne/\" target=\"_blank\" rel=\"noopener\">our group</a> in which we play with L table. But might be less useful than <code>phylo2L</code> function :-)</p>\n<p>Following last post, this function <code>pruneL</code> prunes an L table by removing the extinct lineages.</p>\n<a id=\"more\"></a>\n<p><code>pruneL</code> function can be found <a href=\"https://github.com/xl0418/Code/blob/f4dfd4acc15af6855572fb4659f396cea14bb83b/Pro2/R_p2/pruneL.R\" target=\"_blank\" rel=\"noopener\">here</a> if you want to improve the function. There is a huge room for improvement. This version is only the first draft of the function. And I guess I should remove the argument <code>dropextinct</code>. It doesn’t make any sense. What do you think?</p>\n","site":{"data":{}},"excerpt":"<p>I guess this function is specially useful to <a href=\"https://www.rug.nl/staff/r.s.etienne/\" target=\"_blank\" rel=\"noopener\">our group</a> in which we play with L table. But might be less useful than <code>phylo2L</code> function :-)</p>\n<p>Following last post, this function <code>pruneL</code> prunes an L table by removing the extinct lineages.</p>","more":"<p><code>pruneL</code> function can be found <a href=\"https://github.com/xl0418/Code/blob/f4dfd4acc15af6855572fb4659f396cea14bb83b/Pro2/R_p2/pruneL.R\" target=\"_blank\" rel=\"noopener\">here</a> if you want to improve the function. There is a huge room for improvement. This version is only the first draft of the function. And I guess I should remove the argument <code>dropextinct</code>. It doesn’t make any sense. What do you think?</p>"},{"title":"Partial least square regression","top":2,"_content":"\nI found the following interpretation to partial least square regression is much better than mine. So I cited it as below:\n>\"Partial least squares regression (PLS regression) is a statistical method that bears some relation to principal components regression; instead of finding hyperplanes of maximum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the observable variables to a new space. Because both the X and Y data are projected to new spaces, the PLS family of methods are known as bilinear factor models. Partial least squares Discriminant Analysis (PLS-DA) is a variant used when the Y is categorical.\"\n>\n>---from [Wikipedia](https://en.wikipedia.org/wiki/Partial_least_squares_regression)\n\n\n\n<!--more-->\n\n## PLS algorithm\nFor record purpose, I am deriving the PLS algorithm in this post. \n\nConsider a linear regression problem in a vector format as in Eq.[eq:1] given an observed data, which is denoted by a vector \\\\(\\boldsymbol{x}^{T}=(x_{1},x_{2},\\cdots,x_{n})\\\\)\n\n$$\n\\begin{align}\ny\t&=b_{1}x_{1}+\\cdots+b_{n}x_{n}+e \\\\\\\\ \n\t&=\\boldsymbol{x}^{T}\\boldsymbol{b}+e \\tag{1}\n\\end{align}\n$$\n\nWe are aiming to work out the unknown regression coefficients vector \\\\(\\boldsymbol{b}\\\\) for future prediction of \\\\(y\\\\) once we have the observed data \\\\(\\boldsymbol{x}\\\\). To keep the prediction as accurate as possible, we will want to make the residual error e as small as possible. This is simply the whole idea of the linear regression method. So one solution is to use training samples, that is given m observations \\\\(\\boldsymbol{y}^{T}=(y_{1},\\cdots,y_{m}),X=[x_{ij}],i=1,\\cdots,m,j=1,\\cdots,n \\\\)\nto solve \\\\(\\boldsymbol{b}\\\\) out in terms of keeping the residual error sufficiently small\n\n$$\n\\begin{pmatrix}\ny_{1}\\\\\\\\\ny_{2}\\\\\\\\\n\\vdots\\\\\\\\\ny_{m}\n\\end{pmatrix}=\\begin{pmatrix}\nx_{11} & x_{12} & \\cdots & x_{1n}\\\\\\\\\nx_{21} & x_{22} & \\cdots & x_{2n}\\\\\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\\\\\nx_{m1} & x_{m2} & \\cdots & x_{mn}\n\\end{pmatrix}\\begin{pmatrix}\nb_{1}\\\\\\\\\nb_{2}\\\\\\\\\n\\vdots\\\\\\\\\nb_{n}\n\\end{pmatrix}+\\begin{pmatrix}\ne_{1}\\\\\\\\\ne_{2}\\\\\\\\\n\\vdots\\\\\\\\\ne_{m}\n\\end{pmatrix}\\tag{2}\n$$\n\nFor comparability among different data, all the observations are assumed to be centered and normalized in advance. Now the problem is translated to an optimization problem, that is to find a least square solution for the regression coefficients such that the sum of the square errors of the residuals is minimized. For simplicity, I would like to use the matrix form for Eq.[2]\n\n$$\nY=X\\boldsymbol{b}+\\boldsymbol{e}. \\tag{3}\n$$\n\nWe want to find out \\\\(\\boldsymbol{b}\\\\) such that\n\n$$\nmin\\\\{\\boldsymbol{e}^{T}\\boldsymbol{e}\\\\}.\n$$\n\nThe least square solution for \\\\(\\boldsymbol{b}\\\\) is given by\n\n$$\n\\boldsymbol{b}=(X^{T}X)^{-1}X^{T}Y. \\tag{4}\n$$\n\nThe derivation is simply as follows:\n\n$$\nF_{\\boldsymbol{b}}=\\frac{1}{2}(X\\boldsymbol{b}-Y)^{T}(X\\boldsymbol{b}-Y) \\tag{5}\n$$\n\nSo the derivative of F_{\\boldsymbol{b}} yields\n\n$$\n\\begin{align}\n\\nabla_{\\boldsymbol{b}}F_{\\boldsymbol{b}}\t\n&=\\frac{1}{2}\\nabla_{\\boldsymbol{b}}(X\\boldsymbol{b}-Y)^{T}(X\\boldsymbol{b}-Y)\\\\\\\\\n\t&=\\frac{1}{2}\\nabla_{\\boldsymbol{b}}(\\boldsymbol{b}^{T}X^{T}-Y^{T})(X\\boldsymbol{b}-Y)\\\\\\\\\n\t&=\\frac{1}{2}\\nabla_{\\boldsymbol{b}}(\\boldsymbol{b}^{T}X^{T}X\\boldsymbol{b}-\\boldsymbol{b}^{T}X^{T}Y-Y^{T}X\\boldsymbol{b}+Y^{T}Y)\\\\\\\\\n\t&=\\frac{1}{2}\\nabla_{\\boldsymbol{b}}tr(\\boldsymbol{b}^{T}X^{T}X\\boldsymbol{b}-\\boldsymbol{b}^{T}X^{T}Y-Y^{T}X\\boldsymbol{b}+Y^{T}Y)\\\\\\\\\n\t&=\\frac{1}{2}\\nabla_{\\boldsymbol{b}}\\left\\(tr(\\boldsymbol{b}^{T}X^{T}X\\boldsymbol{b})-2tr(Y^{T}X\\boldsymbol{b})+tr(Y^{T}Y)\\right\\)\\\\\\\\\n\t&=X^{T}X\\boldsymbol{b}-X^{T}Y. \n\\end{align}\n$$\n\nBy setting the derivative to be 0, solution Eq.[4] is obtained.\nHowever, this least square solution may have problem when the training sample is not enough, that is m<n. In that case, the matrix \\\\(X^{T}X\\\\) doesn't have full rank which means it is nonsingular. \n\nTo solve this problem, we can project each measurement \\\\(\\boldsymbol{x_{i}}=(x_{i1},\\cdots,x_{in})\\\\) into a lower-dimensional subspace spanned by data \\\\(T=XW\\\\). We can think of this as forming a smaller set of features, each being the linear combination of the original set of features. These new features are also called “latent” variables. Therefore, the linear regression can be written as a linear regression system on the new latent variables\n\n$$\n\\begin{align}\nY &=TQ^{T}+F \\tag{6} \\\\\\\\\nX &=TP^{T}+E \\tag{7}\n\\end{align}\n$$\n\nwhere \\\\(P,Q\\\\) are the coefficient matrices and E,F are matrices of errors. As in partial least squares regression (PLSR), the weight matrix \\\\(W\\\\) reflects the covariance structure between the predictor and response variables. Hence, maximizing the covariance of the latent variables and the response variables \\\\(Cov(T,Y)\\\\) gives us the weight matrix \\\\(W\\\\). Once obtaining \\\\(W\\\\) and then constructing \\\\(T\\\\), \\\\(Q^{T}\\\\) is solved by the least squares solution Eq.[6]:\n\n$$\nQ^{T}=(T^{T}T)^{-1}T^{T}Y.\n$$\n\nPlug Eq.[eq:latent] into the regression equation Eq.[6], we obtain the solution of the matrix \\\\(\\boldsymbol{b}\\\\) of the coefficient in the model Eq.[3]\n\n$$\n\\boldsymbol{b}=W(T^{T}T)^{-1}T^{T}Y.\n$$\n\nSo this is the whole idea of PLS. A commonly used algorithm to compute PLSR is the nonlinear iterative partial least square (NIPALS) method. The steps are summarized as follows:\n\n\n**NIPALS**\n\n1. Normalize \\\\(m\\\\) training samples by \n\n$$\n\\begin{align}\nx_{ij} &=\\left(x_{ij}-\\bar{x} _{\\cdot\\ j}\\right)/ \\sigma _{x_{\\cdot\\ j}} \\\\\\\\\ny_{i} &=\\left(y_{i}-\\bar{y}\\right)/ \\sigma _{y}\\ \\text{for }i=1,\\cdots,m,j=1,\\cdots,n\n\\end{align}\n$$\n\n**Compute the scores**\n\n2. Compute the dominant eigenvector of \\\\(X_{0}^{T}Y_{0}Y_{0}^{T}X_{0}\\\\) and assign it to \\\\(w_{1}\\\\) (the step of maximizing the covariance). Normalize \\\\(w_{1}\\\\). The scores \\\\(t_{1}\\\\) of \\\\(X_{0}\\\\) yields\n\n$$\nt_{1}=X_{0}w_{1}.\n$$\n\nDo the same to \\\\(Y_{0}\\\\) and obtain a normalized vector \\\\(c_{1}\\\\). The scores \\\\(v_{1}\\\\) of \\\\(Y_{0}\\\\) yields\n\n$$\nv_{1}=Y_{0}c_{1}.\n$$\n\n\n**Compute the loadings**\n\n3. Based on the regression equations\n\n$$\n\\begin{align}\nX\t&=t_{1}p_{1}^{T}+E \\\\\\\\\nY\t&=t_{1}q_{1}^{T}+F,\n\\end{align}\n$$\n\nthe least square method gives us \n\n\n$$\n\\begin{align}\np_{1} &=\\frac{X^{T}t_{1}}{t_{1}t_{1}^{T}} \\\\\\\\\nq_{1} &=\\frac{Y^{T}t_{1}}{t_{1}t_{1}^{T}}.\n\\end{align}\n$$\n\n\n4. Check if \\\\(F\\\\) is sufficiently small. Otherwise, set \\\\(E\\\\) as \\\\(X_{1}\\\\), \\\\(F\\\\) as \\\\(Y_{1}\\\\). Repeat step 2-3. \n\n5. Finally, we obtain \n\n$$\n\\begin{align}\nX &=t_{1}p_{1}^{T}+t_{2}p_{2}^{T}+\\cdots+t_{n}p_{n}^{T}+E \\\\\\\\\nY &=t_{1}q_{1}^{T}+t_{2}q_{2}^{T}+\\cdots+t_{n}q_{n}^{T}+F\n\\end{align}\n$$\n\nwhich in a matrix format is as follows:\n\n$$\n\\begin{align}\nX &=TP^{T}+E \\\\\\\\\nY &=TQ^{T}+F \\\\\\\\\n\t&=XWQ^{T}+F \\\\\\\\\n\t&:=XB+F.\n\\end{align}\n$$\n\nSo, when we have a new data \\\\(x_{new}\\\\), to define the PLS components in our ABC-MCMC algorithm, we only need to compute \n\n$$\nt_{1}=x_{new}^{T}p_{1},t_{2}=x_{new}^{T}p_{2},\\cdots,t_{n}=x_{new}^{T}p_{n}\n$$\n\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>","source":"_posts/2018-10-25-PLSR.md","raw":"---\ntitle: Partial least square regression\ntop: 2\ncategories: [Research,Numerical method]\ntags: [regression,least square method,partial least square,math,machine learning]\n---\n\nI found the following interpretation to partial least square regression is much better than mine. So I cited it as below:\n>\"Partial least squares regression (PLS regression) is a statistical method that bears some relation to principal components regression; instead of finding hyperplanes of maximum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the observable variables to a new space. Because both the X and Y data are projected to new spaces, the PLS family of methods are known as bilinear factor models. Partial least squares Discriminant Analysis (PLS-DA) is a variant used when the Y is categorical.\"\n>\n>---from [Wikipedia](https://en.wikipedia.org/wiki/Partial_least_squares_regression)\n\n\n\n<!--more-->\n\n## PLS algorithm\nFor record purpose, I am deriving the PLS algorithm in this post. \n\nConsider a linear regression problem in a vector format as in Eq.[eq:1] given an observed data, which is denoted by a vector \\\\(\\boldsymbol{x}^{T}=(x_{1},x_{2},\\cdots,x_{n})\\\\)\n\n$$\n\\begin{align}\ny\t&=b_{1}x_{1}+\\cdots+b_{n}x_{n}+e \\\\\\\\ \n\t&=\\boldsymbol{x}^{T}\\boldsymbol{b}+e \\tag{1}\n\\end{align}\n$$\n\nWe are aiming to work out the unknown regression coefficients vector \\\\(\\boldsymbol{b}\\\\) for future prediction of \\\\(y\\\\) once we have the observed data \\\\(\\boldsymbol{x}\\\\). To keep the prediction as accurate as possible, we will want to make the residual error e as small as possible. This is simply the whole idea of the linear regression method. So one solution is to use training samples, that is given m observations \\\\(\\boldsymbol{y}^{T}=(y_{1},\\cdots,y_{m}),X=[x_{ij}],i=1,\\cdots,m,j=1,\\cdots,n \\\\)\nto solve \\\\(\\boldsymbol{b}\\\\) out in terms of keeping the residual error sufficiently small\n\n$$\n\\begin{pmatrix}\ny_{1}\\\\\\\\\ny_{2}\\\\\\\\\n\\vdots\\\\\\\\\ny_{m}\n\\end{pmatrix}=\\begin{pmatrix}\nx_{11} & x_{12} & \\cdots & x_{1n}\\\\\\\\\nx_{21} & x_{22} & \\cdots & x_{2n}\\\\\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\\\\\nx_{m1} & x_{m2} & \\cdots & x_{mn}\n\\end{pmatrix}\\begin{pmatrix}\nb_{1}\\\\\\\\\nb_{2}\\\\\\\\\n\\vdots\\\\\\\\\nb_{n}\n\\end{pmatrix}+\\begin{pmatrix}\ne_{1}\\\\\\\\\ne_{2}\\\\\\\\\n\\vdots\\\\\\\\\ne_{m}\n\\end{pmatrix}\\tag{2}\n$$\n\nFor comparability among different data, all the observations are assumed to be centered and normalized in advance. Now the problem is translated to an optimization problem, that is to find a least square solution for the regression coefficients such that the sum of the square errors of the residuals is minimized. For simplicity, I would like to use the matrix form for Eq.[2]\n\n$$\nY=X\\boldsymbol{b}+\\boldsymbol{e}. \\tag{3}\n$$\n\nWe want to find out \\\\(\\boldsymbol{b}\\\\) such that\n\n$$\nmin\\\\{\\boldsymbol{e}^{T}\\boldsymbol{e}\\\\}.\n$$\n\nThe least square solution for \\\\(\\boldsymbol{b}\\\\) is given by\n\n$$\n\\boldsymbol{b}=(X^{T}X)^{-1}X^{T}Y. \\tag{4}\n$$\n\nThe derivation is simply as follows:\n\n$$\nF_{\\boldsymbol{b}}=\\frac{1}{2}(X\\boldsymbol{b}-Y)^{T}(X\\boldsymbol{b}-Y) \\tag{5}\n$$\n\nSo the derivative of F_{\\boldsymbol{b}} yields\n\n$$\n\\begin{align}\n\\nabla_{\\boldsymbol{b}}F_{\\boldsymbol{b}}\t\n&=\\frac{1}{2}\\nabla_{\\boldsymbol{b}}(X\\boldsymbol{b}-Y)^{T}(X\\boldsymbol{b}-Y)\\\\\\\\\n\t&=\\frac{1}{2}\\nabla_{\\boldsymbol{b}}(\\boldsymbol{b}^{T}X^{T}-Y^{T})(X\\boldsymbol{b}-Y)\\\\\\\\\n\t&=\\frac{1}{2}\\nabla_{\\boldsymbol{b}}(\\boldsymbol{b}^{T}X^{T}X\\boldsymbol{b}-\\boldsymbol{b}^{T}X^{T}Y-Y^{T}X\\boldsymbol{b}+Y^{T}Y)\\\\\\\\\n\t&=\\frac{1}{2}\\nabla_{\\boldsymbol{b}}tr(\\boldsymbol{b}^{T}X^{T}X\\boldsymbol{b}-\\boldsymbol{b}^{T}X^{T}Y-Y^{T}X\\boldsymbol{b}+Y^{T}Y)\\\\\\\\\n\t&=\\frac{1}{2}\\nabla_{\\boldsymbol{b}}\\left\\(tr(\\boldsymbol{b}^{T}X^{T}X\\boldsymbol{b})-2tr(Y^{T}X\\boldsymbol{b})+tr(Y^{T}Y)\\right\\)\\\\\\\\\n\t&=X^{T}X\\boldsymbol{b}-X^{T}Y. \n\\end{align}\n$$\n\nBy setting the derivative to be 0, solution Eq.[4] is obtained.\nHowever, this least square solution may have problem when the training sample is not enough, that is m<n. In that case, the matrix \\\\(X^{T}X\\\\) doesn't have full rank which means it is nonsingular. \n\nTo solve this problem, we can project each measurement \\\\(\\boldsymbol{x_{i}}=(x_{i1},\\cdots,x_{in})\\\\) into a lower-dimensional subspace spanned by data \\\\(T=XW\\\\). We can think of this as forming a smaller set of features, each being the linear combination of the original set of features. These new features are also called “latent” variables. Therefore, the linear regression can be written as a linear regression system on the new latent variables\n\n$$\n\\begin{align}\nY &=TQ^{T}+F \\tag{6} \\\\\\\\\nX &=TP^{T}+E \\tag{7}\n\\end{align}\n$$\n\nwhere \\\\(P,Q\\\\) are the coefficient matrices and E,F are matrices of errors. As in partial least squares regression (PLSR), the weight matrix \\\\(W\\\\) reflects the covariance structure between the predictor and response variables. Hence, maximizing the covariance of the latent variables and the response variables \\\\(Cov(T,Y)\\\\) gives us the weight matrix \\\\(W\\\\). Once obtaining \\\\(W\\\\) and then constructing \\\\(T\\\\), \\\\(Q^{T}\\\\) is solved by the least squares solution Eq.[6]:\n\n$$\nQ^{T}=(T^{T}T)^{-1}T^{T}Y.\n$$\n\nPlug Eq.[eq:latent] into the regression equation Eq.[6], we obtain the solution of the matrix \\\\(\\boldsymbol{b}\\\\) of the coefficient in the model Eq.[3]\n\n$$\n\\boldsymbol{b}=W(T^{T}T)^{-1}T^{T}Y.\n$$\n\nSo this is the whole idea of PLS. A commonly used algorithm to compute PLSR is the nonlinear iterative partial least square (NIPALS) method. The steps are summarized as follows:\n\n\n**NIPALS**\n\n1. Normalize \\\\(m\\\\) training samples by \n\n$$\n\\begin{align}\nx_{ij} &=\\left(x_{ij}-\\bar{x} _{\\cdot\\ j}\\right)/ \\sigma _{x_{\\cdot\\ j}} \\\\\\\\\ny_{i} &=\\left(y_{i}-\\bar{y}\\right)/ \\sigma _{y}\\ \\text{for }i=1,\\cdots,m,j=1,\\cdots,n\n\\end{align}\n$$\n\n**Compute the scores**\n\n2. Compute the dominant eigenvector of \\\\(X_{0}^{T}Y_{0}Y_{0}^{T}X_{0}\\\\) and assign it to \\\\(w_{1}\\\\) (the step of maximizing the covariance). Normalize \\\\(w_{1}\\\\). The scores \\\\(t_{1}\\\\) of \\\\(X_{0}\\\\) yields\n\n$$\nt_{1}=X_{0}w_{1}.\n$$\n\nDo the same to \\\\(Y_{0}\\\\) and obtain a normalized vector \\\\(c_{1}\\\\). The scores \\\\(v_{1}\\\\) of \\\\(Y_{0}\\\\) yields\n\n$$\nv_{1}=Y_{0}c_{1}.\n$$\n\n\n**Compute the loadings**\n\n3. Based on the regression equations\n\n$$\n\\begin{align}\nX\t&=t_{1}p_{1}^{T}+E \\\\\\\\\nY\t&=t_{1}q_{1}^{T}+F,\n\\end{align}\n$$\n\nthe least square method gives us \n\n\n$$\n\\begin{align}\np_{1} &=\\frac{X^{T}t_{1}}{t_{1}t_{1}^{T}} \\\\\\\\\nq_{1} &=\\frac{Y^{T}t_{1}}{t_{1}t_{1}^{T}}.\n\\end{align}\n$$\n\n\n4. Check if \\\\(F\\\\) is sufficiently small. Otherwise, set \\\\(E\\\\) as \\\\(X_{1}\\\\), \\\\(F\\\\) as \\\\(Y_{1}\\\\). Repeat step 2-3. \n\n5. Finally, we obtain \n\n$$\n\\begin{align}\nX &=t_{1}p_{1}^{T}+t_{2}p_{2}^{T}+\\cdots+t_{n}p_{n}^{T}+E \\\\\\\\\nY &=t_{1}q_{1}^{T}+t_{2}q_{2}^{T}+\\cdots+t_{n}q_{n}^{T}+F\n\\end{align}\n$$\n\nwhich in a matrix format is as follows:\n\n$$\n\\begin{align}\nX &=TP^{T}+E \\\\\\\\\nY &=TQ^{T}+F \\\\\\\\\n\t&=XWQ^{T}+F \\\\\\\\\n\t&:=XB+F.\n\\end{align}\n$$\n\nSo, when we have a new data \\\\(x_{new}\\\\), to define the PLS components in our ABC-MCMC algorithm, we only need to compute \n\n$$\nt_{1}=x_{new}^{T}p_{1},t_{2}=x_{new}^{T}p_{2},\\cdots,t_{n}=x_{new}^{T}p_{n}\n$$\n\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>","slug":"2018-10-25-PLSR","published":1,"date":"2020-10-05T11:30:58.212Z","updated":"2020-10-05T11:30:58.212Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy127000cnj391jb65uit","content":"<p>I found the following interpretation to partial least square regression is much better than mine. So I cited it as below:</p>\n<blockquote>\n<p>“Partial least squares regression (PLS regression) is a statistical method that bears some relation to principal components regression; instead of finding hyperplanes of maximum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the observable variables to a new space. Because both the X and Y data are projected to new spaces, the PLS family of methods are known as bilinear factor models. Partial least squares Discriminant Analysis (PLS-DA) is a variant used when the Y is categorical.”</p>\n<p>—from <a href=\"https://en.wikipedia.org/wiki/Partial_least_squares_regression\" target=\"_blank\" rel=\"noopener\">Wikipedia</a></p>\n</blockquote>\n<a id=\"more\"></a>\n<h2 id=\"PLS-algorithm\"><a href=\"#PLS-algorithm\" class=\"headerlink\" title=\"PLS algorithm\"></a>PLS algorithm</h2><p>For record purpose, I am deriving the PLS algorithm in this post. </p>\n<p>Consider a linear regression problem in a vector format as in Eq.[eq:1] given an observed data, which is denoted by a vector \\(\\boldsymbol{x}^{T}=(x_{1},x_{2},\\cdots,x_{n})\\)</p>\n<p>$$<br>\\begin{align}<br>y    &amp;=b_{1}x_{1}+\\cdots+b_{n}x_{n}+e \\\\<br>    &amp;=\\boldsymbol{x}^{T}\\boldsymbol{b}+e \\tag{1}<br>\\end{align}<br>$$</p>\n<p>We are aiming to work out the unknown regression coefficients vector \\(\\boldsymbol{b}\\) for future prediction of \\(y\\) once we have the observed data \\(\\boldsymbol{x}\\). To keep the prediction as accurate as possible, we will want to make the residual error e as small as possible. This is simply the whole idea of the linear regression method. So one solution is to use training samples, that is given m observations \\(\\boldsymbol{y}^{T}=(y_{1},\\cdots,y_{m}),X=[x_{ij}],i=1,\\cdots,m,j=1,\\cdots,n \\)<br>to solve \\(\\boldsymbol{b}\\) out in terms of keeping the residual error sufficiently small</p>\n<p>$$<br>\\begin{pmatrix}<br>y_{1}\\\\<br>y_{2}\\\\<br>\\vdots\\\\<br>y_{m}<br>\\end{pmatrix}=\\begin{pmatrix}<br>x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1n}\\\\<br>x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2n}\\\\<br>\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\<br>x_{m1} &amp; x_{m2} &amp; \\cdots &amp; x_{mn}<br>\\end{pmatrix}\\begin{pmatrix}<br>b_{1}\\\\<br>b_{2}\\\\<br>\\vdots\\\\<br>b_{n}<br>\\end{pmatrix}+\\begin{pmatrix}<br>e_{1}\\\\<br>e_{2}\\\\<br>\\vdots\\\\<br>e_{m}<br>\\end{pmatrix}\\tag{2}<br>$$</p>\n<p>For comparability among different data, all the observations are assumed to be centered and normalized in advance. Now the problem is translated to an optimization problem, that is to find a least square solution for the regression coefficients such that the sum of the square errors of the residuals is minimized. For simplicity, I would like to use the matrix form for Eq.[2]</p>\n<p>$$<br>Y=X\\boldsymbol{b}+\\boldsymbol{e}. \\tag{3}<br>$$</p>\n<p>We want to find out \\(\\boldsymbol{b}\\) such that</p>\n<p>$$<br>min\\{\\boldsymbol{e}^{T}\\boldsymbol{e}\\}.<br>$$</p>\n<p>The least square solution for \\(\\boldsymbol{b}\\) is given by</p>\n<p>$$<br>\\boldsymbol{b}=(X^{T}X)^{-1}X^{T}Y. \\tag{4}<br>$$</p>\n<p>The derivation is simply as follows:</p>\n<p>$$<br>F_{\\boldsymbol{b}}=\\frac{1}{2}(X\\boldsymbol{b}-Y)^{T}(X\\boldsymbol{b}-Y) \\tag{5}<br>$$</p>\n<p>So the derivative of F_{\\boldsymbol{b}} yields</p>\n<p>$$<br>\\begin{align}<br>\\nabla_{\\boldsymbol{b}}F_{\\boldsymbol{b}}<br>&amp;=\\frac{1}{2}\\nabla_{\\boldsymbol{b}}(X\\boldsymbol{b}-Y)^{T}(X\\boldsymbol{b}-Y)\\\\<br>    &amp;=\\frac{1}{2}\\nabla_{\\boldsymbol{b}}(\\boldsymbol{b}^{T}X^{T}-Y^{T})(X\\boldsymbol{b}-Y)\\\\<br>    &amp;=\\frac{1}{2}\\nabla_{\\boldsymbol{b}}(\\boldsymbol{b}^{T}X^{T}X\\boldsymbol{b}-\\boldsymbol{b}^{T}X^{T}Y-Y^{T}X\\boldsymbol{b}+Y^{T}Y)\\\\<br>    &amp;=\\frac{1}{2}\\nabla_{\\boldsymbol{b}}tr(\\boldsymbol{b}^{T}X^{T}X\\boldsymbol{b}-\\boldsymbol{b}^{T}X^{T}Y-Y^{T}X\\boldsymbol{b}+Y^{T}Y)\\\\<br>    &amp;=\\frac{1}{2}\\nabla_{\\boldsymbol{b}}\\left(tr(\\boldsymbol{b}^{T}X^{T}X\\boldsymbol{b})-2tr(Y^{T}X\\boldsymbol{b})+tr(Y^{T}Y)\\right)\\\\<br>    &amp;=X^{T}X\\boldsymbol{b}-X^{T}Y.<br>\\end{align}<br>$$</p>\n<p>By setting the derivative to be 0, solution Eq.[4] is obtained.<br>However, this least square solution may have problem when the training sample is not enough, that is m&lt;n. In that case, the matrix \\(X^{T}X\\) doesn’t have full rank which means it is nonsingular. </p>\n<p>To solve this problem, we can project each measurement \\(\\boldsymbol{x_{i}}=(x_{i1},\\cdots,x_{in})\\) into a lower-dimensional subspace spanned by data \\(T=XW\\). We can think of this as forming a smaller set of features, each being the linear combination of the original set of features. These new features are also called “latent” variables. Therefore, the linear regression can be written as a linear regression system on the new latent variables</p>\n<p>$$<br>\\begin{align}<br>Y &amp;=TQ^{T}+F \\tag{6} \\\\<br>X &amp;=TP^{T}+E \\tag{7}<br>\\end{align}<br>$$</p>\n<p>where \\(P,Q\\) are the coefficient matrices and E,F are matrices of errors. As in partial least squares regression (PLSR), the weight matrix \\(W\\) reflects the covariance structure between the predictor and response variables. Hence, maximizing the covariance of the latent variables and the response variables \\(Cov(T,Y)\\) gives us the weight matrix \\(W\\). Once obtaining \\(W\\) and then constructing \\(T\\), \\(Q^{T}\\) is solved by the least squares solution Eq.[6]:</p>\n<p>$$<br>Q^{T}=(T^{T}T)^{-1}T^{T}Y.<br>$$</p>\n<p>Plug Eq.[eq:latent] into the regression equation Eq.[6], we obtain the solution of the matrix \\(\\boldsymbol{b}\\) of the coefficient in the model Eq.[3]</p>\n<p>$$<br>\\boldsymbol{b}=W(T^{T}T)^{-1}T^{T}Y.<br>$$</p>\n<p>So this is the whole idea of PLS. A commonly used algorithm to compute PLSR is the nonlinear iterative partial least square (NIPALS) method. The steps are summarized as follows:</p>\n<p><strong>NIPALS</strong></p>\n<ol>\n<li>Normalize \\(m\\) training samples by </li>\n</ol>\n<p>$$<br>\\begin{align}<br>x_{ij} &amp;=\\left(x_{ij}-\\bar{x} _{\\cdot\\ j}\\right)/ \\sigma _{x_{\\cdot\\ j}} \\\\<br>y_{i} &amp;=\\left(y_{i}-\\bar{y}\\right)/ \\sigma _{y}\\ \\text{for }i=1,\\cdots,m,j=1,\\cdots,n<br>\\end{align}<br>$$</p>\n<p><strong>Compute the scores</strong></p>\n<ol start=\"2\">\n<li>Compute the dominant eigenvector of \\(X_{0}^{T}Y_{0}Y_{0}^{T}X_{0}\\) and assign it to \\(w_{1}\\) (the step of maximizing the covariance). Normalize \\(w_{1}\\). The scores \\(t_{1}\\) of \\(X_{0}\\) yields</li>\n</ol>\n<p>$$<br>t_{1}=X_{0}w_{1}.<br>$$</p>\n<p>Do the same to \\(Y_{0}\\) and obtain a normalized vector \\(c_{1}\\). The scores \\(v_{1}\\) of \\(Y_{0}\\) yields</p>\n<p>$$<br>v_{1}=Y_{0}c_{1}.<br>$$</p>\n<p><strong>Compute the loadings</strong></p>\n<ol start=\"3\">\n<li>Based on the regression equations</li>\n</ol>\n<p>$$<br>\\begin{align}<br>X    &amp;=t_{1}p_{1}^{T}+E \\\\<br>Y    &amp;=t_{1}q_{1}^{T}+F,<br>\\end{align}<br>$$</p>\n<p>the least square method gives us </p>\n<p>$$<br>\\begin{align}<br>p_{1} &amp;=\\frac{X^{T}t_{1}}{t_{1}t_{1}^{T}} \\\\<br>q_{1} &amp;=\\frac{Y^{T}t_{1}}{t_{1}t_{1}^{T}}.<br>\\end{align}<br>$$</p>\n<ol start=\"4\">\n<li><p>Check if \\(F\\) is sufficiently small. Otherwise, set \\(E\\) as \\(X_{1}\\), \\(F\\) as \\(Y_{1}\\). Repeat step 2-3. </p>\n</li>\n<li><p>Finally, we obtain </p>\n</li>\n</ol>\n<p>$$<br>\\begin{align}<br>X &amp;=t_{1}p_{1}^{T}+t_{2}p_{2}^{T}+\\cdots+t_{n}p_{n}^{T}+E \\\\<br>Y &amp;=t_{1}q_{1}^{T}+t_{2}q_{2}^{T}+\\cdots+t_{n}q_{n}^{T}+F<br>\\end{align}<br>$$</p>\n<p>which in a matrix format is as follows:</p>\n<p>$$<br>\\begin{align}<br>X &amp;=TP^{T}+E \\\\<br>Y &amp;=TQ^{T}+F \\\\<br>    &amp;=XWQ^{T}+F \\\\<br>    &amp;:=XB+F.<br>\\end{align}<br>$$</p>\n<p>So, when we have a new data \\(x_{new}\\), to define the PLS components in our ABC-MCMC algorithm, we only need to compute </p>\n<p>$$<br>t_{1}=x_{new}^{T}p_{1},t_{2}=x_{new}^{T}p_{2},\\cdots,t_{n}=x_{new}^{T}p_{n}<br>$$</p>\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>","site":{"data":{}},"excerpt":"<p>I found the following interpretation to partial least square regression is much better than mine. So I cited it as below:</p>\n<blockquote>\n<p>“Partial least squares regression (PLS regression) is a statistical method that bears some relation to principal components regression; instead of finding hyperplanes of maximum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the observable variables to a new space. Because both the X and Y data are projected to new spaces, the PLS family of methods are known as bilinear factor models. Partial least squares Discriminant Analysis (PLS-DA) is a variant used when the Y is categorical.”</p>\n<p>—from <a href=\"https://en.wikipedia.org/wiki/Partial_least_squares_regression\" target=\"_blank\" rel=\"noopener\">Wikipedia</a></p>\n</blockquote>","more":"<h2 id=\"PLS-algorithm\"><a href=\"#PLS-algorithm\" class=\"headerlink\" title=\"PLS algorithm\"></a>PLS algorithm</h2><p>For record purpose, I am deriving the PLS algorithm in this post. </p>\n<p>Consider a linear regression problem in a vector format as in Eq.[eq:1] given an observed data, which is denoted by a vector \\(\\boldsymbol{x}^{T}=(x_{1},x_{2},\\cdots,x_{n})\\)</p>\n<p>$$<br>\\begin{align}<br>y    &amp;=b_{1}x_{1}+\\cdots+b_{n}x_{n}+e \\\\<br>    &amp;=\\boldsymbol{x}^{T}\\boldsymbol{b}+e \\tag{1}<br>\\end{align}<br>$$</p>\n<p>We are aiming to work out the unknown regression coefficients vector \\(\\boldsymbol{b}\\) for future prediction of \\(y\\) once we have the observed data \\(\\boldsymbol{x}\\). To keep the prediction as accurate as possible, we will want to make the residual error e as small as possible. This is simply the whole idea of the linear regression method. So one solution is to use training samples, that is given m observations \\(\\boldsymbol{y}^{T}=(y_{1},\\cdots,y_{m}),X=[x_{ij}],i=1,\\cdots,m,j=1,\\cdots,n \\)<br>to solve \\(\\boldsymbol{b}\\) out in terms of keeping the residual error sufficiently small</p>\n<p>$$<br>\\begin{pmatrix}<br>y_{1}\\\\<br>y_{2}\\\\<br>\\vdots\\\\<br>y_{m}<br>\\end{pmatrix}=\\begin{pmatrix}<br>x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1n}\\\\<br>x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2n}\\\\<br>\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\<br>x_{m1} &amp; x_{m2} &amp; \\cdots &amp; x_{mn}<br>\\end{pmatrix}\\begin{pmatrix}<br>b_{1}\\\\<br>b_{2}\\\\<br>\\vdots\\\\<br>b_{n}<br>\\end{pmatrix}+\\begin{pmatrix}<br>e_{1}\\\\<br>e_{2}\\\\<br>\\vdots\\\\<br>e_{m}<br>\\end{pmatrix}\\tag{2}<br>$$</p>\n<p>For comparability among different data, all the observations are assumed to be centered and normalized in advance. Now the problem is translated to an optimization problem, that is to find a least square solution for the regression coefficients such that the sum of the square errors of the residuals is minimized. For simplicity, I would like to use the matrix form for Eq.[2]</p>\n<p>$$<br>Y=X\\boldsymbol{b}+\\boldsymbol{e}. \\tag{3}<br>$$</p>\n<p>We want to find out \\(\\boldsymbol{b}\\) such that</p>\n<p>$$<br>min\\{\\boldsymbol{e}^{T}\\boldsymbol{e}\\}.<br>$$</p>\n<p>The least square solution for \\(\\boldsymbol{b}\\) is given by</p>\n<p>$$<br>\\boldsymbol{b}=(X^{T}X)^{-1}X^{T}Y. \\tag{4}<br>$$</p>\n<p>The derivation is simply as follows:</p>\n<p>$$<br>F_{\\boldsymbol{b}}=\\frac{1}{2}(X\\boldsymbol{b}-Y)^{T}(X\\boldsymbol{b}-Y) \\tag{5}<br>$$</p>\n<p>So the derivative of F_{\\boldsymbol{b}} yields</p>\n<p>$$<br>\\begin{align}<br>\\nabla_{\\boldsymbol{b}}F_{\\boldsymbol{b}}<br>&amp;=\\frac{1}{2}\\nabla_{\\boldsymbol{b}}(X\\boldsymbol{b}-Y)^{T}(X\\boldsymbol{b}-Y)\\\\<br>    &amp;=\\frac{1}{2}\\nabla_{\\boldsymbol{b}}(\\boldsymbol{b}^{T}X^{T}-Y^{T})(X\\boldsymbol{b}-Y)\\\\<br>    &amp;=\\frac{1}{2}\\nabla_{\\boldsymbol{b}}(\\boldsymbol{b}^{T}X^{T}X\\boldsymbol{b}-\\boldsymbol{b}^{T}X^{T}Y-Y^{T}X\\boldsymbol{b}+Y^{T}Y)\\\\<br>    &amp;=\\frac{1}{2}\\nabla_{\\boldsymbol{b}}tr(\\boldsymbol{b}^{T}X^{T}X\\boldsymbol{b}-\\boldsymbol{b}^{T}X^{T}Y-Y^{T}X\\boldsymbol{b}+Y^{T}Y)\\\\<br>    &amp;=\\frac{1}{2}\\nabla_{\\boldsymbol{b}}\\left(tr(\\boldsymbol{b}^{T}X^{T}X\\boldsymbol{b})-2tr(Y^{T}X\\boldsymbol{b})+tr(Y^{T}Y)\\right)\\\\<br>    &amp;=X^{T}X\\boldsymbol{b}-X^{T}Y.<br>\\end{align}<br>$$</p>\n<p>By setting the derivative to be 0, solution Eq.[4] is obtained.<br>However, this least square solution may have problem when the training sample is not enough, that is m&lt;n. In that case, the matrix \\(X^{T}X\\) doesn’t have full rank which means it is nonsingular. </p>\n<p>To solve this problem, we can project each measurement \\(\\boldsymbol{x_{i}}=(x_{i1},\\cdots,x_{in})\\) into a lower-dimensional subspace spanned by data \\(T=XW\\). We can think of this as forming a smaller set of features, each being the linear combination of the original set of features. These new features are also called “latent” variables. Therefore, the linear regression can be written as a linear regression system on the new latent variables</p>\n<p>$$<br>\\begin{align}<br>Y &amp;=TQ^{T}+F \\tag{6} \\\\<br>X &amp;=TP^{T}+E \\tag{7}<br>\\end{align}<br>$$</p>\n<p>where \\(P,Q\\) are the coefficient matrices and E,F are matrices of errors. As in partial least squares regression (PLSR), the weight matrix \\(W\\) reflects the covariance structure between the predictor and response variables. Hence, maximizing the covariance of the latent variables and the response variables \\(Cov(T,Y)\\) gives us the weight matrix \\(W\\). Once obtaining \\(W\\) and then constructing \\(T\\), \\(Q^{T}\\) is solved by the least squares solution Eq.[6]:</p>\n<p>$$<br>Q^{T}=(T^{T}T)^{-1}T^{T}Y.<br>$$</p>\n<p>Plug Eq.[eq:latent] into the regression equation Eq.[6], we obtain the solution of the matrix \\(\\boldsymbol{b}\\) of the coefficient in the model Eq.[3]</p>\n<p>$$<br>\\boldsymbol{b}=W(T^{T}T)^{-1}T^{T}Y.<br>$$</p>\n<p>So this is the whole idea of PLS. A commonly used algorithm to compute PLSR is the nonlinear iterative partial least square (NIPALS) method. The steps are summarized as follows:</p>\n<p><strong>NIPALS</strong></p>\n<ol>\n<li>Normalize \\(m\\) training samples by </li>\n</ol>\n<p>$$<br>\\begin{align}<br>x_{ij} &amp;=\\left(x_{ij}-\\bar{x} _{\\cdot\\ j}\\right)/ \\sigma _{x_{\\cdot\\ j}} \\\\<br>y_{i} &amp;=\\left(y_{i}-\\bar{y}\\right)/ \\sigma _{y}\\ \\text{for }i=1,\\cdots,m,j=1,\\cdots,n<br>\\end{align}<br>$$</p>\n<p><strong>Compute the scores</strong></p>\n<ol start=\"2\">\n<li>Compute the dominant eigenvector of \\(X_{0}^{T}Y_{0}Y_{0}^{T}X_{0}\\) and assign it to \\(w_{1}\\) (the step of maximizing the covariance). Normalize \\(w_{1}\\). The scores \\(t_{1}\\) of \\(X_{0}\\) yields</li>\n</ol>\n<p>$$<br>t_{1}=X_{0}w_{1}.<br>$$</p>\n<p>Do the same to \\(Y_{0}\\) and obtain a normalized vector \\(c_{1}\\). The scores \\(v_{1}\\) of \\(Y_{0}\\) yields</p>\n<p>$$<br>v_{1}=Y_{0}c_{1}.<br>$$</p>\n<p><strong>Compute the loadings</strong></p>\n<ol start=\"3\">\n<li>Based on the regression equations</li>\n</ol>\n<p>$$<br>\\begin{align}<br>X    &amp;=t_{1}p_{1}^{T}+E \\\\<br>Y    &amp;=t_{1}q_{1}^{T}+F,<br>\\end{align}<br>$$</p>\n<p>the least square method gives us </p>\n<p>$$<br>\\begin{align}<br>p_{1} &amp;=\\frac{X^{T}t_{1}}{t_{1}t_{1}^{T}} \\\\<br>q_{1} &amp;=\\frac{Y^{T}t_{1}}{t_{1}t_{1}^{T}}.<br>\\end{align}<br>$$</p>\n<ol start=\"4\">\n<li><p>Check if \\(F\\) is sufficiently small. Otherwise, set \\(E\\) as \\(X_{1}\\), \\(F\\) as \\(Y_{1}\\). Repeat step 2-3. </p>\n</li>\n<li><p>Finally, we obtain </p>\n</li>\n</ol>\n<p>$$<br>\\begin{align}<br>X &amp;=t_{1}p_{1}^{T}+t_{2}p_{2}^{T}+\\cdots+t_{n}p_{n}^{T}+E \\\\<br>Y &amp;=t_{1}q_{1}^{T}+t_{2}q_{2}^{T}+\\cdots+t_{n}q_{n}^{T}+F<br>\\end{align}<br>$$</p>\n<p>which in a matrix format is as follows:</p>\n<p>$$<br>\\begin{align}<br>X &amp;=TP^{T}+E \\\\<br>Y &amp;=TQ^{T}+F \\\\<br>    &amp;=XWQ^{T}+F \\\\<br>    &amp;:=XB+F.<br>\\end{align}<br>$$</p>\n<p>So, when we have a new data \\(x_{new}\\), to define the PLS components in our ABC-MCMC algorithm, we only need to compute </p>\n<p>$$<br>t_{1}=x_{new}^{T}p_{1},t_{2}=x_{new}^{T}p_{2},\\cdots,t_{n}=x_{new}^{T}p_{n}<br>$$</p>\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>"},{"title":"Sed:a fast way to extract information from data","_content":"\nIn the third project, a bunch of mega data are generated for analysis. However, the simulated data is too big to load in memory. Thus, I need to extract some information without opening the file. After quite a few time searching for a solution, `sed` command in bash language helps me out. More details about the third project is coming soon.\n\n<!--more-->\n\n## The data file\nThe data file contains a few snapshot on different time points. Some matrix are written in the file when a snapshot time is hit. These matrix may be very large according to the parameter settings you chose. What I want to extract is the matrix \\\\(\\boldsymbol{D}\\\\) and \\\\(\\boldsymbol{R}\\\\) at the end. \n\n![Raw data files.](2018-10-29-sed/rawdata.png)\n\n## Extracting the matrix\nFirstly, I extract all the \\\\(\\boldsymbol{D}\\\\)s, for example, from the data file. The following code does the thing for me.\n\n```bash\n#!/bin/bash\nfor j in {0..4};\ndo\nfor i in {0..4};\ndo \nsed -n '/D'{'length(D)+1'}' = \\[\\r/,/\\];/p' test\"$j$i\".m > Ds\"$j$i\".Rdata\necho $j$i' done'\ndone\ndone\n```\n\nAs you see, there are in total 25 data files I need to deal with. For each data file, `sed` finds the line starting with \"\\\\(D\\left\\\\{length(D)+1\\right\\\\} = [\\\\)\" till \"\\\\(];\\\\)\" from the file testji.m and writes/prints (\\\\(p\\\\)) all the matched lines to the file Dsji.Rdata. \n\n![All D matrix are extracted.](2018-10-29-sed/ds.png)\n\n## Replacing the last D matrix\nThen, I want to extract the last  \\\\(\\boldsymbol{D}\\\\) matrix. How can I do that? Use the following code:\n\n```bash\n#!/bin/bash\nfor j in {0..4};\ndo\nfor i in {0..4};\ndo \nA=$(grep -c 'D'{'length(D)+1'}' = \\[' Ds\"$j$i\".Rdata)\nB=$[$A-1]\nsed '/D'{'length(D)+1'}'/{G;s/\\nX\\{'$B'\\}//;tend;x;s/^/X/;x;P;d};p;d;:end;s/D'{'length(D)+1'}'/D/;:a;n;ba' Ds\"$j$i\".Rdata>Dt\"$j$i\".Rdata\nsed -n '/D = \\[\\r/,/\\];/p' Dt\"$j$i\".Rdata > D\"$j$i\".Rdata\n\nC=$(cat D\"$j$i\".Rdata |wc -l)\nC=$[$C-2]\nD=$[$C+1]\n\nsed -i -e 's/D = \\[/D = structure(c(/' -e 's/\\];/),.Dim=c('$(echo $C)','$(echo $C)'))/' -e 's/;/ /g' -e '2,${s/ /,/g}' -e '2,${s/,,/ /g}' -e '3,'$(echo $D)'{s/^/,/g}' D\"$j$i\".Rdata\necho $j$i' done'\ndone\ndone\n```\n\nThe command `grep` returns the number of the replicates of the line \"\\\\(D\\left\\\\{length(D)+1\\right\\\\} = [\\\\)\". Then `sed` replaces the head of the last matrix with \\\\(D\\\\) and extracts this matrix out and writes into file Dij.Rdata. As I want to work in R with these results, I reformat the structure of the results to fit the matrix format in R which is something like:\n\n```R\nD = structure(c(1,0,0,1),.Dim=c(2,2))\n```\n\nAt last, I can directly `source` the file in R and read the matrix. \n\n![The last D matrix is captured.](2018-10-29-sed/d.png)\n\n## Details in sed\nThe first `sed` in the second script is the key part in this function. \n\n```bash\nsed '/D'{'length(D)+1'}'/{G;s/\\nX\\{'$B'\\}//;tend;x;s/^/X/;x;P;d};p;d;:end;s/D'{'length(D)+1'}'/D/;:a;n;ba' Ds\"$j$i\".Rdata>Dt\"$j$i\".Rdata\n```\n\nI don't fully understand what every letter in this command means. In this [answer](https://superuser.com/questions/394282/sed-perform-only-first-nth-matched-replacement), the author explained the idea is to store a \\\\(X\\\\) at each match in the holdspace, and when all the \\\\(X\\\\)s are there, loop till the end of file. If you know more, please comment after the post. Thanks!\n\nThe third `sed` reformats the matrix to fit in R. `-e` executes several commands in one line. \n\n```bash\nsed -i -e 's/D = \\[/D = structure(c(/' -e 's/\\];/),.Dim=c('$(echo $C)','$(echo $C)'))/' -e 's/;/ /g' -e '2,${s/ /,/g}' -e '2,${s/,,/ /g}' -e '3,'$(echo $D)'{s/^/,/g}' D\"$j$i\".Rdata\n```\n\nThe first segment replaces \"\\\\(D = [ \\\\)\" with \"\\\\(D = structure(c( \\\\)\" while the second replaces the tail \"];\" with \"(,.Dim=c(row,col)\". Then all the signs \";\" are substituted by space and the single space is substituted by \",\" from the second line to the end. Following the replacement of \",,\" by space from the second line and add \",\" at the beginning of the line from the third line on. Finally, a standard matrix form is rebuilt.  \n\nBTW, a good tutorial can be found [here](http://www.grymoire.com/Unix/Sed.html). Have fun! ","source":"_posts/2018-10-29-sed.md","raw":"---\ntitle: \"Sed:a fast way to extract information from data\" \ncategories: [Research, Bash, sed]\ntags: [project 3, bash, mega data, extract information]\n---\n\nIn the third project, a bunch of mega data are generated for analysis. However, the simulated data is too big to load in memory. Thus, I need to extract some information without opening the file. After quite a few time searching for a solution, `sed` command in bash language helps me out. More details about the third project is coming soon.\n\n<!--more-->\n\n## The data file\nThe data file contains a few snapshot on different time points. Some matrix are written in the file when a snapshot time is hit. These matrix may be very large according to the parameter settings you chose. What I want to extract is the matrix \\\\(\\boldsymbol{D}\\\\) and \\\\(\\boldsymbol{R}\\\\) at the end. \n\n![Raw data files.](2018-10-29-sed/rawdata.png)\n\n## Extracting the matrix\nFirstly, I extract all the \\\\(\\boldsymbol{D}\\\\)s, for example, from the data file. The following code does the thing for me.\n\n```bash\n#!/bin/bash\nfor j in {0..4};\ndo\nfor i in {0..4};\ndo \nsed -n '/D'{'length(D)+1'}' = \\[\\r/,/\\];/p' test\"$j$i\".m > Ds\"$j$i\".Rdata\necho $j$i' done'\ndone\ndone\n```\n\nAs you see, there are in total 25 data files I need to deal with. For each data file, `sed` finds the line starting with \"\\\\(D\\left\\\\{length(D)+1\\right\\\\} = [\\\\)\" till \"\\\\(];\\\\)\" from the file testji.m and writes/prints (\\\\(p\\\\)) all the matched lines to the file Dsji.Rdata. \n\n![All D matrix are extracted.](2018-10-29-sed/ds.png)\n\n## Replacing the last D matrix\nThen, I want to extract the last  \\\\(\\boldsymbol{D}\\\\) matrix. How can I do that? Use the following code:\n\n```bash\n#!/bin/bash\nfor j in {0..4};\ndo\nfor i in {0..4};\ndo \nA=$(grep -c 'D'{'length(D)+1'}' = \\[' Ds\"$j$i\".Rdata)\nB=$[$A-1]\nsed '/D'{'length(D)+1'}'/{G;s/\\nX\\{'$B'\\}//;tend;x;s/^/X/;x;P;d};p;d;:end;s/D'{'length(D)+1'}'/D/;:a;n;ba' Ds\"$j$i\".Rdata>Dt\"$j$i\".Rdata\nsed -n '/D = \\[\\r/,/\\];/p' Dt\"$j$i\".Rdata > D\"$j$i\".Rdata\n\nC=$(cat D\"$j$i\".Rdata |wc -l)\nC=$[$C-2]\nD=$[$C+1]\n\nsed -i -e 's/D = \\[/D = structure(c(/' -e 's/\\];/),.Dim=c('$(echo $C)','$(echo $C)'))/' -e 's/;/ /g' -e '2,${s/ /,/g}' -e '2,${s/,,/ /g}' -e '3,'$(echo $D)'{s/^/,/g}' D\"$j$i\".Rdata\necho $j$i' done'\ndone\ndone\n```\n\nThe command `grep` returns the number of the replicates of the line \"\\\\(D\\left\\\\{length(D)+1\\right\\\\} = [\\\\)\". Then `sed` replaces the head of the last matrix with \\\\(D\\\\) and extracts this matrix out and writes into file Dij.Rdata. As I want to work in R with these results, I reformat the structure of the results to fit the matrix format in R which is something like:\n\n```R\nD = structure(c(1,0,0,1),.Dim=c(2,2))\n```\n\nAt last, I can directly `source` the file in R and read the matrix. \n\n![The last D matrix is captured.](2018-10-29-sed/d.png)\n\n## Details in sed\nThe first `sed` in the second script is the key part in this function. \n\n```bash\nsed '/D'{'length(D)+1'}'/{G;s/\\nX\\{'$B'\\}//;tend;x;s/^/X/;x;P;d};p;d;:end;s/D'{'length(D)+1'}'/D/;:a;n;ba' Ds\"$j$i\".Rdata>Dt\"$j$i\".Rdata\n```\n\nI don't fully understand what every letter in this command means. In this [answer](https://superuser.com/questions/394282/sed-perform-only-first-nth-matched-replacement), the author explained the idea is to store a \\\\(X\\\\) at each match in the holdspace, and when all the \\\\(X\\\\)s are there, loop till the end of file. If you know more, please comment after the post. Thanks!\n\nThe third `sed` reformats the matrix to fit in R. `-e` executes several commands in one line. \n\n```bash\nsed -i -e 's/D = \\[/D = structure(c(/' -e 's/\\];/),.Dim=c('$(echo $C)','$(echo $C)'))/' -e 's/;/ /g' -e '2,${s/ /,/g}' -e '2,${s/,,/ /g}' -e '3,'$(echo $D)'{s/^/,/g}' D\"$j$i\".Rdata\n```\n\nThe first segment replaces \"\\\\(D = [ \\\\)\" with \"\\\\(D = structure(c( \\\\)\" while the second replaces the tail \"];\" with \"(,.Dim=c(row,col)\". Then all the signs \";\" are substituted by space and the single space is substituted by \",\" from the second line to the end. Following the replacement of \",,\" by space from the second line and add \",\" at the beginning of the line from the third line on. Finally, a standard matrix form is rebuilt.  \n\nBTW, a good tutorial can be found [here](http://www.grymoire.com/Unix/Sed.html). Have fun! ","slug":"2018-10-29-sed","published":1,"date":"2020-10-05T11:30:58.213Z","updated":"2020-10-05T11:30:58.213Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy129000dnj39b7c8c9kf","content":"<p>In the third project, a bunch of mega data are generated for analysis. However, the simulated data is too big to load in memory. Thus, I need to extract some information without opening the file. After quite a few time searching for a solution, <code>sed</code> command in bash language helps me out. More details about the third project is coming soon.</p>\n<a id=\"more\"></a>\n<h2 id=\"The-data-file\"><a href=\"#The-data-file\" class=\"headerlink\" title=\"The data file\"></a>The data file</h2><p>The data file contains a few snapshot on different time points. Some matrix are written in the file when a snapshot time is hit. These matrix may be very large according to the parameter settings you chose. What I want to extract is the matrix \\(\\boldsymbol{D}\\) and \\(\\boldsymbol{R}\\) at the end. </p>\n<p><img src=\"/2020/10/05/2018-10-29-sed/rawdata.png\" alt=\"Raw data files.\"></p>\n<h2 id=\"Extracting-the-matrix\"><a href=\"#Extracting-the-matrix\" class=\"headerlink\" title=\"Extracting the matrix\"></a>Extracting the matrix</h2><p>Firstly, I extract all the \\(\\boldsymbol{D}\\)s, for example, from the data file. The following code does the thing for me.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#!/bin/bash</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> &#123;0..4&#125;;</span><br><span class=\"line\"><span class=\"keyword\">do</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> &#123;0..4&#125;;</span><br><span class=\"line\"><span class=\"keyword\">do</span> </span><br><span class=\"line\">sed -n <span class=\"string\">'/D'</span>&#123;<span class=\"string\">'length(D)+1'</span>&#125;<span class=\"string\">' = \\[\\r/,/\\];/p'</span> <span class=\"built_in\">test</span><span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.m &gt; Ds<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata</span><br><span class=\"line\"><span class=\"built_in\">echo</span> <span class=\"variable\">$j</span><span class=\"variable\">$i</span><span class=\"string\">' done'</span></span><br><span class=\"line\"><span class=\"keyword\">done</span></span><br><span class=\"line\"><span class=\"keyword\">done</span></span><br></pre></td></tr></table></figure>\n<p>As you see, there are in total 25 data files I need to deal with. For each data file, <code>sed</code> finds the line starting with “\\(D\\left\\{length(D)+1\\right\\} = [\\)” till “\\(];\\)” from the file testji.m and writes/prints (\\(p\\)) all the matched lines to the file Dsji.Rdata. </p>\n<p><img src=\"/2020/10/05/2018-10-29-sed/ds.png\" alt=\"All D matrix are extracted.\"></p>\n<h2 id=\"Replacing-the-last-D-matrix\"><a href=\"#Replacing-the-last-D-matrix\" class=\"headerlink\" title=\"Replacing the last D matrix\"></a>Replacing the last D matrix</h2><p>Then, I want to extract the last  \\(\\boldsymbol{D}\\) matrix. How can I do that? Use the following code:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#!/bin/bash</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> &#123;0..4&#125;;</span><br><span class=\"line\"><span class=\"keyword\">do</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> &#123;0..4&#125;;</span><br><span class=\"line\"><span class=\"keyword\">do</span> </span><br><span class=\"line\">A=$(grep -c <span class=\"string\">'D'</span>&#123;<span class=\"string\">'length(D)+1'</span>&#125;<span class=\"string\">' = \\['</span> Ds<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata)</span><br><span class=\"line\">B=$[<span class=\"variable\">$A</span>-1]</span><br><span class=\"line\">sed <span class=\"string\">'/D'</span>&#123;<span class=\"string\">'length(D)+1'</span>&#125;<span class=\"string\">'/&#123;G;s/\\nX\\&#123;'</span><span class=\"variable\">$B</span><span class=\"string\">'\\&#125;//;tend;x;s/^/X/;x;P;d&#125;;p;d;:end;s/D'</span>&#123;<span class=\"string\">'length(D)+1'</span>&#125;<span class=\"string\">'/D/;:a;n;ba'</span> Ds<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata&gt;Dt<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata</span><br><span class=\"line\">sed -n <span class=\"string\">'/D = \\[\\r/,/\\];/p'</span> Dt<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata &gt; D<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata</span><br><span class=\"line\"></span><br><span class=\"line\">C=$(cat D<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata |wc -l)</span><br><span class=\"line\">C=$[<span class=\"variable\">$C</span>-2]</span><br><span class=\"line\">D=$[<span class=\"variable\">$C</span>+1]</span><br><span class=\"line\"></span><br><span class=\"line\">sed -i -e <span class=\"string\">'s/D = \\[/D = structure(c(/'</span> -e <span class=\"string\">'s/\\];/),.Dim=c('</span>$(<span class=\"built_in\">echo</span> <span class=\"variable\">$C</span>)<span class=\"string\">','</span>$(<span class=\"built_in\">echo</span> <span class=\"variable\">$C</span>)<span class=\"string\">'))/'</span> -e <span class=\"string\">'s/;/ /g'</span> -e <span class=\"string\">'2,$&#123;s/ /,/g&#125;'</span> -e <span class=\"string\">'2,$&#123;s/,,/ /g&#125;'</span> -e <span class=\"string\">'3,'</span>$(<span class=\"built_in\">echo</span> <span class=\"variable\">$D</span>)<span class=\"string\">'&#123;s/^/,/g&#125;'</span> D<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata</span><br><span class=\"line\"><span class=\"built_in\">echo</span> <span class=\"variable\">$j</span><span class=\"variable\">$i</span><span class=\"string\">' done'</span></span><br><span class=\"line\"><span class=\"keyword\">done</span></span><br><span class=\"line\"><span class=\"keyword\">done</span></span><br></pre></td></tr></table></figure>\n<p>The command <code>grep</code> returns the number of the replicates of the line “\\(D\\left\\{length(D)+1\\right\\} = [\\)”. Then <code>sed</code> replaces the head of the last matrix with \\(D\\) and extracts this matrix out and writes into file Dij.Rdata. As I want to work in R with these results, I reformat the structure of the results to fit the matrix format in R which is something like:</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">D = structure(c(<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>),.Dim=c(<span class=\"number\">2</span>,<span class=\"number\">2</span>))</span><br></pre></td></tr></table></figure>\n<p>At last, I can directly <code>source</code> the file in R and read the matrix. </p>\n<p><img src=\"/2020/10/05/2018-10-29-sed/d.png\" alt=\"The last D matrix is captured.\"></p>\n<h2 id=\"Details-in-sed\"><a href=\"#Details-in-sed\" class=\"headerlink\" title=\"Details in sed\"></a>Details in sed</h2><p>The first <code>sed</code> in the second script is the key part in this function. </p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sed <span class=\"string\">'/D'</span>&#123;<span class=\"string\">'length(D)+1'</span>&#125;<span class=\"string\">'/&#123;G;s/\\nX\\&#123;'</span><span class=\"variable\">$B</span><span class=\"string\">'\\&#125;//;tend;x;s/^/X/;x;P;d&#125;;p;d;:end;s/D'</span>&#123;<span class=\"string\">'length(D)+1'</span>&#125;<span class=\"string\">'/D/;:a;n;ba'</span> Ds<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata&gt;Dt<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata</span><br></pre></td></tr></table></figure>\n<p>I don’t fully understand what every letter in this command means. In this <a href=\"https://superuser.com/questions/394282/sed-perform-only-first-nth-matched-replacement\" target=\"_blank\" rel=\"noopener\">answer</a>, the author explained the idea is to store a \\(X\\) at each match in the holdspace, and when all the \\(X\\)s are there, loop till the end of file. If you know more, please comment after the post. Thanks!</p>\n<p>The third <code>sed</code> reformats the matrix to fit in R. <code>-e</code> executes several commands in one line. </p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sed -i -e <span class=\"string\">'s/D = \\[/D = structure(c(/'</span> -e <span class=\"string\">'s/\\];/),.Dim=c('</span>$(<span class=\"built_in\">echo</span> <span class=\"variable\">$C</span>)<span class=\"string\">','</span>$(<span class=\"built_in\">echo</span> <span class=\"variable\">$C</span>)<span class=\"string\">'))/'</span> -e <span class=\"string\">'s/;/ /g'</span> -e <span class=\"string\">'2,$&#123;s/ /,/g&#125;'</span> -e <span class=\"string\">'2,$&#123;s/,,/ /g&#125;'</span> -e <span class=\"string\">'3,'</span>$(<span class=\"built_in\">echo</span> <span class=\"variable\">$D</span>)<span class=\"string\">'&#123;s/^/,/g&#125;'</span> D<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata</span><br></pre></td></tr></table></figure>\n<p>The first segment replaces “\\(D = [ \\)” with “\\(D = structure(c( \\)” while the second replaces the tail “];” with “(,.Dim=c(row,col)”. Then all the signs “;” are substituted by space and the single space is substituted by “,” from the second line to the end. Following the replacement of “,,” by space from the second line and add “,” at the beginning of the line from the third line on. Finally, a standard matrix form is rebuilt.  </p>\n<p>BTW, a good tutorial can be found <a href=\"http://www.grymoire.com/Unix/Sed.html\" target=\"_blank\" rel=\"noopener\">here</a>. Have fun! </p>\n","site":{"data":{}},"excerpt":"<p>In the third project, a bunch of mega data are generated for analysis. However, the simulated data is too big to load in memory. Thus, I need to extract some information without opening the file. After quite a few time searching for a solution, <code>sed</code> command in bash language helps me out. More details about the third project is coming soon.</p>","more":"<h2 id=\"The-data-file\"><a href=\"#The-data-file\" class=\"headerlink\" title=\"The data file\"></a>The data file</h2><p>The data file contains a few snapshot on different time points. Some matrix are written in the file when a snapshot time is hit. These matrix may be very large according to the parameter settings you chose. What I want to extract is the matrix \\(\\boldsymbol{D}\\) and \\(\\boldsymbol{R}\\) at the end. </p>\n<p><img src=\"/2020/10/05/2018-10-29-sed/rawdata.png\" alt=\"Raw data files.\"></p>\n<h2 id=\"Extracting-the-matrix\"><a href=\"#Extracting-the-matrix\" class=\"headerlink\" title=\"Extracting the matrix\"></a>Extracting the matrix</h2><p>Firstly, I extract all the \\(\\boldsymbol{D}\\)s, for example, from the data file. The following code does the thing for me.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#!/bin/bash</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> &#123;0..4&#125;;</span><br><span class=\"line\"><span class=\"keyword\">do</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> &#123;0..4&#125;;</span><br><span class=\"line\"><span class=\"keyword\">do</span> </span><br><span class=\"line\">sed -n <span class=\"string\">'/D'</span>&#123;<span class=\"string\">'length(D)+1'</span>&#125;<span class=\"string\">' = \\[\\r/,/\\];/p'</span> <span class=\"built_in\">test</span><span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.m &gt; Ds<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata</span><br><span class=\"line\"><span class=\"built_in\">echo</span> <span class=\"variable\">$j</span><span class=\"variable\">$i</span><span class=\"string\">' done'</span></span><br><span class=\"line\"><span class=\"keyword\">done</span></span><br><span class=\"line\"><span class=\"keyword\">done</span></span><br></pre></td></tr></table></figure>\n<p>As you see, there are in total 25 data files I need to deal with. For each data file, <code>sed</code> finds the line starting with “\\(D\\left\\{length(D)+1\\right\\} = [\\)” till “\\(];\\)” from the file testji.m and writes/prints (\\(p\\)) all the matched lines to the file Dsji.Rdata. </p>\n<p><img src=\"/2020/10/05/2018-10-29-sed/ds.png\" alt=\"All D matrix are extracted.\"></p>\n<h2 id=\"Replacing-the-last-D-matrix\"><a href=\"#Replacing-the-last-D-matrix\" class=\"headerlink\" title=\"Replacing the last D matrix\"></a>Replacing the last D matrix</h2><p>Then, I want to extract the last  \\(\\boldsymbol{D}\\) matrix. How can I do that? Use the following code:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#!/bin/bash</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> &#123;0..4&#125;;</span><br><span class=\"line\"><span class=\"keyword\">do</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> &#123;0..4&#125;;</span><br><span class=\"line\"><span class=\"keyword\">do</span> </span><br><span class=\"line\">A=$(grep -c <span class=\"string\">'D'</span>&#123;<span class=\"string\">'length(D)+1'</span>&#125;<span class=\"string\">' = \\['</span> Ds<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata)</span><br><span class=\"line\">B=$[<span class=\"variable\">$A</span>-1]</span><br><span class=\"line\">sed <span class=\"string\">'/D'</span>&#123;<span class=\"string\">'length(D)+1'</span>&#125;<span class=\"string\">'/&#123;G;s/\\nX\\&#123;'</span><span class=\"variable\">$B</span><span class=\"string\">'\\&#125;//;tend;x;s/^/X/;x;P;d&#125;;p;d;:end;s/D'</span>&#123;<span class=\"string\">'length(D)+1'</span>&#125;<span class=\"string\">'/D/;:a;n;ba'</span> Ds<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata&gt;Dt<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata</span><br><span class=\"line\">sed -n <span class=\"string\">'/D = \\[\\r/,/\\];/p'</span> Dt<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata &gt; D<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata</span><br><span class=\"line\"></span><br><span class=\"line\">C=$(cat D<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata |wc -l)</span><br><span class=\"line\">C=$[<span class=\"variable\">$C</span>-2]</span><br><span class=\"line\">D=$[<span class=\"variable\">$C</span>+1]</span><br><span class=\"line\"></span><br><span class=\"line\">sed -i -e <span class=\"string\">'s/D = \\[/D = structure(c(/'</span> -e <span class=\"string\">'s/\\];/),.Dim=c('</span>$(<span class=\"built_in\">echo</span> <span class=\"variable\">$C</span>)<span class=\"string\">','</span>$(<span class=\"built_in\">echo</span> <span class=\"variable\">$C</span>)<span class=\"string\">'))/'</span> -e <span class=\"string\">'s/;/ /g'</span> -e <span class=\"string\">'2,$&#123;s/ /,/g&#125;'</span> -e <span class=\"string\">'2,$&#123;s/,,/ /g&#125;'</span> -e <span class=\"string\">'3,'</span>$(<span class=\"built_in\">echo</span> <span class=\"variable\">$D</span>)<span class=\"string\">'&#123;s/^/,/g&#125;'</span> D<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata</span><br><span class=\"line\"><span class=\"built_in\">echo</span> <span class=\"variable\">$j</span><span class=\"variable\">$i</span><span class=\"string\">' done'</span></span><br><span class=\"line\"><span class=\"keyword\">done</span></span><br><span class=\"line\"><span class=\"keyword\">done</span></span><br></pre></td></tr></table></figure>\n<p>The command <code>grep</code> returns the number of the replicates of the line “\\(D\\left\\{length(D)+1\\right\\} = [\\)”. Then <code>sed</code> replaces the head of the last matrix with \\(D\\) and extracts this matrix out and writes into file Dij.Rdata. As I want to work in R with these results, I reformat the structure of the results to fit the matrix format in R which is something like:</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">D = structure(c(<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>),.Dim=c(<span class=\"number\">2</span>,<span class=\"number\">2</span>))</span><br></pre></td></tr></table></figure>\n<p>At last, I can directly <code>source</code> the file in R and read the matrix. </p>\n<p><img src=\"/2020/10/05/2018-10-29-sed/d.png\" alt=\"The last D matrix is captured.\"></p>\n<h2 id=\"Details-in-sed\"><a href=\"#Details-in-sed\" class=\"headerlink\" title=\"Details in sed\"></a>Details in sed</h2><p>The first <code>sed</code> in the second script is the key part in this function. </p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sed <span class=\"string\">'/D'</span>&#123;<span class=\"string\">'length(D)+1'</span>&#125;<span class=\"string\">'/&#123;G;s/\\nX\\&#123;'</span><span class=\"variable\">$B</span><span class=\"string\">'\\&#125;//;tend;x;s/^/X/;x;P;d&#125;;p;d;:end;s/D'</span>&#123;<span class=\"string\">'length(D)+1'</span>&#125;<span class=\"string\">'/D/;:a;n;ba'</span> Ds<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata&gt;Dt<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata</span><br></pre></td></tr></table></figure>\n<p>I don’t fully understand what every letter in this command means. In this <a href=\"https://superuser.com/questions/394282/sed-perform-only-first-nth-matched-replacement\" target=\"_blank\" rel=\"noopener\">answer</a>, the author explained the idea is to store a \\(X\\) at each match in the holdspace, and when all the \\(X\\)s are there, loop till the end of file. If you know more, please comment after the post. Thanks!</p>\n<p>The third <code>sed</code> reformats the matrix to fit in R. <code>-e</code> executes several commands in one line. </p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sed -i -e <span class=\"string\">'s/D = \\[/D = structure(c(/'</span> -e <span class=\"string\">'s/\\];/),.Dim=c('</span>$(<span class=\"built_in\">echo</span> <span class=\"variable\">$C</span>)<span class=\"string\">','</span>$(<span class=\"built_in\">echo</span> <span class=\"variable\">$C</span>)<span class=\"string\">'))/'</span> -e <span class=\"string\">'s/;/ /g'</span> -e <span class=\"string\">'2,$&#123;s/ /,/g&#125;'</span> -e <span class=\"string\">'2,$&#123;s/,,/ /g&#125;'</span> -e <span class=\"string\">'3,'</span>$(<span class=\"built_in\">echo</span> <span class=\"variable\">$D</span>)<span class=\"string\">'&#123;s/^/,/g&#125;'</span> D<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata</span><br></pre></td></tr></table></figure>\n<p>The first segment replaces “\\(D = [ \\)” with “\\(D = structure(c( \\)” while the second replaces the tail “];” with “(,.Dim=c(row,col)”. Then all the signs “;” are substituted by space and the single space is substituted by “,” from the second line to the end. Following the replacement of “,,” by space from the second line and add “,” at the beginning of the line from the third line on. Finally, a standard matrix form is rebuilt.  </p>\n<p>BTW, a good tutorial can be found <a href=\"http://www.grymoire.com/Unix/Sed.html\" target=\"_blank\" rel=\"noopener\">here</a>. Have fun! </p>"},{"title":"R update all packages","_content":"\nA convenient way to update all the packages.\n\n<!--more-->\n\n## Update all the packages\nVery simple, just one piece of code as follows:\n\n```R\nupdate.packages(ask = FALSE, dependencies = c('Suggests'))\n```\n\nRun it in the console of your Rstudio and take a cup of coffee. ","source":"_posts/2018-11-26-Rupdateallpackages.md","raw":"---\ntitle: R update all packages\ncategories: [Research, R]\ntags: [R,update, packages]\n---\n\nA convenient way to update all the packages.\n\n<!--more-->\n\n## Update all the packages\nVery simple, just one piece of code as follows:\n\n```R\nupdate.packages(ask = FALSE, dependencies = c('Suggests'))\n```\n\nRun it in the console of your Rstudio and take a cup of coffee. ","slug":"2018-11-26-Rupdateallpackages","published":1,"date":"2020-10-05T11:30:58.214Z","updated":"2020-10-05T11:30:58.214Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy12f000gnj39rfo06fgd","content":"<p>A convenient way to update all the packages.</p>\n<a id=\"more\"></a>\n<h2 id=\"Update-all-the-packages\"><a href=\"#Update-all-the-packages\" class=\"headerlink\" title=\"Update all the packages\"></a>Update all the packages</h2><p>Very simple, just one piece of code as follows:</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">update.packages(ask = <span class=\"literal\">FALSE</span>, dependencies = c(<span class=\"string\">'Suggests'</span>))</span><br></pre></td></tr></table></figure>\n<p>Run it in the console of your Rstudio and take a cup of coffee. </p>\n","site":{"data":{}},"excerpt":"<p>A convenient way to update all the packages.</p>","more":"<h2 id=\"Update-all-the-packages\"><a href=\"#Update-all-the-packages\" class=\"headerlink\" title=\"Update all the packages\"></a>Update all the packages</h2><p>Very simple, just one piece of code as follows:</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">update.packages(ask = <span class=\"literal\">FALSE</span>, dependencies = c(<span class=\"string\">'Suggests'</span>))</span><br></pre></td></tr></table></figure>\n<p>Run it in the console of your Rstudio and take a cup of coffee. </p>"},{"title":"Color your plots","_content":"\nShare a fancy [website](https://www.webdesignrankings.com/resources/lolcolors/) that shows \"Curated color palette\".\n\n<!--more-->\n\n## Gallery\nSome examples by using different colors. \n\n![colors 1](2018-11-29-color/col1.png)\n![colors 2](2018-11-29-color/col2.png)\n![colors 3](2018-11-29-color/col3.png)\n![colors 4](2018-11-29-color/col4.png)\n\nPlots are generated by [ggtree](http://bioconductor.org/packages/release/bioc/html/ggtree.html). And the data is simulated under our model. Coming soon!\n\nPS: which one do you prefer? Or all suck...","source":"_posts/2018-11-29-color.md","raw":"---\ntitle: Color your plots\ncategories: [Research, Data visualization, R]\ntags: [R,color, python, plot]\n---\n\nShare a fancy [website](https://www.webdesignrankings.com/resources/lolcolors/) that shows \"Curated color palette\".\n\n<!--more-->\n\n## Gallery\nSome examples by using different colors. \n\n![colors 1](2018-11-29-color/col1.png)\n![colors 2](2018-11-29-color/col2.png)\n![colors 3](2018-11-29-color/col3.png)\n![colors 4](2018-11-29-color/col4.png)\n\nPlots are generated by [ggtree](http://bioconductor.org/packages/release/bioc/html/ggtree.html). And the data is simulated under our model. Coming soon!\n\nPS: which one do you prefer? Or all suck...","slug":"2018-11-29-color","published":1,"date":"2020-10-05T11:30:58.214Z","updated":"2020-10-05T11:30:58.215Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy12g000hnj39lo0atwq7","content":"<p>Share a fancy <a href=\"https://www.webdesignrankings.com/resources/lolcolors/\" target=\"_blank\" rel=\"noopener\">website</a> that shows “Curated color palette”.</p>\n<a id=\"more\"></a>\n<h2 id=\"Gallery\"><a href=\"#Gallery\" class=\"headerlink\" title=\"Gallery\"></a>Gallery</h2><p>Some examples by using different colors. </p>\n<p><img src=\"/2020/10/05/2018-11-29-color/col1.png\" alt=\"colors 1\"><br><img src=\"/2020/10/05/2018-11-29-color/col2.png\" alt=\"colors 2\"><br><img src=\"/2020/10/05/2018-11-29-color/col3.png\" alt=\"colors 3\"><br><img src=\"/2020/10/05/2018-11-29-color/col4.png\" alt=\"colors 4\"></p>\n<p>Plots are generated by <a href=\"http://bioconductor.org/packages/release/bioc/html/ggtree.html\" target=\"_blank\" rel=\"noopener\">ggtree</a>. And the data is simulated under our model. Coming soon!</p>\n<p>PS: which one do you prefer? Or all suck…</p>\n","site":{"data":{}},"excerpt":"<p>Share a fancy <a href=\"https://www.webdesignrankings.com/resources/lolcolors/\" target=\"_blank\" rel=\"noopener\">website</a> that shows “Curated color palette”.</p>","more":"<h2 id=\"Gallery\"><a href=\"#Gallery\" class=\"headerlink\" title=\"Gallery\"></a>Gallery</h2><p>Some examples by using different colors. </p>\n<p><img src=\"/2020/10/05/2018-11-29-color/col1.png\" alt=\"colors 1\"><br><img src=\"/2020/10/05/2018-11-29-color/col2.png\" alt=\"colors 2\"><br><img src=\"/2020/10/05/2018-11-29-color/col3.png\" alt=\"colors 3\"><br><img src=\"/2020/10/05/2018-11-29-color/col4.png\" alt=\"colors 4\"></p>\n<p>Plots are generated by <a href=\"http://bioconductor.org/packages/release/bioc/html/ggtree.html\" target=\"_blank\" rel=\"noopener\">ggtree</a>. And the data is simulated under our model. Coming soon!</p>\n<p>PS: which one do you prefer? Or all suck…</p>"},{"title":"Draw the evolution of SMC","_content":"\nFinally, I have done visualizing the evolution of SMC in R instead of in Python. Here, two illustrations are given. \n\n<!--more-->\n\n## Generating Data\nThe data I used in the examples is generated under SMC applying on our trait-population coevolution model, a likelihood-free method for searching the true parameters when an analytic formulation of the model is not achievable. The data is consisting of 30 iterations samples. Each iteration contains 10K sampling. The algorithm starts by sampling from a uniform distribution in range (0,1). After each iteration, it computes the fitness of the samples and highly weights the samples with high fitness. Finally, it converges to the true value.  \n\n## Mountain plot\nMountain plot is recently highly evaluated theme for comparing the distribution of data across groups. The idea is stem from [Henrik's plot](https://twitter.com/hnrklndbrg/status/883675698300420098)\n\n![plot 1](2018-11-30-SMCplots/Rplot1.png)\n\nForget to point out the true value is at 1.0, which is the peak of the distribution at 30th iteration. \n\n## Heatmap plot\nAnother nice presentation is realized by the heat map. The data was manipulated a bit. The density of the samples was converted to the frequency in tiny cells. Then it can be applied by the heat map plot.\n\n![plot 1](2018-11-30-SMCplots/Rplot2.png)\n\nWhich one do you prefer?\n","source":"_posts/2018-11-30-SMCplots.md","raw":"---\ntitle: Draw the evolution of SMC\ncategories: [Research, Data visualization, R]\ntags: [R,ggplot, SMC, Data visualization]\n---\n\nFinally, I have done visualizing the evolution of SMC in R instead of in Python. Here, two illustrations are given. \n\n<!--more-->\n\n## Generating Data\nThe data I used in the examples is generated under SMC applying on our trait-population coevolution model, a likelihood-free method for searching the true parameters when an analytic formulation of the model is not achievable. The data is consisting of 30 iterations samples. Each iteration contains 10K sampling. The algorithm starts by sampling from a uniform distribution in range (0,1). After each iteration, it computes the fitness of the samples and highly weights the samples with high fitness. Finally, it converges to the true value.  \n\n## Mountain plot\nMountain plot is recently highly evaluated theme for comparing the distribution of data across groups. The idea is stem from [Henrik's plot](https://twitter.com/hnrklndbrg/status/883675698300420098)\n\n![plot 1](2018-11-30-SMCplots/Rplot1.png)\n\nForget to point out the true value is at 1.0, which is the peak of the distribution at 30th iteration. \n\n## Heatmap plot\nAnother nice presentation is realized by the heat map. The data was manipulated a bit. The density of the samples was converted to the frequency in tiny cells. Then it can be applied by the heat map plot.\n\n![plot 1](2018-11-30-SMCplots/Rplot2.png)\n\nWhich one do you prefer?\n","slug":"2018-11-30-SMCplots","published":1,"date":"2020-10-05T11:30:58.215Z","updated":"2020-10-05T11:30:58.216Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy12j000knj39y3w1kxyt","content":"<p>Finally, I have done visualizing the evolution of SMC in R instead of in Python. Here, two illustrations are given. </p>\n<a id=\"more\"></a>\n<h2 id=\"Generating-Data\"><a href=\"#Generating-Data\" class=\"headerlink\" title=\"Generating Data\"></a>Generating Data</h2><p>The data I used in the examples is generated under SMC applying on our trait-population coevolution model, a likelihood-free method for searching the true parameters when an analytic formulation of the model is not achievable. The data is consisting of 30 iterations samples. Each iteration contains 10K sampling. The algorithm starts by sampling from a uniform distribution in range (0,1). After each iteration, it computes the fitness of the samples and highly weights the samples with high fitness. Finally, it converges to the true value.  </p>\n<h2 id=\"Mountain-plot\"><a href=\"#Mountain-plot\" class=\"headerlink\" title=\"Mountain plot\"></a>Mountain plot</h2><p>Mountain plot is recently highly evaluated theme for comparing the distribution of data across groups. The idea is stem from <a href=\"https://twitter.com/hnrklndbrg/status/883675698300420098\" target=\"_blank\" rel=\"noopener\">Henrik’s plot</a></p>\n<p><img src=\"/2020/10/05/2018-11-30-SMCplots/Rplot1.png\" alt=\"plot 1\"></p>\n<p>Forget to point out the true value is at 1.0, which is the peak of the distribution at 30th iteration. </p>\n<h2 id=\"Heatmap-plot\"><a href=\"#Heatmap-plot\" class=\"headerlink\" title=\"Heatmap plot\"></a>Heatmap plot</h2><p>Another nice presentation is realized by the heat map. The data was manipulated a bit. The density of the samples was converted to the frequency in tiny cells. Then it can be applied by the heat map plot.</p>\n<p><img src=\"/2020/10/05/2018-11-30-SMCplots/Rplot2.png\" alt=\"plot 1\"></p>\n<p>Which one do you prefer?</p>\n","site":{"data":{}},"excerpt":"<p>Finally, I have done visualizing the evolution of SMC in R instead of in Python. Here, two illustrations are given. </p>","more":"<h2 id=\"Generating-Data\"><a href=\"#Generating-Data\" class=\"headerlink\" title=\"Generating Data\"></a>Generating Data</h2><p>The data I used in the examples is generated under SMC applying on our trait-population coevolution model, a likelihood-free method for searching the true parameters when an analytic formulation of the model is not achievable. The data is consisting of 30 iterations samples. Each iteration contains 10K sampling. The algorithm starts by sampling from a uniform distribution in range (0,1). After each iteration, it computes the fitness of the samples and highly weights the samples with high fitness. Finally, it converges to the true value.  </p>\n<h2 id=\"Mountain-plot\"><a href=\"#Mountain-plot\" class=\"headerlink\" title=\"Mountain plot\"></a>Mountain plot</h2><p>Mountain plot is recently highly evaluated theme for comparing the distribution of data across groups. The idea is stem from <a href=\"https://twitter.com/hnrklndbrg/status/883675698300420098\" target=\"_blank\" rel=\"noopener\">Henrik’s plot</a></p>\n<p><img src=\"/2020/10/05/2018-11-30-SMCplots/Rplot1.png\" alt=\"plot 1\"></p>\n<p>Forget to point out the true value is at 1.0, which is the peak of the distribution at 30th iteration. </p>\n<h2 id=\"Heatmap-plot\"><a href=\"#Heatmap-plot\" class=\"headerlink\" title=\"Heatmap plot\"></a>Heatmap plot</h2><p>Another nice presentation is realized by the heat map. The data was manipulated a bit. The density of the samples was converted to the frequency in tiny cells. Then it can be applied by the heat map plot.</p>\n<p><img src=\"/2020/10/05/2018-11-30-SMCplots/Rplot2.png\" alt=\"plot 1\"></p>\n<p>Which one do you prefer?</p>"},{"title":"ggradar2: Help document ","_content":"\nIn this blog, the details of the arguments used in [ggradar2](https://github.com/xl0418/ggradar2) are provided.\n\n<!--more-->\n\n## ggradar2: arguments\n\n\n**plot.data** :\nThe input data should be in the data.frame format with columns named. The values in the data are supposed to be numeric. If you want to apply scales like 'high', 'middle', 'low'. Please evaluate them first and put them as `gridline.label` . \n\nNotice that 'group' column is suggested to included in your data. `ggradar2` now can smartly detect if 'group' is correctly provided. If not, you will be asked if the first column is allowed to defined as the group column. \n\n```\nWARNING: 'group' column is not detected. The first column will be chosen as the group name. Yes/no? (y/n)\n```\n\nIf you want to plot multiple plots against some subgroups, please specify it in the column `data$facet1`.\n\n**base.size**: The size of radar chart. The default value is 20. \n\n**webtype**: `\"mini\"` set a web type with 3 grid lines while `\"lux\"` set a type with 5 grid lines. Default setting is `\"mini\"`.\n\n**axis.labels**:  The label of each column in your data is plotted around the radar. \n\n**axis.label.offset**: The offset of axis labels.\n\n**axis.label.size**: The size of axis labels.\n\n**axis.line.colour**: The color of axis labels.\n\n**grid.min, grid.max**: Rescale your values in this range.\n\n**centre.y**: The radius of inner circle. \n\n**label.centre.y**: `TRUE` prints the central value. `FALSE` turns it off.\n\n**grid.line.width**: The width of grid lines.\n\n**grid.line.trend**: `\"classic\"` sets equal width of the grid lines. `\"increase\"` sets an outward-increasing width of the grid lines. `\"decrease\"` sets an outward-decreasing width of the grid lines.\n\n**gridline.min.linetype, gridline.mid.linetype, gridline.max.linetype**: Set the grid line type for the inner, middle and outer circles. The default setting is `\"longdash\"`.\n\n**gridline.min.colour, gridline.mid.colour, gridline.max.colour**: Set the colors for the inner, middle and outer circles. The default settings are `\"grey\", \"#007A87\", \"grey\"`.\n\n**grid.label.size**: Set the size of grid labels.\n\n**gridline.label.offset**: The offset of grid labels.\n\n**gridline.label**: The default setting is the percentage. Replace it with your labels.\n\n**group.line.width**: The width of group lines.\n \n**group.point.size**: The size of the point in each axis for group lines.\n\n**group.colours**: Set colors for the group lines.\n\n**polygonfill**: Turn on/off the polygon fill.\n\n**polygonfill.transparency**: The transparency of polygon fills.\n\n**group.fill.colours**: The colors of polygon fills.\n\n**background.circle.colour**: The background color for the radar.\n\n**background.circle.transparency**: The transparency of the background.\n\n**radarshape**: `\"round\"` gives you a round radar. `\"sharp\"` gives you a sharp radar.\n\n**multiplots**: Turn on/off multi-plotting function. If on, `data$facet1` column should be included in your data. TRUE/FALSE\n\n**fullscore**: Set full scores to your values. \n\n**stripbackground**: Turn on/off the background for the panels of multiple plots.\n\nThe followings are basic settings.\n\n```R\nlegend.title=\"\",\nplot.legend=TRUE,\nplot.title=\"\",\nlegend.text.size=14,\n```","source":"_posts/2018-12-07-ggradar2helpdocument.md","raw":"---\ntitle: \"ggradar2: Help document \"\ncategories: [Research, Data visualization, R, ggradar2]\ntags: [R,ggplot, ggradar2, Data visualization,help document]\n---\n\nIn this blog, the details of the arguments used in [ggradar2](https://github.com/xl0418/ggradar2) are provided.\n\n<!--more-->\n\n## ggradar2: arguments\n\n\n**plot.data** :\nThe input data should be in the data.frame format with columns named. The values in the data are supposed to be numeric. If you want to apply scales like 'high', 'middle', 'low'. Please evaluate them first and put them as `gridline.label` . \n\nNotice that 'group' column is suggested to included in your data. `ggradar2` now can smartly detect if 'group' is correctly provided. If not, you will be asked if the first column is allowed to defined as the group column. \n\n```\nWARNING: 'group' column is not detected. The first column will be chosen as the group name. Yes/no? (y/n)\n```\n\nIf you want to plot multiple plots against some subgroups, please specify it in the column `data$facet1`.\n\n**base.size**: The size of radar chart. The default value is 20. \n\n**webtype**: `\"mini\"` set a web type with 3 grid lines while `\"lux\"` set a type with 5 grid lines. Default setting is `\"mini\"`.\n\n**axis.labels**:  The label of each column in your data is plotted around the radar. \n\n**axis.label.offset**: The offset of axis labels.\n\n**axis.label.size**: The size of axis labels.\n\n**axis.line.colour**: The color of axis labels.\n\n**grid.min, grid.max**: Rescale your values in this range.\n\n**centre.y**: The radius of inner circle. \n\n**label.centre.y**: `TRUE` prints the central value. `FALSE` turns it off.\n\n**grid.line.width**: The width of grid lines.\n\n**grid.line.trend**: `\"classic\"` sets equal width of the grid lines. `\"increase\"` sets an outward-increasing width of the grid lines. `\"decrease\"` sets an outward-decreasing width of the grid lines.\n\n**gridline.min.linetype, gridline.mid.linetype, gridline.max.linetype**: Set the grid line type for the inner, middle and outer circles. The default setting is `\"longdash\"`.\n\n**gridline.min.colour, gridline.mid.colour, gridline.max.colour**: Set the colors for the inner, middle and outer circles. The default settings are `\"grey\", \"#007A87\", \"grey\"`.\n\n**grid.label.size**: Set the size of grid labels.\n\n**gridline.label.offset**: The offset of grid labels.\n\n**gridline.label**: The default setting is the percentage. Replace it with your labels.\n\n**group.line.width**: The width of group lines.\n \n**group.point.size**: The size of the point in each axis for group lines.\n\n**group.colours**: Set colors for the group lines.\n\n**polygonfill**: Turn on/off the polygon fill.\n\n**polygonfill.transparency**: The transparency of polygon fills.\n\n**group.fill.colours**: The colors of polygon fills.\n\n**background.circle.colour**: The background color for the radar.\n\n**background.circle.transparency**: The transparency of the background.\n\n**radarshape**: `\"round\"` gives you a round radar. `\"sharp\"` gives you a sharp radar.\n\n**multiplots**: Turn on/off multi-plotting function. If on, `data$facet1` column should be included in your data. TRUE/FALSE\n\n**fullscore**: Set full scores to your values. \n\n**stripbackground**: Turn on/off the background for the panels of multiple plots.\n\nThe followings are basic settings.\n\n```R\nlegend.title=\"\",\nplot.legend=TRUE,\nplot.title=\"\",\nlegend.text.size=14,\n```","slug":"2018-12-07-ggradar2helpdocument","published":1,"date":"2020-10-05T11:30:58.220Z","updated":"2020-10-05T11:30:58.220Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy12n000mnj39nfs62fxf","content":"<p>In this blog, the details of the arguments used in <a href=\"https://github.com/xl0418/ggradar2\" target=\"_blank\" rel=\"noopener\">ggradar2</a> are provided.</p>\n<a id=\"more\"></a>\n<h2 id=\"ggradar2-arguments\"><a href=\"#ggradar2-arguments\" class=\"headerlink\" title=\"ggradar2: arguments\"></a>ggradar2: arguments</h2><p><strong>plot.data</strong> :<br>The input data should be in the data.frame format with columns named. The values in the data are supposed to be numeric. If you want to apply scales like ‘high’, ‘middle’, ‘low’. Please evaluate them first and put them as <code>gridline.label</code> . </p>\n<p>Notice that ‘group’ column is suggested to included in your data. <code>ggradar2</code> now can smartly detect if ‘group’ is correctly provided. If not, you will be asked if the first column is allowed to defined as the group column. </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">WARNING: &apos;group&apos; column is not detected. The first column will be chosen as the group name. Yes/no? (y/n)</span><br></pre></td></tr></table></figure>\n<p>If you want to plot multiple plots against some subgroups, please specify it in the column <code>data$facet1</code>.</p>\n<p><strong>base.size</strong>: The size of radar chart. The default value is 20. </p>\n<p><strong>webtype</strong>: <code>&quot;mini&quot;</code> set a web type with 3 grid lines while <code>&quot;lux&quot;</code> set a type with 5 grid lines. Default setting is <code>&quot;mini&quot;</code>.</p>\n<p><strong>axis.labels</strong>:  The label of each column in your data is plotted around the radar. </p>\n<p><strong>axis.label.offset</strong>: The offset of axis labels.</p>\n<p><strong>axis.label.size</strong>: The size of axis labels.</p>\n<p><strong>axis.line.colour</strong>: The color of axis labels.</p>\n<p><strong>grid.min, grid.max</strong>: Rescale your values in this range.</p>\n<p><strong>centre.y</strong>: The radius of inner circle. </p>\n<p><strong>label.centre.y</strong>: <code>TRUE</code> prints the central value. <code>FALSE</code> turns it off.</p>\n<p><strong>grid.line.width</strong>: The width of grid lines.</p>\n<p><strong>grid.line.trend</strong>: <code>&quot;classic&quot;</code> sets equal width of the grid lines. <code>&quot;increase&quot;</code> sets an outward-increasing width of the grid lines. <code>&quot;decrease&quot;</code> sets an outward-decreasing width of the grid lines.</p>\n<p><strong>gridline.min.linetype, gridline.mid.linetype, gridline.max.linetype</strong>: Set the grid line type for the inner, middle and outer circles. The default setting is <code>&quot;longdash&quot;</code>.</p>\n<p><strong>gridline.min.colour, gridline.mid.colour, gridline.max.colour</strong>: Set the colors for the inner, middle and outer circles. The default settings are <code>&quot;grey&quot;, &quot;#007A87&quot;, &quot;grey&quot;</code>.</p>\n<p><strong>grid.label.size</strong>: Set the size of grid labels.</p>\n<p><strong>gridline.label.offset</strong>: The offset of grid labels.</p>\n<p><strong>gridline.label</strong>: The default setting is the percentage. Replace it with your labels.</p>\n<p><strong>group.line.width</strong>: The width of group lines.</p>\n<p><strong>group.point.size</strong>: The size of the point in each axis for group lines.</p>\n<p><strong>group.colours</strong>: Set colors for the group lines.</p>\n<p><strong>polygonfill</strong>: Turn on/off the polygon fill.</p>\n<p><strong>polygonfill.transparency</strong>: The transparency of polygon fills.</p>\n<p><strong>group.fill.colours</strong>: The colors of polygon fills.</p>\n<p><strong>background.circle.colour</strong>: The background color for the radar.</p>\n<p><strong>background.circle.transparency</strong>: The transparency of the background.</p>\n<p><strong>radarshape</strong>: <code>&quot;round&quot;</code> gives you a round radar. <code>&quot;sharp&quot;</code> gives you a sharp radar.</p>\n<p><strong>multiplots</strong>: Turn on/off multi-plotting function. If on, <code>data$facet1</code> column should be included in your data. TRUE/FALSE</p>\n<p><strong>fullscore</strong>: Set full scores to your values. </p>\n<p><strong>stripbackground</strong>: Turn on/off the background for the panels of multiple plots.</p>\n<p>The followings are basic settings.</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">legend.title=<span class=\"string\">\"\"</span>,</span><br><span class=\"line\">plot.legend=<span class=\"literal\">TRUE</span>,</span><br><span class=\"line\">plot.title=<span class=\"string\">\"\"</span>,</span><br><span class=\"line\">legend.text.size=<span class=\"number\">14</span>,</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"<p>In this blog, the details of the arguments used in <a href=\"https://github.com/xl0418/ggradar2\" target=\"_blank\" rel=\"noopener\">ggradar2</a> are provided.</p>","more":"<h2 id=\"ggradar2-arguments\"><a href=\"#ggradar2-arguments\" class=\"headerlink\" title=\"ggradar2: arguments\"></a>ggradar2: arguments</h2><p><strong>plot.data</strong> :<br>The input data should be in the data.frame format with columns named. The values in the data are supposed to be numeric. If you want to apply scales like ‘high’, ‘middle’, ‘low’. Please evaluate them first and put them as <code>gridline.label</code> . </p>\n<p>Notice that ‘group’ column is suggested to included in your data. <code>ggradar2</code> now can smartly detect if ‘group’ is correctly provided. If not, you will be asked if the first column is allowed to defined as the group column. </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">WARNING: &apos;group&apos; column is not detected. The first column will be chosen as the group name. Yes/no? (y/n)</span><br></pre></td></tr></table></figure>\n<p>If you want to plot multiple plots against some subgroups, please specify it in the column <code>data$facet1</code>.</p>\n<p><strong>base.size</strong>: The size of radar chart. The default value is 20. </p>\n<p><strong>webtype</strong>: <code>&quot;mini&quot;</code> set a web type with 3 grid lines while <code>&quot;lux&quot;</code> set a type with 5 grid lines. Default setting is <code>&quot;mini&quot;</code>.</p>\n<p><strong>axis.labels</strong>:  The label of each column in your data is plotted around the radar. </p>\n<p><strong>axis.label.offset</strong>: The offset of axis labels.</p>\n<p><strong>axis.label.size</strong>: The size of axis labels.</p>\n<p><strong>axis.line.colour</strong>: The color of axis labels.</p>\n<p><strong>grid.min, grid.max</strong>: Rescale your values in this range.</p>\n<p><strong>centre.y</strong>: The radius of inner circle. </p>\n<p><strong>label.centre.y</strong>: <code>TRUE</code> prints the central value. <code>FALSE</code> turns it off.</p>\n<p><strong>grid.line.width</strong>: The width of grid lines.</p>\n<p><strong>grid.line.trend</strong>: <code>&quot;classic&quot;</code> sets equal width of the grid lines. <code>&quot;increase&quot;</code> sets an outward-increasing width of the grid lines. <code>&quot;decrease&quot;</code> sets an outward-decreasing width of the grid lines.</p>\n<p><strong>gridline.min.linetype, gridline.mid.linetype, gridline.max.linetype</strong>: Set the grid line type for the inner, middle and outer circles. The default setting is <code>&quot;longdash&quot;</code>.</p>\n<p><strong>gridline.min.colour, gridline.mid.colour, gridline.max.colour</strong>: Set the colors for the inner, middle and outer circles. The default settings are <code>&quot;grey&quot;, &quot;#007A87&quot;, &quot;grey&quot;</code>.</p>\n<p><strong>grid.label.size</strong>: Set the size of grid labels.</p>\n<p><strong>gridline.label.offset</strong>: The offset of grid labels.</p>\n<p><strong>gridline.label</strong>: The default setting is the percentage. Replace it with your labels.</p>\n<p><strong>group.line.width</strong>: The width of group lines.</p>\n<p><strong>group.point.size</strong>: The size of the point in each axis for group lines.</p>\n<p><strong>group.colours</strong>: Set colors for the group lines.</p>\n<p><strong>polygonfill</strong>: Turn on/off the polygon fill.</p>\n<p><strong>polygonfill.transparency</strong>: The transparency of polygon fills.</p>\n<p><strong>group.fill.colours</strong>: The colors of polygon fills.</p>\n<p><strong>background.circle.colour</strong>: The background color for the radar.</p>\n<p><strong>background.circle.transparency</strong>: The transparency of the background.</p>\n<p><strong>radarshape</strong>: <code>&quot;round&quot;</code> gives you a round radar. <code>&quot;sharp&quot;</code> gives you a sharp radar.</p>\n<p><strong>multiplots</strong>: Turn on/off multi-plotting function. If on, <code>data$facet1</code> column should be included in your data. TRUE/FALSE</p>\n<p><strong>fullscore</strong>: Set full scores to your values. </p>\n<p><strong>stripbackground</strong>: Turn on/off the background for the panels of multiple plots.</p>\n<p>The followings are basic settings.</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">legend.title=<span class=\"string\">\"\"</span>,</span><br><span class=\"line\">plot.legend=<span class=\"literal\">TRUE</span>,</span><br><span class=\"line\">plot.title=<span class=\"string\">\"\"</span>,</span><br><span class=\"line\">legend.text.size=<span class=\"number\">14</span>,</span><br></pre></td></tr></table></figure>"},{"title":"ggradar2: deploy a radar to your data ","_content":"\n[`ggradar2`](https://github.com/xl0418/ggradar2) is  now available. A large amount of features have been added to make your radar chart powerful. See [`ggradar2`](https://github.com/xl0418/ggradar2) on my [Github](https://github.com/xl0418)\n\n<!--more-->\n\n## Introduction\nggradar2 is a gg-function to draw a radar plot for data analysis. It is stem from [ggradar](https://github.com/ricardo-bion/ggradar) but has been extended with more cool features. \n\n\n## Install ggradar2\nRun the code \n\n```R\ndevtools::install_github(\"xl0418/ggradar2\",dependencies=TRUE)\n```\n\n## Use ggradar2\n### Load data.\n\n```R\nlibrary(ggradar2)\ndata(mtcars)\n% Extract the group names. Otherwise, the first column will be chosen as the group names.\ngroup = row.names(mtcars)\ndf = cbind(group,mtcars)\n% The radar chart is not a nice presentation if you want to compare too many groups. Thus here \n% we only focus on 4 groups.\ndftest = head(df,4)\n% To better distinguish two different styles, 6 groups are selected for illustration.\ndftest = dftest[,1:7]\n```\n\n### Default style\nBy default\n\n```R\nggradar2(dftest)\n```\n\nreturns \n\n![default](2018-12-05-ggradar2/roundfill.png)\n\n### No fill with round grid\nIf you don't want to fill the polygon, run\n\n```R\nggradar2(dftest,polygonfill = FALSE)\n```\n\n![default](2018-12-05-ggradar2/roundnofill.png)\n\n### Web type\nA new web type 'lux' has been added by `webtype`.\n\nmini type\n\n```R\nggradar2(dftest,webtype = 'mini')\n```\n\n![default](2018-12-05-ggradar2/mini.png)\n\nluxurious type\n\n```R\nggradar2(dftest,webtype = 'lux')\n```\n\n![default](2018-12-05-ggradar2/lux.png)\n\n\n### Gird line trend\nUse `grid.line.trend = 'increase'` to plot an outward-increasing grid lines.\n\n```R\nggradar2(dftest,style = 'sharp',webtype = 'lux',\n              group.line.width = 0.5,grid.line.trend = 'increase',gridline.min.linetype = 'solid',\n              gridline.max.linetype = 'solid',gridline.min.colour = 'black',gridline.max.colour='black')\n```\n\n![default](2018-12-05-ggradar2/trend.png)\n\n\n### Full score\nUse `fullscore = c(...)` to set the full score to each variable.\n\n```R\nfullscore <- c(100,10,300,150,10,10)\na <- ggradar2(dftest,fullscore = fullscore)\n```\n\n![default](2018-12-05-ggradar2/fullscore.png)\n\n\n### Sharp grid\nA new style has been added. Call out the straight line style by running \n\n```R\nggradar2(dftest,style = 'sharp')\n```\n\n![default](2018-12-05-ggradar2/straightfill.png)\n\n### Sharp grid without fill\nGet rid of the fill\n\n```R\nggradar2(dftest,style = 'sharp',polygonfill = FALSE)\n```\n\n![default](2018-12-05-ggradar2/straightnofill.png)\n\n### Removing the legend \n\n```R\nggradar2(dftest,style = 'sharp',\npolygonfill = FALSE,plot.legend = FALSE)\n```\n\n![default](2018-12-05-ggradar2/nolegend.png)\n\n### Multiple plots by subgroups\n\n```R\n# Extract 3 brands of cars out of the data frame\nfacettest <- df[c(1,2,4,5,8:14),]\n# Set the subgroup names\nfacet1 <- mapply(rep,c('Mazda','Hornet','Merc'),c(2,2,7))\nfacet1 <- Reduce(c,facet1)\nfacettest <- cbind(facettest,facet1)\nggradar2(facettest,multiplots = TRUE)\n```\n\nNotice that the column name for the subgroups should be 'facet1'. Otherwise, ggradar2 could not recognize it.\n\n![default](2018-12-05-ggradar2/multipleplotsbig.png)","source":"_posts/2018-12-05-ggradar2.md","raw":"---\ntitle: \"ggradar2: deploy a radar to your data \"\ncategories: [Research, Data visualization, R, ggradar2]\ntags: [R,ggplot, ggradar2, Data visualization]\n---\n\n[`ggradar2`](https://github.com/xl0418/ggradar2) is  now available. A large amount of features have been added to make your radar chart powerful. See [`ggradar2`](https://github.com/xl0418/ggradar2) on my [Github](https://github.com/xl0418)\n\n<!--more-->\n\n## Introduction\nggradar2 is a gg-function to draw a radar plot for data analysis. It is stem from [ggradar](https://github.com/ricardo-bion/ggradar) but has been extended with more cool features. \n\n\n## Install ggradar2\nRun the code \n\n```R\ndevtools::install_github(\"xl0418/ggradar2\",dependencies=TRUE)\n```\n\n## Use ggradar2\n### Load data.\n\n```R\nlibrary(ggradar2)\ndata(mtcars)\n% Extract the group names. Otherwise, the first column will be chosen as the group names.\ngroup = row.names(mtcars)\ndf = cbind(group,mtcars)\n% The radar chart is not a nice presentation if you want to compare too many groups. Thus here \n% we only focus on 4 groups.\ndftest = head(df,4)\n% To better distinguish two different styles, 6 groups are selected for illustration.\ndftest = dftest[,1:7]\n```\n\n### Default style\nBy default\n\n```R\nggradar2(dftest)\n```\n\nreturns \n\n![default](2018-12-05-ggradar2/roundfill.png)\n\n### No fill with round grid\nIf you don't want to fill the polygon, run\n\n```R\nggradar2(dftest,polygonfill = FALSE)\n```\n\n![default](2018-12-05-ggradar2/roundnofill.png)\n\n### Web type\nA new web type 'lux' has been added by `webtype`.\n\nmini type\n\n```R\nggradar2(dftest,webtype = 'mini')\n```\n\n![default](2018-12-05-ggradar2/mini.png)\n\nluxurious type\n\n```R\nggradar2(dftest,webtype = 'lux')\n```\n\n![default](2018-12-05-ggradar2/lux.png)\n\n\n### Gird line trend\nUse `grid.line.trend = 'increase'` to plot an outward-increasing grid lines.\n\n```R\nggradar2(dftest,style = 'sharp',webtype = 'lux',\n              group.line.width = 0.5,grid.line.trend = 'increase',gridline.min.linetype = 'solid',\n              gridline.max.linetype = 'solid',gridline.min.colour = 'black',gridline.max.colour='black')\n```\n\n![default](2018-12-05-ggradar2/trend.png)\n\n\n### Full score\nUse `fullscore = c(...)` to set the full score to each variable.\n\n```R\nfullscore <- c(100,10,300,150,10,10)\na <- ggradar2(dftest,fullscore = fullscore)\n```\n\n![default](2018-12-05-ggradar2/fullscore.png)\n\n\n### Sharp grid\nA new style has been added. Call out the straight line style by running \n\n```R\nggradar2(dftest,style = 'sharp')\n```\n\n![default](2018-12-05-ggradar2/straightfill.png)\n\n### Sharp grid without fill\nGet rid of the fill\n\n```R\nggradar2(dftest,style = 'sharp',polygonfill = FALSE)\n```\n\n![default](2018-12-05-ggradar2/straightnofill.png)\n\n### Removing the legend \n\n```R\nggradar2(dftest,style = 'sharp',\npolygonfill = FALSE,plot.legend = FALSE)\n```\n\n![default](2018-12-05-ggradar2/nolegend.png)\n\n### Multiple plots by subgroups\n\n```R\n# Extract 3 brands of cars out of the data frame\nfacettest <- df[c(1,2,4,5,8:14),]\n# Set the subgroup names\nfacet1 <- mapply(rep,c('Mazda','Hornet','Merc'),c(2,2,7))\nfacet1 <- Reduce(c,facet1)\nfacettest <- cbind(facettest,facet1)\nggradar2(facettest,multiplots = TRUE)\n```\n\nNotice that the column name for the subgroups should be 'facet1'. Otherwise, ggradar2 could not recognize it.\n\n![default](2018-12-05-ggradar2/multipleplotsbig.png)","slug":"2018-12-05-ggradar2","published":1,"date":"2020-10-05T11:30:58.216Z","updated":"2020-10-05T11:30:58.216Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy12p000qnj39hhnq6saz","content":"<p><a href=\"https://github.com/xl0418/ggradar2\" target=\"_blank\" rel=\"noopener\"><code>ggradar2</code></a> is  now available. A large amount of features have been added to make your radar chart powerful. See <a href=\"https://github.com/xl0418/ggradar2\" target=\"_blank\" rel=\"noopener\"><code>ggradar2</code></a> on my <a href=\"https://github.com/xl0418\" target=\"_blank\" rel=\"noopener\">Github</a></p>\n<a id=\"more\"></a>\n<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>ggradar2 is a gg-function to draw a radar plot for data analysis. It is stem from <a href=\"https://github.com/ricardo-bion/ggradar\" target=\"_blank\" rel=\"noopener\">ggradar</a> but has been extended with more cool features. </p>\n<h2 id=\"Install-ggradar2\"><a href=\"#Install-ggradar2\" class=\"headerlink\" title=\"Install ggradar2\"></a>Install ggradar2</h2><p>Run the code </p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">devtools::install_github(<span class=\"string\">\"xl0418/ggradar2\"</span>,dependencies=<span class=\"literal\">TRUE</span>)</span><br></pre></td></tr></table></figure>\n<h2 id=\"Use-ggradar2\"><a href=\"#Use-ggradar2\" class=\"headerlink\" title=\"Use ggradar2\"></a>Use ggradar2</h2><h3 id=\"Load-data\"><a href=\"#Load-data\" class=\"headerlink\" title=\"Load data.\"></a>Load data.</h3><figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">library</span>(ggradar2)</span><br><span class=\"line\">data(mtcars)</span><br><span class=\"line\">% Extract the group names. Otherwise, the first column will be chosen as the group names.</span><br><span class=\"line\">group = row.names(mtcars)</span><br><span class=\"line\">df = cbind(group,mtcars)</span><br><span class=\"line\">% The radar chart is not a nice presentation <span class=\"keyword\">if</span> you want to compare too many groups. Thus here </span><br><span class=\"line\">% we only focus on <span class=\"number\">4</span> groups.</span><br><span class=\"line\">dftest = head(df,<span class=\"number\">4</span>)</span><br><span class=\"line\">% To better distinguish two different styles, <span class=\"number\">6</span> groups are selected <span class=\"keyword\">for</span> illustration.</span><br><span class=\"line\">dftest = dftest[,<span class=\"number\">1</span>:<span class=\"number\">7</span>]</span><br></pre></td></tr></table></figure>\n<h3 id=\"Default-style\"><a href=\"#Default-style\" class=\"headerlink\" title=\"Default style\"></a>Default style</h3><p>By default</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ggradar2(dftest)</span><br></pre></td></tr></table></figure>\n<p>returns </p>\n<p><img src=\"/2020/10/05/2018-12-05-ggradar2/roundfill.png\" alt=\"default\"></p>\n<h3 id=\"No-fill-with-round-grid\"><a href=\"#No-fill-with-round-grid\" class=\"headerlink\" title=\"No fill with round grid\"></a>No fill with round grid</h3><p>If you don’t want to fill the polygon, run</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ggradar2(dftest,polygonfill = <span class=\"literal\">FALSE</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2020/10/05/2018-12-05-ggradar2/roundnofill.png\" alt=\"default\"></p>\n<h3 id=\"Web-type\"><a href=\"#Web-type\" class=\"headerlink\" title=\"Web type\"></a>Web type</h3><p>A new web type ‘lux’ has been added by <code>webtype</code>.</p>\n<p>mini type</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ggradar2(dftest,webtype = <span class=\"string\">'mini'</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2020/10/05/2018-12-05-ggradar2/mini.png\" alt=\"default\"></p>\n<p>luxurious type</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ggradar2(dftest,webtype = <span class=\"string\">'lux'</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2020/10/05/2018-12-05-ggradar2/lux.png\" alt=\"default\"></p>\n<h3 id=\"Gird-line-trend\"><a href=\"#Gird-line-trend\" class=\"headerlink\" title=\"Gird line trend\"></a>Gird line trend</h3><p>Use <code>grid.line.trend = &#39;increase&#39;</code> to plot an outward-increasing grid lines.</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ggradar2(dftest,style = <span class=\"string\">'sharp'</span>,webtype = <span class=\"string\">'lux'</span>,</span><br><span class=\"line\">              group.line.width = <span class=\"number\">0.5</span>,grid.line.trend = <span class=\"string\">'increase'</span>,gridline.min.linetype = <span class=\"string\">'solid'</span>,</span><br><span class=\"line\">              gridline.max.linetype = <span class=\"string\">'solid'</span>,gridline.min.colour = <span class=\"string\">'black'</span>,gridline.max.colour=<span class=\"string\">'black'</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2020/10/05/2018-12-05-ggradar2/trend.png\" alt=\"default\"></p>\n<h3 id=\"Full-score\"><a href=\"#Full-score\" class=\"headerlink\" title=\"Full score\"></a>Full score</h3><p>Use <code>fullscore = c(...)</code> to set the full score to each variable.</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fullscore &lt;- c(<span class=\"number\">100</span>,<span class=\"number\">10</span>,<span class=\"number\">300</span>,<span class=\"number\">150</span>,<span class=\"number\">10</span>,<span class=\"number\">10</span>)</span><br><span class=\"line\">a &lt;- ggradar2(dftest,fullscore = fullscore)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2020/10/05/2018-12-05-ggradar2/fullscore.png\" alt=\"default\"></p>\n<h3 id=\"Sharp-grid\"><a href=\"#Sharp-grid\" class=\"headerlink\" title=\"Sharp grid\"></a>Sharp grid</h3><p>A new style has been added. Call out the straight line style by running </p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ggradar2(dftest,style = <span class=\"string\">'sharp'</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2020/10/05/2018-12-05-ggradar2/straightfill.png\" alt=\"default\"></p>\n<h3 id=\"Sharp-grid-without-fill\"><a href=\"#Sharp-grid-without-fill\" class=\"headerlink\" title=\"Sharp grid without fill\"></a>Sharp grid without fill</h3><p>Get rid of the fill</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ggradar2(dftest,style = <span class=\"string\">'sharp'</span>,polygonfill = <span class=\"literal\">FALSE</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2020/10/05/2018-12-05-ggradar2/straightnofill.png\" alt=\"default\"></p>\n<h3 id=\"Removing-the-legend\"><a href=\"#Removing-the-legend\" class=\"headerlink\" title=\"Removing the legend\"></a>Removing the legend</h3><figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ggradar2(dftest,style = <span class=\"string\">'sharp'</span>,</span><br><span class=\"line\">polygonfill = <span class=\"literal\">FALSE</span>,plot.legend = <span class=\"literal\">FALSE</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2020/10/05/2018-12-05-ggradar2/nolegend.png\" alt=\"default\"></p>\n<h3 id=\"Multiple-plots-by-subgroups\"><a href=\"#Multiple-plots-by-subgroups\" class=\"headerlink\" title=\"Multiple plots by subgroups\"></a>Multiple plots by subgroups</h3><figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Extract 3 brands of cars out of the data frame</span></span><br><span class=\"line\">facettest &lt;- df[c(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">8</span>:<span class=\"number\">14</span>),]</span><br><span class=\"line\"><span class=\"comment\"># Set the subgroup names</span></span><br><span class=\"line\">facet1 &lt;- mapply(rep,c(<span class=\"string\">'Mazda'</span>,<span class=\"string\">'Hornet'</span>,<span class=\"string\">'Merc'</span>),c(<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">7</span>))</span><br><span class=\"line\">facet1 &lt;- Reduce(c,facet1)</span><br><span class=\"line\">facettest &lt;- cbind(facettest,facet1)</span><br><span class=\"line\">ggradar2(facettest,multiplots = <span class=\"literal\">TRUE</span>)</span><br></pre></td></tr></table></figure>\n<p>Notice that the column name for the subgroups should be ‘facet1’. Otherwise, ggradar2 could not recognize it.</p>\n<p><img src=\"/2020/10/05/2018-12-05-ggradar2/multipleplotsbig.png\" alt=\"default\"></p>\n","site":{"data":{}},"excerpt":"<p><a href=\"https://github.com/xl0418/ggradar2\" target=\"_blank\" rel=\"noopener\"><code>ggradar2</code></a> is  now available. A large amount of features have been added to make your radar chart powerful. See <a href=\"https://github.com/xl0418/ggradar2\" target=\"_blank\" rel=\"noopener\"><code>ggradar2</code></a> on my <a href=\"https://github.com/xl0418\" target=\"_blank\" rel=\"noopener\">Github</a></p>","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>ggradar2 is a gg-function to draw a radar plot for data analysis. It is stem from <a href=\"https://github.com/ricardo-bion/ggradar\" target=\"_blank\" rel=\"noopener\">ggradar</a> but has been extended with more cool features. </p>\n<h2 id=\"Install-ggradar2\"><a href=\"#Install-ggradar2\" class=\"headerlink\" title=\"Install ggradar2\"></a>Install ggradar2</h2><p>Run the code </p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">devtools::install_github(<span class=\"string\">\"xl0418/ggradar2\"</span>,dependencies=<span class=\"literal\">TRUE</span>)</span><br></pre></td></tr></table></figure>\n<h2 id=\"Use-ggradar2\"><a href=\"#Use-ggradar2\" class=\"headerlink\" title=\"Use ggradar2\"></a>Use ggradar2</h2><h3 id=\"Load-data\"><a href=\"#Load-data\" class=\"headerlink\" title=\"Load data.\"></a>Load data.</h3><figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">library</span>(ggradar2)</span><br><span class=\"line\">data(mtcars)</span><br><span class=\"line\">% Extract the group names. Otherwise, the first column will be chosen as the group names.</span><br><span class=\"line\">group = row.names(mtcars)</span><br><span class=\"line\">df = cbind(group,mtcars)</span><br><span class=\"line\">% The radar chart is not a nice presentation <span class=\"keyword\">if</span> you want to compare too many groups. Thus here </span><br><span class=\"line\">% we only focus on <span class=\"number\">4</span> groups.</span><br><span class=\"line\">dftest = head(df,<span class=\"number\">4</span>)</span><br><span class=\"line\">% To better distinguish two different styles, <span class=\"number\">6</span> groups are selected <span class=\"keyword\">for</span> illustration.</span><br><span class=\"line\">dftest = dftest[,<span class=\"number\">1</span>:<span class=\"number\">7</span>]</span><br></pre></td></tr></table></figure>\n<h3 id=\"Default-style\"><a href=\"#Default-style\" class=\"headerlink\" title=\"Default style\"></a>Default style</h3><p>By default</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ggradar2(dftest)</span><br></pre></td></tr></table></figure>\n<p>returns </p>\n<p><img src=\"/2020/10/05/2018-12-05-ggradar2/roundfill.png\" alt=\"default\"></p>\n<h3 id=\"No-fill-with-round-grid\"><a href=\"#No-fill-with-round-grid\" class=\"headerlink\" title=\"No fill with round grid\"></a>No fill with round grid</h3><p>If you don’t want to fill the polygon, run</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ggradar2(dftest,polygonfill = <span class=\"literal\">FALSE</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2020/10/05/2018-12-05-ggradar2/roundnofill.png\" alt=\"default\"></p>\n<h3 id=\"Web-type\"><a href=\"#Web-type\" class=\"headerlink\" title=\"Web type\"></a>Web type</h3><p>A new web type ‘lux’ has been added by <code>webtype</code>.</p>\n<p>mini type</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ggradar2(dftest,webtype = <span class=\"string\">'mini'</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2020/10/05/2018-12-05-ggradar2/mini.png\" alt=\"default\"></p>\n<p>luxurious type</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ggradar2(dftest,webtype = <span class=\"string\">'lux'</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2020/10/05/2018-12-05-ggradar2/lux.png\" alt=\"default\"></p>\n<h3 id=\"Gird-line-trend\"><a href=\"#Gird-line-trend\" class=\"headerlink\" title=\"Gird line trend\"></a>Gird line trend</h3><p>Use <code>grid.line.trend = &#39;increase&#39;</code> to plot an outward-increasing grid lines.</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ggradar2(dftest,style = <span class=\"string\">'sharp'</span>,webtype = <span class=\"string\">'lux'</span>,</span><br><span class=\"line\">              group.line.width = <span class=\"number\">0.5</span>,grid.line.trend = <span class=\"string\">'increase'</span>,gridline.min.linetype = <span class=\"string\">'solid'</span>,</span><br><span class=\"line\">              gridline.max.linetype = <span class=\"string\">'solid'</span>,gridline.min.colour = <span class=\"string\">'black'</span>,gridline.max.colour=<span class=\"string\">'black'</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2020/10/05/2018-12-05-ggradar2/trend.png\" alt=\"default\"></p>\n<h3 id=\"Full-score\"><a href=\"#Full-score\" class=\"headerlink\" title=\"Full score\"></a>Full score</h3><p>Use <code>fullscore = c(...)</code> to set the full score to each variable.</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fullscore &lt;- c(<span class=\"number\">100</span>,<span class=\"number\">10</span>,<span class=\"number\">300</span>,<span class=\"number\">150</span>,<span class=\"number\">10</span>,<span class=\"number\">10</span>)</span><br><span class=\"line\">a &lt;- ggradar2(dftest,fullscore = fullscore)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2020/10/05/2018-12-05-ggradar2/fullscore.png\" alt=\"default\"></p>\n<h3 id=\"Sharp-grid\"><a href=\"#Sharp-grid\" class=\"headerlink\" title=\"Sharp grid\"></a>Sharp grid</h3><p>A new style has been added. Call out the straight line style by running </p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ggradar2(dftest,style = <span class=\"string\">'sharp'</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2020/10/05/2018-12-05-ggradar2/straightfill.png\" alt=\"default\"></p>\n<h3 id=\"Sharp-grid-without-fill\"><a href=\"#Sharp-grid-without-fill\" class=\"headerlink\" title=\"Sharp grid without fill\"></a>Sharp grid without fill</h3><p>Get rid of the fill</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ggradar2(dftest,style = <span class=\"string\">'sharp'</span>,polygonfill = <span class=\"literal\">FALSE</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2020/10/05/2018-12-05-ggradar2/straightnofill.png\" alt=\"default\"></p>\n<h3 id=\"Removing-the-legend\"><a href=\"#Removing-the-legend\" class=\"headerlink\" title=\"Removing the legend\"></a>Removing the legend</h3><figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ggradar2(dftest,style = <span class=\"string\">'sharp'</span>,</span><br><span class=\"line\">polygonfill = <span class=\"literal\">FALSE</span>,plot.legend = <span class=\"literal\">FALSE</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2020/10/05/2018-12-05-ggradar2/nolegend.png\" alt=\"default\"></p>\n<h3 id=\"Multiple-plots-by-subgroups\"><a href=\"#Multiple-plots-by-subgroups\" class=\"headerlink\" title=\"Multiple plots by subgroups\"></a>Multiple plots by subgroups</h3><figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Extract 3 brands of cars out of the data frame</span></span><br><span class=\"line\">facettest &lt;- df[c(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">8</span>:<span class=\"number\">14</span>),]</span><br><span class=\"line\"><span class=\"comment\"># Set the subgroup names</span></span><br><span class=\"line\">facet1 &lt;- mapply(rep,c(<span class=\"string\">'Mazda'</span>,<span class=\"string\">'Hornet'</span>,<span class=\"string\">'Merc'</span>),c(<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">7</span>))</span><br><span class=\"line\">facet1 &lt;- Reduce(c,facet1)</span><br><span class=\"line\">facettest &lt;- cbind(facettest,facet1)</span><br><span class=\"line\">ggradar2(facettest,multiplots = <span class=\"literal\">TRUE</span>)</span><br></pre></td></tr></table></figure>\n<p>Notice that the column name for the subgroups should be ‘facet1’. Otherwise, ggradar2 could not recognize it.</p>\n<p><img src=\"/2020/10/05/2018-12-05-ggradar2/multipleplotsbig.png\" alt=\"default\"></p>"},{"title":"PyCUDA series 1: Build GPU programming environment ","_content":"\nAs my research always involves a huge amount of computation, parallel technique is a super fancy way to save my time. But the resource of cluster at my university is limited. I recently received a warning that CIT has detected multiple-account usage under my IP. To avoid the risk of being fired, I have to figure out another way to parallelize my job instead of parallelizing accounts. Right on time, GPU programming came out as an ideal option. I hope the GPU resource of my university is not full loaded yet.\n\nDue to some incompatibilities of CUDA and Visual Studio 2017, I spent half a day to figure out the solutions and finally succeeded building up GPU programming environment on Windows 10 with Pycuda installed in Python. Here, I post the procedure of the build and some solutions to the incompatibilities.\n\n<!--more-->\n \n# Prerequisite \n\nFor now, I am working on **Windows 10** with **Python 3.7.0**. The CUDA version that is compatibly built up for me is **CUDA 10.0**. I used **Pycharm 2018.3** as the Python IDE. Probably I will try to build on OS 10 soon in the future.\n\n# Simple procedure \n\n1.  Install [Python 3.7](https://www.python.org/downloads/) and [PyCharm 2018.3](https://www.jetbrains.com/pycharm/download/).\n\n2.  Install [Visual Studio 2017](https://visualstudio.microsoft.com/zh-hans/downloads/) as CUDA needs C++ compile. \n\n3.  Open the Visual Studio installer under the folder Visual Studio 2017. \n \n![default](2019-01-04-PyCUDAseries1/vsinstaller.png)\n\n4. Select: **Modify** under Visual Studio 2017 -> **Installation details**.\n\n5. Install options: select only the **Windows 10 SDK**.\n\n![default](2019-01-04-PyCUDAseries1/win10SDK.png)\n\n6.  Install [CUDA 10.0](https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exelocal) and follow the steps [here](https://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html#windows) to set up CUDA environment.\n\n\n# Tips\n\n- To make sure that CUDA is successfully installed, check `nvcc -v` at your terminal. \n- The step to build samples in Nvidia's guide is not necessary. I couldn't build the samples due to some incompatibilities of vs c++ but still have it work on Python.\n\n\n\n# Problem shooting\n\n- **CUDA install failed**\n\nI only came across this issue on my desktop at office that has an old monitor. Then, I resolved it by customizing the installation with uncheck of Visual Studio integration. No idea why but it works. Also check answer [here](https://www.reddit.com/r/nvidia/comments/9d2f23/cuda_install_problems_windows_10_geforce_1070ti/)\n\n\n- **nvcc fatal : Cannot find compiler 'cl.exe' in PATH **\n\nCheck [here](https://stackoverflow.com/questions/8125826/error-compiling-cuda-from-command-prompt). In principle, you should add `cl.exe` to the environment variables.\n","source":"_posts/2019-01-04-PyCUDAseries1.md","raw":"---\ntitle: \"PyCUDA series 1: Build GPU programming environment \"\ncategories: [Research,GPU programming,pyCUDA]\ntags: [Python, CUDA, GPU programming, Parallel computation]\n---\n\nAs my research always involves a huge amount of computation, parallel technique is a super fancy way to save my time. But the resource of cluster at my university is limited. I recently received a warning that CIT has detected multiple-account usage under my IP. To avoid the risk of being fired, I have to figure out another way to parallelize my job instead of parallelizing accounts. Right on time, GPU programming came out as an ideal option. I hope the GPU resource of my university is not full loaded yet.\n\nDue to some incompatibilities of CUDA and Visual Studio 2017, I spent half a day to figure out the solutions and finally succeeded building up GPU programming environment on Windows 10 with Pycuda installed in Python. Here, I post the procedure of the build and some solutions to the incompatibilities.\n\n<!--more-->\n \n# Prerequisite \n\nFor now, I am working on **Windows 10** with **Python 3.7.0**. The CUDA version that is compatibly built up for me is **CUDA 10.0**. I used **Pycharm 2018.3** as the Python IDE. Probably I will try to build on OS 10 soon in the future.\n\n# Simple procedure \n\n1.  Install [Python 3.7](https://www.python.org/downloads/) and [PyCharm 2018.3](https://www.jetbrains.com/pycharm/download/).\n\n2.  Install [Visual Studio 2017](https://visualstudio.microsoft.com/zh-hans/downloads/) as CUDA needs C++ compile. \n\n3.  Open the Visual Studio installer under the folder Visual Studio 2017. \n \n![default](2019-01-04-PyCUDAseries1/vsinstaller.png)\n\n4. Select: **Modify** under Visual Studio 2017 -> **Installation details**.\n\n5. Install options: select only the **Windows 10 SDK**.\n\n![default](2019-01-04-PyCUDAseries1/win10SDK.png)\n\n6.  Install [CUDA 10.0](https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exelocal) and follow the steps [here](https://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html#windows) to set up CUDA environment.\n\n\n# Tips\n\n- To make sure that CUDA is successfully installed, check `nvcc -v` at your terminal. \n- The step to build samples in Nvidia's guide is not necessary. I couldn't build the samples due to some incompatibilities of vs c++ but still have it work on Python.\n\n\n\n# Problem shooting\n\n- **CUDA install failed**\n\nI only came across this issue on my desktop at office that has an old monitor. Then, I resolved it by customizing the installation with uncheck of Visual Studio integration. No idea why but it works. Also check answer [here](https://www.reddit.com/r/nvidia/comments/9d2f23/cuda_install_problems_windows_10_geforce_1070ti/)\n\n\n- **nvcc fatal : Cannot find compiler 'cl.exe' in PATH **\n\nCheck [here](https://stackoverflow.com/questions/8125826/error-compiling-cuda-from-command-prompt). In principle, you should add `cl.exe` to the environment variables.\n","slug":"2019-01-04-PyCUDAseries1","published":1,"date":"2020-10-05T11:30:58.220Z","updated":"2020-10-05T11:30:58.221Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy12q000snj39ka9xwjdx","content":"<p>As my research always involves a huge amount of computation, parallel technique is a super fancy way to save my time. But the resource of cluster at my university is limited. I recently received a warning that CIT has detected multiple-account usage under my IP. To avoid the risk of being fired, I have to figure out another way to parallelize my job instead of parallelizing accounts. Right on time, GPU programming came out as an ideal option. I hope the GPU resource of my university is not full loaded yet.</p>\n<p>Due to some incompatibilities of CUDA and Visual Studio 2017, I spent half a day to figure out the solutions and finally succeeded building up GPU programming environment on Windows 10 with Pycuda installed in Python. Here, I post the procedure of the build and some solutions to the incompatibilities.</p>\n<a id=\"more\"></a>\n<h1 id=\"Prerequisite\"><a href=\"#Prerequisite\" class=\"headerlink\" title=\"Prerequisite\"></a>Prerequisite</h1><p>For now, I am working on <strong>Windows 10</strong> with <strong>Python 3.7.0</strong>. The CUDA version that is compatibly built up for me is <strong>CUDA 10.0</strong>. I used <strong>Pycharm 2018.3</strong> as the Python IDE. Probably I will try to build on OS 10 soon in the future.</p>\n<h1 id=\"Simple-procedure\"><a href=\"#Simple-procedure\" class=\"headerlink\" title=\"Simple procedure\"></a>Simple procedure</h1><ol>\n<li><p>Install <a href=\"https://www.python.org/downloads/\" target=\"_blank\" rel=\"noopener\">Python 3.7</a> and <a href=\"https://www.jetbrains.com/pycharm/download/\" target=\"_blank\" rel=\"noopener\">PyCharm 2018.3</a>.</p>\n</li>\n<li><p>Install <a href=\"https://visualstudio.microsoft.com/zh-hans/downloads/\" target=\"_blank\" rel=\"noopener\">Visual Studio 2017</a> as CUDA needs C++ compile. </p>\n</li>\n<li><p>Open the Visual Studio installer under the folder Visual Studio 2017. </p>\n</li>\n</ol>\n<p><img src=\"/2020/10/05/2019-01-04-PyCUDAseries1/vsinstaller.png\" alt=\"default\"></p>\n<ol start=\"4\">\n<li><p>Select: <strong>Modify</strong> under Visual Studio 2017 -&gt; <strong>Installation details</strong>.</p>\n</li>\n<li><p>Install options: select only the <strong>Windows 10 SDK</strong>.</p>\n</li>\n</ol>\n<p><img src=\"/2020/10/05/2019-01-04-PyCUDAseries1/win10SDK.png\" alt=\"default\"></p>\n<ol start=\"6\">\n<li>Install <a href=\"https://developer.nvidia.com/cuda-downloads?target_os=Windows&amp;target_arch=x86_64&amp;target_version=10&amp;target_type=exelocal\" target=\"_blank\" rel=\"noopener\">CUDA 10.0</a> and follow the steps <a href=\"https://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html#windows\" target=\"_blank\" rel=\"noopener\">here</a> to set up CUDA environment.</li>\n</ol>\n<h1 id=\"Tips\"><a href=\"#Tips\" class=\"headerlink\" title=\"Tips\"></a>Tips</h1><ul>\n<li>To make sure that CUDA is successfully installed, check <code>nvcc -v</code> at your terminal. </li>\n<li>The step to build samples in Nvidia’s guide is not necessary. I couldn’t build the samples due to some incompatibilities of vs c++ but still have it work on Python.</li>\n</ul>\n<h1 id=\"Problem-shooting\"><a href=\"#Problem-shooting\" class=\"headerlink\" title=\"Problem shooting\"></a>Problem shooting</h1><ul>\n<li><strong>CUDA install failed</strong></li>\n</ul>\n<p>I only came across this issue on my desktop at office that has an old monitor. Then, I resolved it by customizing the installation with uncheck of Visual Studio integration. No idea why but it works. Also check answer <a href=\"https://www.reddit.com/r/nvidia/comments/9d2f23/cuda_install_problems_windows_10_geforce_1070ti/\" target=\"_blank\" rel=\"noopener\">here</a></p>\n<ul>\n<li><strong>nvcc fatal : Cannot find compiler ‘cl.exe’ in PATH </strong></li>\n</ul>\n<p>Check <a href=\"https://stackoverflow.com/questions/8125826/error-compiling-cuda-from-command-prompt\" target=\"_blank\" rel=\"noopener\">here</a>. In principle, you should add <code>cl.exe</code> to the environment variables.</p>\n","site":{"data":{}},"excerpt":"<p>As my research always involves a huge amount of computation, parallel technique is a super fancy way to save my time. But the resource of cluster at my university is limited. I recently received a warning that CIT has detected multiple-account usage under my IP. To avoid the risk of being fired, I have to figure out another way to parallelize my job instead of parallelizing accounts. Right on time, GPU programming came out as an ideal option. I hope the GPU resource of my university is not full loaded yet.</p>\n<p>Due to some incompatibilities of CUDA and Visual Studio 2017, I spent half a day to figure out the solutions and finally succeeded building up GPU programming environment on Windows 10 with Pycuda installed in Python. Here, I post the procedure of the build and some solutions to the incompatibilities.</p>","more":"<h1 id=\"Prerequisite\"><a href=\"#Prerequisite\" class=\"headerlink\" title=\"Prerequisite\"></a>Prerequisite</h1><p>For now, I am working on <strong>Windows 10</strong> with <strong>Python 3.7.0</strong>. The CUDA version that is compatibly built up for me is <strong>CUDA 10.0</strong>. I used <strong>Pycharm 2018.3</strong> as the Python IDE. Probably I will try to build on OS 10 soon in the future.</p>\n<h1 id=\"Simple-procedure\"><a href=\"#Simple-procedure\" class=\"headerlink\" title=\"Simple procedure\"></a>Simple procedure</h1><ol>\n<li><p>Install <a href=\"https://www.python.org/downloads/\" target=\"_blank\" rel=\"noopener\">Python 3.7</a> and <a href=\"https://www.jetbrains.com/pycharm/download/\" target=\"_blank\" rel=\"noopener\">PyCharm 2018.3</a>.</p>\n</li>\n<li><p>Install <a href=\"https://visualstudio.microsoft.com/zh-hans/downloads/\" target=\"_blank\" rel=\"noopener\">Visual Studio 2017</a> as CUDA needs C++ compile. </p>\n</li>\n<li><p>Open the Visual Studio installer under the folder Visual Studio 2017. </p>\n</li>\n</ol>\n<p><img src=\"/2020/10/05/2019-01-04-PyCUDAseries1/vsinstaller.png\" alt=\"default\"></p>\n<ol start=\"4\">\n<li><p>Select: <strong>Modify</strong> under Visual Studio 2017 -&gt; <strong>Installation details</strong>.</p>\n</li>\n<li><p>Install options: select only the <strong>Windows 10 SDK</strong>.</p>\n</li>\n</ol>\n<p><img src=\"/2020/10/05/2019-01-04-PyCUDAseries1/win10SDK.png\" alt=\"default\"></p>\n<ol start=\"6\">\n<li>Install <a href=\"https://developer.nvidia.com/cuda-downloads?target_os=Windows&amp;target_arch=x86_64&amp;target_version=10&amp;target_type=exelocal\" target=\"_blank\" rel=\"noopener\">CUDA 10.0</a> and follow the steps <a href=\"https://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html#windows\" target=\"_blank\" rel=\"noopener\">here</a> to set up CUDA environment.</li>\n</ol>\n<h1 id=\"Tips\"><a href=\"#Tips\" class=\"headerlink\" title=\"Tips\"></a>Tips</h1><ul>\n<li>To make sure that CUDA is successfully installed, check <code>nvcc -v</code> at your terminal. </li>\n<li>The step to build samples in Nvidia’s guide is not necessary. I couldn’t build the samples due to some incompatibilities of vs c++ but still have it work on Python.</li>\n</ul>\n<h1 id=\"Problem-shooting\"><a href=\"#Problem-shooting\" class=\"headerlink\" title=\"Problem shooting\"></a>Problem shooting</h1><ul>\n<li><strong>CUDA install failed</strong></li>\n</ul>\n<p>I only came across this issue on my desktop at office that has an old monitor. Then, I resolved it by customizing the installation with uncheck of Visual Studio integration. No idea why but it works. Also check answer <a href=\"https://www.reddit.com/r/nvidia/comments/9d2f23/cuda_install_problems_windows_10_geforce_1070ti/\" target=\"_blank\" rel=\"noopener\">here</a></p>\n<ul>\n<li><strong>nvcc fatal : Cannot find compiler ‘cl.exe’ in PATH </strong></li>\n</ul>\n<p>Check <a href=\"https://stackoverflow.com/questions/8125826/error-compiling-cuda-from-command-prompt\" target=\"_blank\" rel=\"noopener\">here</a>. In principle, you should add <code>cl.exe</code> to the environment variables.</p>"},{"title":"Sed again: run on Ubuntu to avoid weird behavior","_content":"\nAs my 2nd project is close to an end, it is right the time to go back to the phylogenetic Janzen-Connell model. Simulations and plotting functions have been done during the gap time squeezed from the 2nd project. The last thing is to analyze the data, in which I bumped into a weird behavior of `sed`. \n\n<!--more-->\n\n## The data file\nIn this project, I have generated in total 108 data file in `.m` (Matlab) format. Each file contain several distance matrix, abundance vectors, an event table and turnover ticks that I want to extract and analyze exclusively. \n\n![Raw data files.](2019-01-10sed2/mfiles.png)\n\n## Extract the matrix\nThe procedure is simple. I modified the previous experimental bash scripts a bit to fit the file names of data. Then execute them sequentially on Windows prompt.\n\n1. Run `extractallDs.sh` and `extractallRs.sh` to extract all D matrix and R matrix from the raw files into `LRsij.Rdata,MRsij.Rdata,HRsij.Rdata,LDsij.Rdata,MDsij.Rdata,HDsij.Rdata`.\n2. Run `Dlast.sh` to extract the last D and R matrix into `LDij.Rdata,LRij.Rdata,MDij.Rdata,MRij.Rdata,HDij.Rdata,HRij.Rdata`.\n3. Do the same thing to extract event table and turnover ticks by running `loopextractEve_Turn.sh`\n\nlike \n\n![Raw data files.](2019-01-10sed2/rdatafiles.png)\n\nBut I obtained nothing in the files. \n\n## Bug shooting\nSame simulation code generates the same data structure. But why couldn't the same bash script extract information as before? At that moment, I kind of had a feeling that it must be that the script cannot recognize the newline sign. like the following, if the computer cannot match `\\r`, which is the code for the newline under windows and DOS, the script extracts nothing. \n \n```bash\n#!/bin/bash\nfor j in {0..4};\ndo\nfor i in {0..4};\ndo \nsed -n '/D'{'length(D)+1'}' = \\[\\r/,/\\];/p' test\"$j$i\".m > Ds\"$j$i\".Rdata\necho $j$i' done'\ndone\ndone\n```\n\nBut after I changed it to `\\n` that is the right code for a linux file, it didn't work neither. Even after changing the file code via `unix2dos`, none of the means worked.\n\n\\\\(\\boldsymbol{Finally}\\\\), after several hours of googling, thinking, patting the computer, I tried out the solution. The scripts need to be executed under the linux system. I launched them on Ubuntu again, the newline sign was recognized. \n\nBut it is still a mystery to me that why on a windows prompt none of `\\r` and `\\n` can be recognized. If you know it, pls reply this post. Thanks!","source":"_posts/2019-01-10sed2.md","raw":"---\ntitle: \"Sed again: run on Ubuntu to avoid weird behavior\"\ncategories: [Research, Bash, sed]\ntags: [project 3, bash, mega data, extract information]\n---\n\nAs my 2nd project is close to an end, it is right the time to go back to the phylogenetic Janzen-Connell model. Simulations and plotting functions have been done during the gap time squeezed from the 2nd project. The last thing is to analyze the data, in which I bumped into a weird behavior of `sed`. \n\n<!--more-->\n\n## The data file\nIn this project, I have generated in total 108 data file in `.m` (Matlab) format. Each file contain several distance matrix, abundance vectors, an event table and turnover ticks that I want to extract and analyze exclusively. \n\n![Raw data files.](2019-01-10sed2/mfiles.png)\n\n## Extract the matrix\nThe procedure is simple. I modified the previous experimental bash scripts a bit to fit the file names of data. Then execute them sequentially on Windows prompt.\n\n1. Run `extractallDs.sh` and `extractallRs.sh` to extract all D matrix and R matrix from the raw files into `LRsij.Rdata,MRsij.Rdata,HRsij.Rdata,LDsij.Rdata,MDsij.Rdata,HDsij.Rdata`.\n2. Run `Dlast.sh` to extract the last D and R matrix into `LDij.Rdata,LRij.Rdata,MDij.Rdata,MRij.Rdata,HDij.Rdata,HRij.Rdata`.\n3. Do the same thing to extract event table and turnover ticks by running `loopextractEve_Turn.sh`\n\nlike \n\n![Raw data files.](2019-01-10sed2/rdatafiles.png)\n\nBut I obtained nothing in the files. \n\n## Bug shooting\nSame simulation code generates the same data structure. But why couldn't the same bash script extract information as before? At that moment, I kind of had a feeling that it must be that the script cannot recognize the newline sign. like the following, if the computer cannot match `\\r`, which is the code for the newline under windows and DOS, the script extracts nothing. \n \n```bash\n#!/bin/bash\nfor j in {0..4};\ndo\nfor i in {0..4};\ndo \nsed -n '/D'{'length(D)+1'}' = \\[\\r/,/\\];/p' test\"$j$i\".m > Ds\"$j$i\".Rdata\necho $j$i' done'\ndone\ndone\n```\n\nBut after I changed it to `\\n` that is the right code for a linux file, it didn't work neither. Even after changing the file code via `unix2dos`, none of the means worked.\n\n\\\\(\\boldsymbol{Finally}\\\\), after several hours of googling, thinking, patting the computer, I tried out the solution. The scripts need to be executed under the linux system. I launched them on Ubuntu again, the newline sign was recognized. \n\nBut it is still a mystery to me that why on a windows prompt none of `\\r` and `\\n` can be recognized. If you know it, pls reply this post. Thanks!","slug":"2019-01-10sed2","published":1,"date":"2020-10-05T11:30:58.223Z","updated":"2020-10-05T11:30:58.224Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy12s000vnj39wvrb1si9","content":"<p>As my 2nd project is close to an end, it is right the time to go back to the phylogenetic Janzen-Connell model. Simulations and plotting functions have been done during the gap time squeezed from the 2nd project. The last thing is to analyze the data, in which I bumped into a weird behavior of <code>sed</code>. </p>\n<a id=\"more\"></a>\n<h2 id=\"The-data-file\"><a href=\"#The-data-file\" class=\"headerlink\" title=\"The data file\"></a>The data file</h2><p>In this project, I have generated in total 108 data file in <code>.m</code> (Matlab) format. Each file contain several distance matrix, abundance vectors, an event table and turnover ticks that I want to extract and analyze exclusively. </p>\n<p><img src=\"/2020/10/05/2019-01-10sed2/mfiles.png\" alt=\"Raw data files.\"></p>\n<h2 id=\"Extract-the-matrix\"><a href=\"#Extract-the-matrix\" class=\"headerlink\" title=\"Extract the matrix\"></a>Extract the matrix</h2><p>The procedure is simple. I modified the previous experimental bash scripts a bit to fit the file names of data. Then execute them sequentially on Windows prompt.</p>\n<ol>\n<li>Run <code>extractallDs.sh</code> and <code>extractallRs.sh</code> to extract all D matrix and R matrix from the raw files into <code>LRsij.Rdata,MRsij.Rdata,HRsij.Rdata,LDsij.Rdata,MDsij.Rdata,HDsij.Rdata</code>.</li>\n<li>Run <code>Dlast.sh</code> to extract the last D and R matrix into <code>LDij.Rdata,LRij.Rdata,MDij.Rdata,MRij.Rdata,HDij.Rdata,HRij.Rdata</code>.</li>\n<li>Do the same thing to extract event table and turnover ticks by running <code>loopextractEve_Turn.sh</code></li>\n</ol>\n<p>like </p>\n<p><img src=\"/2020/10/05/2019-01-10sed2/rdatafiles.png\" alt=\"Raw data files.\"></p>\n<p>But I obtained nothing in the files. </p>\n<h2 id=\"Bug-shooting\"><a href=\"#Bug-shooting\" class=\"headerlink\" title=\"Bug shooting\"></a>Bug shooting</h2><p>Same simulation code generates the same data structure. But why couldn’t the same bash script extract information as before? At that moment, I kind of had a feeling that it must be that the script cannot recognize the newline sign. like the following, if the computer cannot match <code>\\r</code>, which is the code for the newline under windows and DOS, the script extracts nothing. </p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#!/bin/bash</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> &#123;0..4&#125;;</span><br><span class=\"line\"><span class=\"keyword\">do</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> &#123;0..4&#125;;</span><br><span class=\"line\"><span class=\"keyword\">do</span> </span><br><span class=\"line\">sed -n <span class=\"string\">'/D'</span>&#123;<span class=\"string\">'length(D)+1'</span>&#125;<span class=\"string\">' = \\[\\r/,/\\];/p'</span> <span class=\"built_in\">test</span><span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.m &gt; Ds<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata</span><br><span class=\"line\"><span class=\"built_in\">echo</span> <span class=\"variable\">$j</span><span class=\"variable\">$i</span><span class=\"string\">' done'</span></span><br><span class=\"line\"><span class=\"keyword\">done</span></span><br><span class=\"line\"><span class=\"keyword\">done</span></span><br></pre></td></tr></table></figure>\n<p>But after I changed it to <code>\\n</code> that is the right code for a linux file, it didn’t work neither. Even after changing the file code via <code>unix2dos</code>, none of the means worked.</p>\n<p>\\(\\boldsymbol{Finally}\\), after several hours of googling, thinking, patting the computer, I tried out the solution. The scripts need to be executed under the linux system. I launched them on Ubuntu again, the newline sign was recognized. </p>\n<p>But it is still a mystery to me that why on a windows prompt none of <code>\\r</code> and <code>\\n</code> can be recognized. If you know it, pls reply this post. Thanks!</p>\n","site":{"data":{}},"excerpt":"<p>As my 2nd project is close to an end, it is right the time to go back to the phylogenetic Janzen-Connell model. Simulations and plotting functions have been done during the gap time squeezed from the 2nd project. The last thing is to analyze the data, in which I bumped into a weird behavior of <code>sed</code>. </p>","more":"<h2 id=\"The-data-file\"><a href=\"#The-data-file\" class=\"headerlink\" title=\"The data file\"></a>The data file</h2><p>In this project, I have generated in total 108 data file in <code>.m</code> (Matlab) format. Each file contain several distance matrix, abundance vectors, an event table and turnover ticks that I want to extract and analyze exclusively. </p>\n<p><img src=\"/2020/10/05/2019-01-10sed2/mfiles.png\" alt=\"Raw data files.\"></p>\n<h2 id=\"Extract-the-matrix\"><a href=\"#Extract-the-matrix\" class=\"headerlink\" title=\"Extract the matrix\"></a>Extract the matrix</h2><p>The procedure is simple. I modified the previous experimental bash scripts a bit to fit the file names of data. Then execute them sequentially on Windows prompt.</p>\n<ol>\n<li>Run <code>extractallDs.sh</code> and <code>extractallRs.sh</code> to extract all D matrix and R matrix from the raw files into <code>LRsij.Rdata,MRsij.Rdata,HRsij.Rdata,LDsij.Rdata,MDsij.Rdata,HDsij.Rdata</code>.</li>\n<li>Run <code>Dlast.sh</code> to extract the last D and R matrix into <code>LDij.Rdata,LRij.Rdata,MDij.Rdata,MRij.Rdata,HDij.Rdata,HRij.Rdata</code>.</li>\n<li>Do the same thing to extract event table and turnover ticks by running <code>loopextractEve_Turn.sh</code></li>\n</ol>\n<p>like </p>\n<p><img src=\"/2020/10/05/2019-01-10sed2/rdatafiles.png\" alt=\"Raw data files.\"></p>\n<p>But I obtained nothing in the files. </p>\n<h2 id=\"Bug-shooting\"><a href=\"#Bug-shooting\" class=\"headerlink\" title=\"Bug shooting\"></a>Bug shooting</h2><p>Same simulation code generates the same data structure. But why couldn’t the same bash script extract information as before? At that moment, I kind of had a feeling that it must be that the script cannot recognize the newline sign. like the following, if the computer cannot match <code>\\r</code>, which is the code for the newline under windows and DOS, the script extracts nothing. </p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#!/bin/bash</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> &#123;0..4&#125;;</span><br><span class=\"line\"><span class=\"keyword\">do</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> &#123;0..4&#125;;</span><br><span class=\"line\"><span class=\"keyword\">do</span> </span><br><span class=\"line\">sed -n <span class=\"string\">'/D'</span>&#123;<span class=\"string\">'length(D)+1'</span>&#125;<span class=\"string\">' = \\[\\r/,/\\];/p'</span> <span class=\"built_in\">test</span><span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.m &gt; Ds<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata</span><br><span class=\"line\"><span class=\"built_in\">echo</span> <span class=\"variable\">$j</span><span class=\"variable\">$i</span><span class=\"string\">' done'</span></span><br><span class=\"line\"><span class=\"keyword\">done</span></span><br><span class=\"line\"><span class=\"keyword\">done</span></span><br></pre></td></tr></table></figure>\n<p>But after I changed it to <code>\\n</code> that is the right code for a linux file, it didn’t work neither. Even after changing the file code via <code>unix2dos</code>, none of the means worked.</p>\n<p>\\(\\boldsymbol{Finally}\\), after several hours of googling, thinking, patting the computer, I tried out the solution. The scripts need to be executed under the linux system. I launched them on Ubuntu again, the newline sign was recognized. </p>\n<p>But it is still a mystery to me that why on a windows prompt none of <code>\\r</code> and <code>\\n</code> can be recognized. If you know it, pls reply this post. Thanks!</p>"},{"title":"PyCUDA series 2: a simple matrix algebra and speed test","_content":"\nIn this post, you will find a simple matrix algebra done by gpu parallelization and a straightforward result of the speed contest between cpu and gpu.\n\n<!--more-->\n \n# A simple matrix algebra to be parallelized\n\nAs a first try, I intended to parallleize a simple matrix algebra in my Project 2 that can be computed as follows on Python:\n\n```Python\nc = a_cpu[:,np.newaxis]-a_cpu\n```\n\nThis is a formula to compute the discrepancy of any pair of elements given by a vector. For example, input \n\n```Python\na = np.array([1,2,3])\n```\n\nthe formula should return \n\n```Python\nc = array([[0,-1,-2,],\n\t[1,0,-1],\n\t[2,1,0]])\n```\n\nA standard python code like the formula above by using `numpy` package, every element in `c_cpu` is computed in a serial order. \n\nIn terms of the power of current cpu, for a lower dimensional matrix, you can hardly feel the elapsed time to compute out the equation. However, if you have a huge matrix of 10000X10000 size and want to loop it 100 times, it costs half a minute on an i7 4790k cpu, although you may ignore the little time consumed. But it is not rare to have a large matrix bigger than 1 million by 1 million and loop it billion times at least in an evolutionary problem. In this case, you are saving your life if the compulation time can be reduced.\n\n# A shortcut to understand GPU parallel computation\n\nNow we have a good way to save our life instead of having a weird pill. CUDA is a parallel computing platform and application programming interface (API) model created by [Nvidia](https://developer.nvidia.com/nvidia-developer-zone). [PyCUDA](https://documen.tician.de/pycuda/index.html) provides a python package to allow people to parallelize their computation on a Graphic Processing Unit (GPU) by Python. For sure, I didn't say everything is parallelizable. Only the procedures that are consisting of independent calculation paths can be parallelized, which means you work on different paths at the same time instead of doing them one by one. Hence, the amount of time that can be saved relies on the percentage of work that is parallelizable in your work. In this sense, matrix algebra is a good representative. Other applications of parallelism involves rendering graphics to a screen, running a Monte Carlo Simulation, multiplying matrices for a machine learning algorithm, or powering a database. Google them yourself. \n\n\nWhy does GPU possess such powerful ability? It owes to its design purpose. A graphic card is designed to process graphics and render them to a screen. What is a graphic? It is just like a matrix. So to process a matrix, the ability to compute multiple entries of the matrix is more important than the ability to deal with just one entry smartly. Hence, a gpu is designed to own a huge amount of threads that can process multiple calculations at one time although they are slow while a cpu only contains few (like an i7 4790k has 4 physical cores which means 4  physical threads, at most 8 threads if including hyperthreads tech) but are faster. A good analogy is that a gpu is like a cluster of students at primary school while a cpu is like a professor. When calculating a complex problem, the professor is definitely faster and smarter. But when computing lots of simple algebra, the cluster of primary students is faster. In summary, CPUs are designed for running a small number of potentially quite complex tasks. GPUs are designed for running a large number of quite\nsimple tasks.\n\n# Parallelize the matrix algebra\n\nAfter having a rough idea of parallelism, let's do it on practice. For a matrix algebra like the above example, what are the independent calculation paths? Apparently, each entry of the output matrix `c` is computed out independently given the input `a`. Thus, we could assign each calculation path to a thread on GPU. \n\n```Python\n\nkernel_code_template = \"\"\"\n__global__ void com_t(int matrixsize,float *a, float *c)\n{\n\n    // 2D Thread ID \n    int tx = blockDim.x*blockIdx.x + threadIdx.x; // Compute column index\n    int ty = blockDim.y*blockIdx.y + threadIdx.y; // Compute row index\n\n    // Pvalue is used to store the element of the matrix\n    // that is computed by the thread\n    float Pvalue = 0;\n\n    // Each thread loads one row of M and one column of N, \n    //   to produce one element of P.\n    if((ty <matrixsize) && (tx < matrixsize))\n    {\n    float Aelement = a[ty];\n    float Belement = a[tx];\n    Pvalue = Aelement - Belement;\n    // Write the matrix to device memory;\n    // each thread writes one element\n    c[ty * matrixsize + tx] = Pvalue;\n    }\n}\n\"\"\"\n```\n\n\nThe most important part of parallelism on GPU is the kernel function which is coded for a single calculation path. The only difference among different calculation paths is the locality information of the entries of matrix `c` in the memory. Normally, we can envisage a memory on GPU as a cluster of grids of blocks of threads. But in fact, it is not this physical structure on the borad of the card. It involves streaming processors (SMs) dealing with wraps. However, here we better use the metaphor.\n\n![default](2019-01-14-PyCUDAseries2/gridblockthread.png)\n\nEach entry in matrix `c` corresponds to a specific combination of `gridIdx.x,gridIdx.y,blockIdx.x,blockIdx.y,threadIdx.x,threadIdx.y`. The value of the indexes also depend on how we allocate the size of grid, block. A regular way for a matrix algebra is to fit the struture to the matrix. For example, if we want to compute a 5X5 matrix, we can allocate the size of block as 5X5 which means we will use a matrix of 5X5 threads for compuatation. But a block contains at most 1024 threads for the current card (GTX 970). Thus for a square matrix we can allocate at most a size of 32X32 threads for one block. If we want to work on a larger matrix, we need more blocks or even more grids. This then causes one issue that some threads may not be used if the matrix size is not a multiplier of the block size. Therefore, we need to constrain our computation within the matrix by using `if((ty <matrixsize) && (tx < matrixsize))`. And make sure the evaluation of matrix `c` is put in the loop. Otherwise, the over requested threads (if the requested threads don't fit the matrix size) will be invoked and replace the inner results in `c`.\n\nOnce we have correctly allocated the memory and located the entry by these indexes, we can formulate your calculation. After that, the code will be uploaded to GPU and tranferred to the code of GPU (compile). Once all entries have been calculated, the results will be stored in the memory on GPU. Then cpu will pull back the result.  \n\n```Python\n\n# compile the kernel code\nmod = compiler.SourceModule(kernel_code_template)\n\n# get the kernel function from the compiled module\nmatrixmul = mod.get_function(\"com_t\")\n\nmatrixsize = 3\nBLOCK_SIZE = 2\n\n# call the kernel on the card\nmatrixmul(np.uint32(matrixsize),\n    # inputs\n    a_gpu,\n    # output\n    c_gpu,\n    # 4 blocks of BLOCK_SIZE x BLOCK_SIZE threads\n    grid = (2,2,1),\n    block = (BLOCK_SIZE, BLOCK_SIZE, 1),\n    )\n```\n\n# Speed comparison between CPU and GPU\n\nAt last, to show the power of GPU even on such simple matrix algebra, I run calculation for different dimensions and loop each calculation 100 times to enlarge the time consumption.\n\n![default](2019-01-14-PyCUDAseries2/speedtest.png)\n\nAs the figure shows, the time consumption for 100 times calculations starts to significantly split up when the dimension of matrix is over 2000. The time consumption on CPU grows almost exponentially along the dimension while on gpu it only grows a little. With the increase of dimension, you save a huge amount of your time and your life.\n\n# End\nThis is a quite simplified introduction to a parallelism example. There is a bank of posts explaining parallelism better and more exhaustive than this one. However, my goal here is to use a simple example and a short length of words to have you get an idea of how it works instead of scaring you away from this field. Once you fall into the trap of \"it looks as simple as the author said\", I believe you will learn more by yourself :-)\n\n# Reference\n[CUDA Programming] by Shane Cook is a good start book for learning both the hardware and the language. \n\nThis [PyCUDA website](https://andreask.cs.illinois.edu/PyCuda) provides some examples on Python.\n\n# Full code\nIt contains the example code and the speed test. Clone it [here](https://github.com/xl0418/GPU_Python/blob/master/gpu_cpu_speedtest.py)\n","source":"_posts/2019-01-14-PyCUDAseries2.md","raw":"---\ntitle: \"PyCUDA series 2: a simple matrix algebra and speed test\"\ncategories: [Research,GPU programming,pyCUDA]\ntags: [Python, CUDA, GPU programming, Parallel computation]\n---\n\nIn this post, you will find a simple matrix algebra done by gpu parallelization and a straightforward result of the speed contest between cpu and gpu.\n\n<!--more-->\n \n# A simple matrix algebra to be parallelized\n\nAs a first try, I intended to parallleize a simple matrix algebra in my Project 2 that can be computed as follows on Python:\n\n```Python\nc = a_cpu[:,np.newaxis]-a_cpu\n```\n\nThis is a formula to compute the discrepancy of any pair of elements given by a vector. For example, input \n\n```Python\na = np.array([1,2,3])\n```\n\nthe formula should return \n\n```Python\nc = array([[0,-1,-2,],\n\t[1,0,-1],\n\t[2,1,0]])\n```\n\nA standard python code like the formula above by using `numpy` package, every element in `c_cpu` is computed in a serial order. \n\nIn terms of the power of current cpu, for a lower dimensional matrix, you can hardly feel the elapsed time to compute out the equation. However, if you have a huge matrix of 10000X10000 size and want to loop it 100 times, it costs half a minute on an i7 4790k cpu, although you may ignore the little time consumed. But it is not rare to have a large matrix bigger than 1 million by 1 million and loop it billion times at least in an evolutionary problem. In this case, you are saving your life if the compulation time can be reduced.\n\n# A shortcut to understand GPU parallel computation\n\nNow we have a good way to save our life instead of having a weird pill. CUDA is a parallel computing platform and application programming interface (API) model created by [Nvidia](https://developer.nvidia.com/nvidia-developer-zone). [PyCUDA](https://documen.tician.de/pycuda/index.html) provides a python package to allow people to parallelize their computation on a Graphic Processing Unit (GPU) by Python. For sure, I didn't say everything is parallelizable. Only the procedures that are consisting of independent calculation paths can be parallelized, which means you work on different paths at the same time instead of doing them one by one. Hence, the amount of time that can be saved relies on the percentage of work that is parallelizable in your work. In this sense, matrix algebra is a good representative. Other applications of parallelism involves rendering graphics to a screen, running a Monte Carlo Simulation, multiplying matrices for a machine learning algorithm, or powering a database. Google them yourself. \n\n\nWhy does GPU possess such powerful ability? It owes to its design purpose. A graphic card is designed to process graphics and render them to a screen. What is a graphic? It is just like a matrix. So to process a matrix, the ability to compute multiple entries of the matrix is more important than the ability to deal with just one entry smartly. Hence, a gpu is designed to own a huge amount of threads that can process multiple calculations at one time although they are slow while a cpu only contains few (like an i7 4790k has 4 physical cores which means 4  physical threads, at most 8 threads if including hyperthreads tech) but are faster. A good analogy is that a gpu is like a cluster of students at primary school while a cpu is like a professor. When calculating a complex problem, the professor is definitely faster and smarter. But when computing lots of simple algebra, the cluster of primary students is faster. In summary, CPUs are designed for running a small number of potentially quite complex tasks. GPUs are designed for running a large number of quite\nsimple tasks.\n\n# Parallelize the matrix algebra\n\nAfter having a rough idea of parallelism, let's do it on practice. For a matrix algebra like the above example, what are the independent calculation paths? Apparently, each entry of the output matrix `c` is computed out independently given the input `a`. Thus, we could assign each calculation path to a thread on GPU. \n\n```Python\n\nkernel_code_template = \"\"\"\n__global__ void com_t(int matrixsize,float *a, float *c)\n{\n\n    // 2D Thread ID \n    int tx = blockDim.x*blockIdx.x + threadIdx.x; // Compute column index\n    int ty = blockDim.y*blockIdx.y + threadIdx.y; // Compute row index\n\n    // Pvalue is used to store the element of the matrix\n    // that is computed by the thread\n    float Pvalue = 0;\n\n    // Each thread loads one row of M and one column of N, \n    //   to produce one element of P.\n    if((ty <matrixsize) && (tx < matrixsize))\n    {\n    float Aelement = a[ty];\n    float Belement = a[tx];\n    Pvalue = Aelement - Belement;\n    // Write the matrix to device memory;\n    // each thread writes one element\n    c[ty * matrixsize + tx] = Pvalue;\n    }\n}\n\"\"\"\n```\n\n\nThe most important part of parallelism on GPU is the kernel function which is coded for a single calculation path. The only difference among different calculation paths is the locality information of the entries of matrix `c` in the memory. Normally, we can envisage a memory on GPU as a cluster of grids of blocks of threads. But in fact, it is not this physical structure on the borad of the card. It involves streaming processors (SMs) dealing with wraps. However, here we better use the metaphor.\n\n![default](2019-01-14-PyCUDAseries2/gridblockthread.png)\n\nEach entry in matrix `c` corresponds to a specific combination of `gridIdx.x,gridIdx.y,blockIdx.x,blockIdx.y,threadIdx.x,threadIdx.y`. The value of the indexes also depend on how we allocate the size of grid, block. A regular way for a matrix algebra is to fit the struture to the matrix. For example, if we want to compute a 5X5 matrix, we can allocate the size of block as 5X5 which means we will use a matrix of 5X5 threads for compuatation. But a block contains at most 1024 threads for the current card (GTX 970). Thus for a square matrix we can allocate at most a size of 32X32 threads for one block. If we want to work on a larger matrix, we need more blocks or even more grids. This then causes one issue that some threads may not be used if the matrix size is not a multiplier of the block size. Therefore, we need to constrain our computation within the matrix by using `if((ty <matrixsize) && (tx < matrixsize))`. And make sure the evaluation of matrix `c` is put in the loop. Otherwise, the over requested threads (if the requested threads don't fit the matrix size) will be invoked and replace the inner results in `c`.\n\nOnce we have correctly allocated the memory and located the entry by these indexes, we can formulate your calculation. After that, the code will be uploaded to GPU and tranferred to the code of GPU (compile). Once all entries have been calculated, the results will be stored in the memory on GPU. Then cpu will pull back the result.  \n\n```Python\n\n# compile the kernel code\nmod = compiler.SourceModule(kernel_code_template)\n\n# get the kernel function from the compiled module\nmatrixmul = mod.get_function(\"com_t\")\n\nmatrixsize = 3\nBLOCK_SIZE = 2\n\n# call the kernel on the card\nmatrixmul(np.uint32(matrixsize),\n    # inputs\n    a_gpu,\n    # output\n    c_gpu,\n    # 4 blocks of BLOCK_SIZE x BLOCK_SIZE threads\n    grid = (2,2,1),\n    block = (BLOCK_SIZE, BLOCK_SIZE, 1),\n    )\n```\n\n# Speed comparison between CPU and GPU\n\nAt last, to show the power of GPU even on such simple matrix algebra, I run calculation for different dimensions and loop each calculation 100 times to enlarge the time consumption.\n\n![default](2019-01-14-PyCUDAseries2/speedtest.png)\n\nAs the figure shows, the time consumption for 100 times calculations starts to significantly split up when the dimension of matrix is over 2000. The time consumption on CPU grows almost exponentially along the dimension while on gpu it only grows a little. With the increase of dimension, you save a huge amount of your time and your life.\n\n# End\nThis is a quite simplified introduction to a parallelism example. There is a bank of posts explaining parallelism better and more exhaustive than this one. However, my goal here is to use a simple example and a short length of words to have you get an idea of how it works instead of scaring you away from this field. Once you fall into the trap of \"it looks as simple as the author said\", I believe you will learn more by yourself :-)\n\n# Reference\n[CUDA Programming] by Shane Cook is a good start book for learning both the hardware and the language. \n\nThis [PyCUDA website](https://andreask.cs.illinois.edu/PyCuda) provides some examples on Python.\n\n# Full code\nIt contains the example code and the speed test. Clone it [here](https://github.com/xl0418/GPU_Python/blob/master/gpu_cpu_speedtest.py)\n","slug":"2019-01-14-PyCUDAseries2","published":1,"date":"2020-10-05T11:30:58.225Z","updated":"2020-10-05T11:30:58.225Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy12u000xnj390mtnx2b7","content":"<p>In this post, you will find a simple matrix algebra done by gpu parallelization and a straightforward result of the speed contest between cpu and gpu.</p>\n<a id=\"more\"></a>\n<h1 id=\"A-simple-matrix-algebra-to-be-parallelized\"><a href=\"#A-simple-matrix-algebra-to-be-parallelized\" class=\"headerlink\" title=\"A simple matrix algebra to be parallelized\"></a>A simple matrix algebra to be parallelized</h1><p>As a first try, I intended to parallleize a simple matrix algebra in my Project 2 that can be computed as follows on Python:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">c = a_cpu[:,np.newaxis]-a_cpu</span><br></pre></td></tr></table></figure>\n<p>This is a formula to compute the discrepancy of any pair of elements given by a vector. For example, input </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>])</span><br></pre></td></tr></table></figure>\n<p>the formula should return </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">c = array([[<span class=\"number\">0</span>,<span class=\"number\">-1</span>,<span class=\"number\">-2</span>,],</span><br><span class=\"line\">\t[<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">-1</span>],</span><br><span class=\"line\">\t[<span class=\"number\">2</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>]])</span><br></pre></td></tr></table></figure>\n<p>A standard python code like the formula above by using <code>numpy</code> package, every element in <code>c_cpu</code> is computed in a serial order. </p>\n<p>In terms of the power of current cpu, for a lower dimensional matrix, you can hardly feel the elapsed time to compute out the equation. However, if you have a huge matrix of 10000X10000 size and want to loop it 100 times, it costs half a minute on an i7 4790k cpu, although you may ignore the little time consumed. But it is not rare to have a large matrix bigger than 1 million by 1 million and loop it billion times at least in an evolutionary problem. In this case, you are saving your life if the compulation time can be reduced.</p>\n<h1 id=\"A-shortcut-to-understand-GPU-parallel-computation\"><a href=\"#A-shortcut-to-understand-GPU-parallel-computation\" class=\"headerlink\" title=\"A shortcut to understand GPU parallel computation\"></a>A shortcut to understand GPU parallel computation</h1><p>Now we have a good way to save our life instead of having a weird pill. CUDA is a parallel computing platform and application programming interface (API) model created by <a href=\"https://developer.nvidia.com/nvidia-developer-zone\" target=\"_blank\" rel=\"noopener\">Nvidia</a>. <a href=\"https://documen.tician.de/pycuda/index.html\" target=\"_blank\" rel=\"noopener\">PyCUDA</a> provides a python package to allow people to parallelize their computation on a Graphic Processing Unit (GPU) by Python. For sure, I didn’t say everything is parallelizable. Only the procedures that are consisting of independent calculation paths can be parallelized, which means you work on different paths at the same time instead of doing them one by one. Hence, the amount of time that can be saved relies on the percentage of work that is parallelizable in your work. In this sense, matrix algebra is a good representative. Other applications of parallelism involves rendering graphics to a screen, running a Monte Carlo Simulation, multiplying matrices for a machine learning algorithm, or powering a database. Google them yourself. </p>\n<p>Why does GPU possess such powerful ability? It owes to its design purpose. A graphic card is designed to process graphics and render them to a screen. What is a graphic? It is just like a matrix. So to process a matrix, the ability to compute multiple entries of the matrix is more important than the ability to deal with just one entry smartly. Hence, a gpu is designed to own a huge amount of threads that can process multiple calculations at one time although they are slow while a cpu only contains few (like an i7 4790k has 4 physical cores which means 4  physical threads, at most 8 threads if including hyperthreads tech) but are faster. A good analogy is that a gpu is like a cluster of students at primary school while a cpu is like a professor. When calculating a complex problem, the professor is definitely faster and smarter. But when computing lots of simple algebra, the cluster of primary students is faster. In summary, CPUs are designed for running a small number of potentially quite complex tasks. GPUs are designed for running a large number of quite<br>simple tasks.</p>\n<h1 id=\"Parallelize-the-matrix-algebra\"><a href=\"#Parallelize-the-matrix-algebra\" class=\"headerlink\" title=\"Parallelize the matrix algebra\"></a>Parallelize the matrix algebra</h1><p>After having a rough idea of parallelism, let’s do it on practice. For a matrix algebra like the above example, what are the independent calculation paths? Apparently, each entry of the output matrix <code>c</code> is computed out independently given the input <code>a</code>. Thus, we could assign each calculation path to a thread on GPU. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">kernel_code_template = <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">__global__ void com_t(int matrixsize,float *a, float *c)</span></span><br><span class=\"line\"><span class=\"string\">&#123;</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    // 2D Thread ID </span></span><br><span class=\"line\"><span class=\"string\">    int tx = blockDim.x*blockIdx.x + threadIdx.x; // Compute column index</span></span><br><span class=\"line\"><span class=\"string\">    int ty = blockDim.y*blockIdx.y + threadIdx.y; // Compute row index</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    // Pvalue is used to store the element of the matrix</span></span><br><span class=\"line\"><span class=\"string\">    // that is computed by the thread</span></span><br><span class=\"line\"><span class=\"string\">    float Pvalue = 0;</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    // Each thread loads one row of M and one column of N, </span></span><br><span class=\"line\"><span class=\"string\">    //   to produce one element of P.</span></span><br><span class=\"line\"><span class=\"string\">    if((ty &lt;matrixsize) &amp;&amp; (tx &lt; matrixsize))</span></span><br><span class=\"line\"><span class=\"string\">    &#123;</span></span><br><span class=\"line\"><span class=\"string\">    float Aelement = a[ty];</span></span><br><span class=\"line\"><span class=\"string\">    float Belement = a[tx];</span></span><br><span class=\"line\"><span class=\"string\">    Pvalue = Aelement - Belement;</span></span><br><span class=\"line\"><span class=\"string\">    // Write the matrix to device memory;</span></span><br><span class=\"line\"><span class=\"string\">    // each thread writes one element</span></span><br><span class=\"line\"><span class=\"string\">    c[ty * matrixsize + tx] = Pvalue;</span></span><br><span class=\"line\"><span class=\"string\">    &#125;</span></span><br><span class=\"line\"><span class=\"string\">&#125;</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br></pre></td></tr></table></figure>\n<p>The most important part of parallelism on GPU is the kernel function which is coded for a single calculation path. The only difference among different calculation paths is the locality information of the entries of matrix <code>c</code> in the memory. Normally, we can envisage a memory on GPU as a cluster of grids of blocks of threads. But in fact, it is not this physical structure on the borad of the card. It involves streaming processors (SMs) dealing with wraps. However, here we better use the metaphor.</p>\n<p><img src=\"/2020/10/05/2019-01-14-PyCUDAseries2/gridblockthread.png\" alt=\"default\"></p>\n<p>Each entry in matrix <code>c</code> corresponds to a specific combination of <code>gridIdx.x,gridIdx.y,blockIdx.x,blockIdx.y,threadIdx.x,threadIdx.y</code>. The value of the indexes also depend on how we allocate the size of grid, block. A regular way for a matrix algebra is to fit the struture to the matrix. For example, if we want to compute a 5X5 matrix, we can allocate the size of block as 5X5 which means we will use a matrix of 5X5 threads for compuatation. But a block contains at most 1024 threads for the current card (GTX 970). Thus for a square matrix we can allocate at most a size of 32X32 threads for one block. If we want to work on a larger matrix, we need more blocks or even more grids. This then causes one issue that some threads may not be used if the matrix size is not a multiplier of the block size. Therefore, we need to constrain our computation within the matrix by using <code>if((ty &lt;matrixsize) &amp;&amp; (tx &lt; matrixsize))</code>. And make sure the evaluation of matrix <code>c</code> is put in the loop. Otherwise, the over requested threads (if the requested threads don’t fit the matrix size) will be invoked and replace the inner results in <code>c</code>.</p>\n<p>Once we have correctly allocated the memory and located the entry by these indexes, we can formulate your calculation. After that, the code will be uploaded to GPU and tranferred to the code of GPU (compile). Once all entries have been calculated, the results will be stored in the memory on GPU. Then cpu will pull back the result.  </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># compile the kernel code</span></span><br><span class=\"line\">mod = compiler.SourceModule(kernel_code_template)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># get the kernel function from the compiled module</span></span><br><span class=\"line\">matrixmul = mod.get_function(<span class=\"string\">\"com_t\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">matrixsize = <span class=\"number\">3</span></span><br><span class=\"line\">BLOCK_SIZE = <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># call the kernel on the card</span></span><br><span class=\"line\">matrixmul(np.uint32(matrixsize),</span><br><span class=\"line\">    <span class=\"comment\"># inputs</span></span><br><span class=\"line\">    a_gpu,</span><br><span class=\"line\">    <span class=\"comment\"># output</span></span><br><span class=\"line\">    c_gpu,</span><br><span class=\"line\">    <span class=\"comment\"># 4 blocks of BLOCK_SIZE x BLOCK_SIZE threads</span></span><br><span class=\"line\">    grid = (<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>),</span><br><span class=\"line\">    block = (BLOCK_SIZE, BLOCK_SIZE, <span class=\"number\">1</span>),</span><br><span class=\"line\">    )</span><br></pre></td></tr></table></figure>\n<h1 id=\"Speed-comparison-between-CPU-and-GPU\"><a href=\"#Speed-comparison-between-CPU-and-GPU\" class=\"headerlink\" title=\"Speed comparison between CPU and GPU\"></a>Speed comparison between CPU and GPU</h1><p>At last, to show the power of GPU even on such simple matrix algebra, I run calculation for different dimensions and loop each calculation 100 times to enlarge the time consumption.</p>\n<p><img src=\"/2020/10/05/2019-01-14-PyCUDAseries2/speedtest.png\" alt=\"default\"></p>\n<p>As the figure shows, the time consumption for 100 times calculations starts to significantly split up when the dimension of matrix is over 2000. The time consumption on CPU grows almost exponentially along the dimension while on gpu it only grows a little. With the increase of dimension, you save a huge amount of your time and your life.</p>\n<h1 id=\"End\"><a href=\"#End\" class=\"headerlink\" title=\"End\"></a>End</h1><p>This is a quite simplified introduction to a parallelism example. There is a bank of posts explaining parallelism better and more exhaustive than this one. However, my goal here is to use a simple example and a short length of words to have you get an idea of how it works instead of scaring you away from this field. Once you fall into the trap of “it looks as simple as the author said”, I believe you will learn more by yourself :-)</p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[CUDA Programming] by Shane Cook is a good start book for learning both the hardware and the language. </p>\n<p>This <a href=\"https://andreask.cs.illinois.edu/PyCuda\" target=\"_blank\" rel=\"noopener\">PyCUDA website</a> provides some examples on Python.</p>\n<h1 id=\"Full-code\"><a href=\"#Full-code\" class=\"headerlink\" title=\"Full code\"></a>Full code</h1><p>It contains the example code and the speed test. Clone it <a href=\"https://github.com/xl0418/GPU_Python/blob/master/gpu_cpu_speedtest.py\" target=\"_blank\" rel=\"noopener\">here</a></p>\n","site":{"data":{}},"excerpt":"<p>In this post, you will find a simple matrix algebra done by gpu parallelization and a straightforward result of the speed contest between cpu and gpu.</p>","more":"<h1 id=\"A-simple-matrix-algebra-to-be-parallelized\"><a href=\"#A-simple-matrix-algebra-to-be-parallelized\" class=\"headerlink\" title=\"A simple matrix algebra to be parallelized\"></a>A simple matrix algebra to be parallelized</h1><p>As a first try, I intended to parallleize a simple matrix algebra in my Project 2 that can be computed as follows on Python:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">c = a_cpu[:,np.newaxis]-a_cpu</span><br></pre></td></tr></table></figure>\n<p>This is a formula to compute the discrepancy of any pair of elements given by a vector. For example, input </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>])</span><br></pre></td></tr></table></figure>\n<p>the formula should return </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">c = array([[<span class=\"number\">0</span>,<span class=\"number\">-1</span>,<span class=\"number\">-2</span>,],</span><br><span class=\"line\">\t[<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">-1</span>],</span><br><span class=\"line\">\t[<span class=\"number\">2</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>]])</span><br></pre></td></tr></table></figure>\n<p>A standard python code like the formula above by using <code>numpy</code> package, every element in <code>c_cpu</code> is computed in a serial order. </p>\n<p>In terms of the power of current cpu, for a lower dimensional matrix, you can hardly feel the elapsed time to compute out the equation. However, if you have a huge matrix of 10000X10000 size and want to loop it 100 times, it costs half a minute on an i7 4790k cpu, although you may ignore the little time consumed. But it is not rare to have a large matrix bigger than 1 million by 1 million and loop it billion times at least in an evolutionary problem. In this case, you are saving your life if the compulation time can be reduced.</p>\n<h1 id=\"A-shortcut-to-understand-GPU-parallel-computation\"><a href=\"#A-shortcut-to-understand-GPU-parallel-computation\" class=\"headerlink\" title=\"A shortcut to understand GPU parallel computation\"></a>A shortcut to understand GPU parallel computation</h1><p>Now we have a good way to save our life instead of having a weird pill. CUDA is a parallel computing platform and application programming interface (API) model created by <a href=\"https://developer.nvidia.com/nvidia-developer-zone\" target=\"_blank\" rel=\"noopener\">Nvidia</a>. <a href=\"https://documen.tician.de/pycuda/index.html\" target=\"_blank\" rel=\"noopener\">PyCUDA</a> provides a python package to allow people to parallelize their computation on a Graphic Processing Unit (GPU) by Python. For sure, I didn’t say everything is parallelizable. Only the procedures that are consisting of independent calculation paths can be parallelized, which means you work on different paths at the same time instead of doing them one by one. Hence, the amount of time that can be saved relies on the percentage of work that is parallelizable in your work. In this sense, matrix algebra is a good representative. Other applications of parallelism involves rendering graphics to a screen, running a Monte Carlo Simulation, multiplying matrices for a machine learning algorithm, or powering a database. Google them yourself. </p>\n<p>Why does GPU possess such powerful ability? It owes to its design purpose. A graphic card is designed to process graphics and render them to a screen. What is a graphic? It is just like a matrix. So to process a matrix, the ability to compute multiple entries of the matrix is more important than the ability to deal with just one entry smartly. Hence, a gpu is designed to own a huge amount of threads that can process multiple calculations at one time although they are slow while a cpu only contains few (like an i7 4790k has 4 physical cores which means 4  physical threads, at most 8 threads if including hyperthreads tech) but are faster. A good analogy is that a gpu is like a cluster of students at primary school while a cpu is like a professor. When calculating a complex problem, the professor is definitely faster and smarter. But when computing lots of simple algebra, the cluster of primary students is faster. In summary, CPUs are designed for running a small number of potentially quite complex tasks. GPUs are designed for running a large number of quite<br>simple tasks.</p>\n<h1 id=\"Parallelize-the-matrix-algebra\"><a href=\"#Parallelize-the-matrix-algebra\" class=\"headerlink\" title=\"Parallelize the matrix algebra\"></a>Parallelize the matrix algebra</h1><p>After having a rough idea of parallelism, let’s do it on practice. For a matrix algebra like the above example, what are the independent calculation paths? Apparently, each entry of the output matrix <code>c</code> is computed out independently given the input <code>a</code>. Thus, we could assign each calculation path to a thread on GPU. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">kernel_code_template = <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">__global__ void com_t(int matrixsize,float *a, float *c)</span></span><br><span class=\"line\"><span class=\"string\">&#123;</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    // 2D Thread ID </span></span><br><span class=\"line\"><span class=\"string\">    int tx = blockDim.x*blockIdx.x + threadIdx.x; // Compute column index</span></span><br><span class=\"line\"><span class=\"string\">    int ty = blockDim.y*blockIdx.y + threadIdx.y; // Compute row index</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    // Pvalue is used to store the element of the matrix</span></span><br><span class=\"line\"><span class=\"string\">    // that is computed by the thread</span></span><br><span class=\"line\"><span class=\"string\">    float Pvalue = 0;</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    // Each thread loads one row of M and one column of N, </span></span><br><span class=\"line\"><span class=\"string\">    //   to produce one element of P.</span></span><br><span class=\"line\"><span class=\"string\">    if((ty &lt;matrixsize) &amp;&amp; (tx &lt; matrixsize))</span></span><br><span class=\"line\"><span class=\"string\">    &#123;</span></span><br><span class=\"line\"><span class=\"string\">    float Aelement = a[ty];</span></span><br><span class=\"line\"><span class=\"string\">    float Belement = a[tx];</span></span><br><span class=\"line\"><span class=\"string\">    Pvalue = Aelement - Belement;</span></span><br><span class=\"line\"><span class=\"string\">    // Write the matrix to device memory;</span></span><br><span class=\"line\"><span class=\"string\">    // each thread writes one element</span></span><br><span class=\"line\"><span class=\"string\">    c[ty * matrixsize + tx] = Pvalue;</span></span><br><span class=\"line\"><span class=\"string\">    &#125;</span></span><br><span class=\"line\"><span class=\"string\">&#125;</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br></pre></td></tr></table></figure>\n<p>The most important part of parallelism on GPU is the kernel function which is coded for a single calculation path. The only difference among different calculation paths is the locality information of the entries of matrix <code>c</code> in the memory. Normally, we can envisage a memory on GPU as a cluster of grids of blocks of threads. But in fact, it is not this physical structure on the borad of the card. It involves streaming processors (SMs) dealing with wraps. However, here we better use the metaphor.</p>\n<p><img src=\"/2020/10/05/2019-01-14-PyCUDAseries2/gridblockthread.png\" alt=\"default\"></p>\n<p>Each entry in matrix <code>c</code> corresponds to a specific combination of <code>gridIdx.x,gridIdx.y,blockIdx.x,blockIdx.y,threadIdx.x,threadIdx.y</code>. The value of the indexes also depend on how we allocate the size of grid, block. A regular way for a matrix algebra is to fit the struture to the matrix. For example, if we want to compute a 5X5 matrix, we can allocate the size of block as 5X5 which means we will use a matrix of 5X5 threads for compuatation. But a block contains at most 1024 threads for the current card (GTX 970). Thus for a square matrix we can allocate at most a size of 32X32 threads for one block. If we want to work on a larger matrix, we need more blocks or even more grids. This then causes one issue that some threads may not be used if the matrix size is not a multiplier of the block size. Therefore, we need to constrain our computation within the matrix by using <code>if((ty &lt;matrixsize) &amp;&amp; (tx &lt; matrixsize))</code>. And make sure the evaluation of matrix <code>c</code> is put in the loop. Otherwise, the over requested threads (if the requested threads don’t fit the matrix size) will be invoked and replace the inner results in <code>c</code>.</p>\n<p>Once we have correctly allocated the memory and located the entry by these indexes, we can formulate your calculation. After that, the code will be uploaded to GPU and tranferred to the code of GPU (compile). Once all entries have been calculated, the results will be stored in the memory on GPU. Then cpu will pull back the result.  </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># compile the kernel code</span></span><br><span class=\"line\">mod = compiler.SourceModule(kernel_code_template)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># get the kernel function from the compiled module</span></span><br><span class=\"line\">matrixmul = mod.get_function(<span class=\"string\">\"com_t\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">matrixsize = <span class=\"number\">3</span></span><br><span class=\"line\">BLOCK_SIZE = <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># call the kernel on the card</span></span><br><span class=\"line\">matrixmul(np.uint32(matrixsize),</span><br><span class=\"line\">    <span class=\"comment\"># inputs</span></span><br><span class=\"line\">    a_gpu,</span><br><span class=\"line\">    <span class=\"comment\"># output</span></span><br><span class=\"line\">    c_gpu,</span><br><span class=\"line\">    <span class=\"comment\"># 4 blocks of BLOCK_SIZE x BLOCK_SIZE threads</span></span><br><span class=\"line\">    grid = (<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>),</span><br><span class=\"line\">    block = (BLOCK_SIZE, BLOCK_SIZE, <span class=\"number\">1</span>),</span><br><span class=\"line\">    )</span><br></pre></td></tr></table></figure>\n<h1 id=\"Speed-comparison-between-CPU-and-GPU\"><a href=\"#Speed-comparison-between-CPU-and-GPU\" class=\"headerlink\" title=\"Speed comparison between CPU and GPU\"></a>Speed comparison between CPU and GPU</h1><p>At last, to show the power of GPU even on such simple matrix algebra, I run calculation for different dimensions and loop each calculation 100 times to enlarge the time consumption.</p>\n<p><img src=\"/2020/10/05/2019-01-14-PyCUDAseries2/speedtest.png\" alt=\"default\"></p>\n<p>As the figure shows, the time consumption for 100 times calculations starts to significantly split up when the dimension of matrix is over 2000. The time consumption on CPU grows almost exponentially along the dimension while on gpu it only grows a little. With the increase of dimension, you save a huge amount of your time and your life.</p>\n<h1 id=\"End\"><a href=\"#End\" class=\"headerlink\" title=\"End\"></a>End</h1><p>This is a quite simplified introduction to a parallelism example. There is a bank of posts explaining parallelism better and more exhaustive than this one. However, my goal here is to use a simple example and a short length of words to have you get an idea of how it works instead of scaring you away from this field. Once you fall into the trap of “it looks as simple as the author said”, I believe you will learn more by yourself :-)</p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[CUDA Programming] by Shane Cook is a good start book for learning both the hardware and the language. </p>\n<p>This <a href=\"https://andreask.cs.illinois.edu/PyCuda\" target=\"_blank\" rel=\"noopener\">PyCUDA website</a> provides some examples on Python.</p>\n<h1 id=\"Full-code\"><a href=\"#Full-code\" class=\"headerlink\" title=\"Full code\"></a>Full code</h1><p>It contains the example code and the speed test. Clone it <a href=\"https://github.com/xl0418/GPU_Python/blob/master/gpu_cpu_speedtest.py\" target=\"_blank\" rel=\"noopener\">here</a></p>"},{"title":"PyCUDA series 3: matrix multiplication using multiple blocks","_content":"\nA simple practice on matrix multiplication is shown in this post. The matrix product function can use multiple blocks to calculate multiplications of two matrix. \n\n<!--more-->\n \n# A simple matrix multiplication\n\nThe most important part is the kernel function, which is given below\n\n```Python\nkernel_code_template = \"\"\"\n__global__ void matrixmulti(float *a, float *b, float *c)\n{\n\n    // 2D Thread ID \n    int tx = blockDim.x*blockIdx.x + threadIdx.x; // Compute column index\n    int ty = blockDim.y*blockIdx.y + threadIdx.y; // Compute row index\n\n    // Each thread loads one row of M and one column of N, \n    //   to produce one element of P.\n    if((ty <%(MATRIX_SIZE)s) && (tx < %(MATRIX_SIZE)s))\n    {\n    // Pvalue is used to store the element of the matrix\n    // that is computed by the thread\n    float Pvalue = 0;\n    for(int k=0; k<%(MATRIX_SIZE)s;++k)\n    {\n    float Aelement = a[ty*%(MATRIX_SIZE)s +k];\n    float Belement = b[k*%(MATRIX_SIZE)s +tx];\n    Pvalue += Aelement * Belement;\n    }\n    c[ty * %(MATRIX_SIZE)s + tx] = Pvalue;\n    }\n\n}\n\"\"\"\n\n```\n\nNote that the evaluation of C should be put in the conditional loop to guarentee that over-requested threads would not be invoked. \n\n\n# An odd bug\n\nThe code works well when the matrix size is less than 320\\*320 and requesting block size to be 32\\*32. But when the matrix size exceeds 320, like 321, the matrix product produced by GPU is not equal to the result by CPU. The difference between them is very tiny, like the scale of 1e-5. So far, I don't quite understand where this bug comes from. Probabily it is due to the limit of the number of blocks in one grid? \n\n\n\n# Full code\nIt contains the example code and the speed test. Clone it [here](https://github.com/xl0418/GPU_Python/blob/master/gpu_matrixmultiplication.py)\n","source":"_posts/2019-01-21-PyCUDAseries3.md","raw":"---\ntitle: \"PyCUDA series 3: matrix multiplication using multiple blocks\"\ncategories: [Research,GPU programming,pyCUDA]\ntags: [Python, CUDA, GPU programming, Parallel computation]\n---\n\nA simple practice on matrix multiplication is shown in this post. The matrix product function can use multiple blocks to calculate multiplications of two matrix. \n\n<!--more-->\n \n# A simple matrix multiplication\n\nThe most important part is the kernel function, which is given below\n\n```Python\nkernel_code_template = \"\"\"\n__global__ void matrixmulti(float *a, float *b, float *c)\n{\n\n    // 2D Thread ID \n    int tx = blockDim.x*blockIdx.x + threadIdx.x; // Compute column index\n    int ty = blockDim.y*blockIdx.y + threadIdx.y; // Compute row index\n\n    // Each thread loads one row of M and one column of N, \n    //   to produce one element of P.\n    if((ty <%(MATRIX_SIZE)s) && (tx < %(MATRIX_SIZE)s))\n    {\n    // Pvalue is used to store the element of the matrix\n    // that is computed by the thread\n    float Pvalue = 0;\n    for(int k=0; k<%(MATRIX_SIZE)s;++k)\n    {\n    float Aelement = a[ty*%(MATRIX_SIZE)s +k];\n    float Belement = b[k*%(MATRIX_SIZE)s +tx];\n    Pvalue += Aelement * Belement;\n    }\n    c[ty * %(MATRIX_SIZE)s + tx] = Pvalue;\n    }\n\n}\n\"\"\"\n\n```\n\nNote that the evaluation of C should be put in the conditional loop to guarentee that over-requested threads would not be invoked. \n\n\n# An odd bug\n\nThe code works well when the matrix size is less than 320\\*320 and requesting block size to be 32\\*32. But when the matrix size exceeds 320, like 321, the matrix product produced by GPU is not equal to the result by CPU. The difference between them is very tiny, like the scale of 1e-5. So far, I don't quite understand where this bug comes from. Probabily it is due to the limit of the number of blocks in one grid? \n\n\n\n# Full code\nIt contains the example code and the speed test. Clone it [here](https://github.com/xl0418/GPU_Python/blob/master/gpu_matrixmultiplication.py)\n","slug":"2019-01-21-PyCUDAseries3","published":1,"date":"2020-10-05T11:30:58.228Z","updated":"2020-10-05T11:30:58.233Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy131000znj39y9u9ccvc","content":"<p>A simple practice on matrix multiplication is shown in this post. The matrix product function can use multiple blocks to calculate multiplications of two matrix. </p>\n<a id=\"more\"></a>\n<h1 id=\"A-simple-matrix-multiplication\"><a href=\"#A-simple-matrix-multiplication\" class=\"headerlink\" title=\"A simple matrix multiplication\"></a>A simple matrix multiplication</h1><p>The most important part is the kernel function, which is given below</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kernel_code_template = <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">__global__ void matrixmulti(float *a, float *b, float *c)</span></span><br><span class=\"line\"><span class=\"string\">&#123;</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    // 2D Thread ID </span></span><br><span class=\"line\"><span class=\"string\">    int tx = blockDim.x*blockIdx.x + threadIdx.x; // Compute column index</span></span><br><span class=\"line\"><span class=\"string\">    int ty = blockDim.y*blockIdx.y + threadIdx.y; // Compute row index</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    // Each thread loads one row of M and one column of N, </span></span><br><span class=\"line\"><span class=\"string\">    //   to produce one element of P.</span></span><br><span class=\"line\"><span class=\"string\">    if((ty &lt;%(MATRIX_SIZE)s) &amp;&amp; (tx &lt; %(MATRIX_SIZE)s))</span></span><br><span class=\"line\"><span class=\"string\">    &#123;</span></span><br><span class=\"line\"><span class=\"string\">    // Pvalue is used to store the element of the matrix</span></span><br><span class=\"line\"><span class=\"string\">    // that is computed by the thread</span></span><br><span class=\"line\"><span class=\"string\">    float Pvalue = 0;</span></span><br><span class=\"line\"><span class=\"string\">    for(int k=0; k&lt;%(MATRIX_SIZE)s;++k)</span></span><br><span class=\"line\"><span class=\"string\">    &#123;</span></span><br><span class=\"line\"><span class=\"string\">    float Aelement = a[ty*%(MATRIX_SIZE)s +k];</span></span><br><span class=\"line\"><span class=\"string\">    float Belement = b[k*%(MATRIX_SIZE)s +tx];</span></span><br><span class=\"line\"><span class=\"string\">    Pvalue += Aelement * Belement;</span></span><br><span class=\"line\"><span class=\"string\">    &#125;</span></span><br><span class=\"line\"><span class=\"string\">    c[ty * %(MATRIX_SIZE)s + tx] = Pvalue;</span></span><br><span class=\"line\"><span class=\"string\">    &#125;</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">&#125;</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br></pre></td></tr></table></figure>\n<p>Note that the evaluation of C should be put in the conditional loop to guarentee that over-requested threads would not be invoked. </p>\n<h1 id=\"An-odd-bug\"><a href=\"#An-odd-bug\" class=\"headerlink\" title=\"An odd bug\"></a>An odd bug</h1><p>The code works well when the matrix size is less than 320*320 and requesting block size to be 32*32. But when the matrix size exceeds 320, like 321, the matrix product produced by GPU is not equal to the result by CPU. The difference between them is very tiny, like the scale of 1e-5. So far, I don’t quite understand where this bug comes from. Probabily it is due to the limit of the number of blocks in one grid? </p>\n<h1 id=\"Full-code\"><a href=\"#Full-code\" class=\"headerlink\" title=\"Full code\"></a>Full code</h1><p>It contains the example code and the speed test. Clone it <a href=\"https://github.com/xl0418/GPU_Python/blob/master/gpu_matrixmultiplication.py\" target=\"_blank\" rel=\"noopener\">here</a></p>\n","site":{"data":{}},"excerpt":"<p>A simple practice on matrix multiplication is shown in this post. The matrix product function can use multiple blocks to calculate multiplications of two matrix. </p>","more":"<h1 id=\"A-simple-matrix-multiplication\"><a href=\"#A-simple-matrix-multiplication\" class=\"headerlink\" title=\"A simple matrix multiplication\"></a>A simple matrix multiplication</h1><p>The most important part is the kernel function, which is given below</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kernel_code_template = <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">__global__ void matrixmulti(float *a, float *b, float *c)</span></span><br><span class=\"line\"><span class=\"string\">&#123;</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    // 2D Thread ID </span></span><br><span class=\"line\"><span class=\"string\">    int tx = blockDim.x*blockIdx.x + threadIdx.x; // Compute column index</span></span><br><span class=\"line\"><span class=\"string\">    int ty = blockDim.y*blockIdx.y + threadIdx.y; // Compute row index</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    // Each thread loads one row of M and one column of N, </span></span><br><span class=\"line\"><span class=\"string\">    //   to produce one element of P.</span></span><br><span class=\"line\"><span class=\"string\">    if((ty &lt;%(MATRIX_SIZE)s) &amp;&amp; (tx &lt; %(MATRIX_SIZE)s))</span></span><br><span class=\"line\"><span class=\"string\">    &#123;</span></span><br><span class=\"line\"><span class=\"string\">    // Pvalue is used to store the element of the matrix</span></span><br><span class=\"line\"><span class=\"string\">    // that is computed by the thread</span></span><br><span class=\"line\"><span class=\"string\">    float Pvalue = 0;</span></span><br><span class=\"line\"><span class=\"string\">    for(int k=0; k&lt;%(MATRIX_SIZE)s;++k)</span></span><br><span class=\"line\"><span class=\"string\">    &#123;</span></span><br><span class=\"line\"><span class=\"string\">    float Aelement = a[ty*%(MATRIX_SIZE)s +k];</span></span><br><span class=\"line\"><span class=\"string\">    float Belement = b[k*%(MATRIX_SIZE)s +tx];</span></span><br><span class=\"line\"><span class=\"string\">    Pvalue += Aelement * Belement;</span></span><br><span class=\"line\"><span class=\"string\">    &#125;</span></span><br><span class=\"line\"><span class=\"string\">    c[ty * %(MATRIX_SIZE)s + tx] = Pvalue;</span></span><br><span class=\"line\"><span class=\"string\">    &#125;</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">&#125;</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br></pre></td></tr></table></figure>\n<p>Note that the evaluation of C should be put in the conditional loop to guarentee that over-requested threads would not be invoked. </p>\n<h1 id=\"An-odd-bug\"><a href=\"#An-odd-bug\" class=\"headerlink\" title=\"An odd bug\"></a>An odd bug</h1><p>The code works well when the matrix size is less than 320*320 and requesting block size to be 32*32. But when the matrix size exceeds 320, like 321, the matrix product produced by GPU is not equal to the result by CPU. The difference between them is very tiny, like the scale of 1e-5. So far, I don’t quite understand where this bug comes from. Probabily it is due to the limit of the number of blocks in one grid? </p>\n<h1 id=\"Full-code\"><a href=\"#Full-code\" class=\"headerlink\" title=\"Full code\"></a>Full code</h1><p>It contains the example code and the speed test. Clone it <a href=\"https://github.com/xl0418/GPU_Python/blob/master/gpu_matrixmultiplication.py\" target=\"_blank\" rel=\"noopener\">here</a></p>"},{"title":"Machine learning on biology S2: how does a neural network work mathematically?","top":4,"_content":"\nThere are tons of documents out there to explain how neural network works in different angles. So I guess people don't mind me adding one more from my perspective. Hopefully, someone may get inspired from this post.\n\n<!--more-->\n\nIn short, the neural network is a kind of system that transforms the input data into its corresponding output (or labels), strictly speaking, for a supervised learning. This system is consisting of three types of layers, i.e. the input layer, the hidden layer and the output layer. Normally, the hidden layer may have multiple layers according to one's design. Among layers, the data from the upper layer are transformed to the data in the lower layer via a linear combination with an initialized weight matrix. After that, a nonlinear transformation that is generally called the activation function is applied to the data that would be converted to a form for the next round until reaching the output layer. To assure the neural network feedback the correct output, people need to train the neural network by adjusting the weight matrix. They are usually adjusted via minimizing the error between the output from the final layer and the known output from the data used for training. \n\nWithout loss of generality, here I consider a simplest neural network which only possesses fully connected layers, meaning that every neuron in each layer connects all neurons of the upper and lower layers. The mathematical logic behind is analogous to other structures of neural network. Firstly, I used a series of graphs to illustrate how the data flows to the final layer and introduce the notations used in the derivation. Then, a general formulation is present subsequently.\n\n# A 6-layer neural network \n\nThe following four graphs illustrate how the data evolves along the neural network and the notations used in the derivation.\n\n![fig](2019-02-08-Machinelearningseries2/step1.png)    \n\n![fig](2019-02-08-Machinelearningseries2/step2.png)    \n\n![fig](2019-02-08-Machinelearningseries2/step3.png)    \n\n![fig](2019-02-08-Machinelearningseries2/step4.png)   \n\n![fig](2019-02-08-Machinelearningseries2/step5.png) \n\n![fig](2019-02-08-Machinelearningseries2/step8.png)         \n\n![fig](2019-02-08-Machinelearningseries2/step7.png)     \n\n![fig](2019-02-08-Machinelearningseries2/step6.png)     \n\n\n\n# 1 Forward flow of the neural network\n\nConsider a neural network with \\\\(k+1\\\\) layers including the input layer and the output layer. The input data is consisting of \\\\(n\\\\) samples with \\\\(L_{1}\\\\) features\n\n![fig](2019-02-08-Machinelearningseries2/1.png)   \n\nThe output label is given accordingly\n\n![fig](2019-02-08-Machinelearningseries2/2.png)   \n\nwhere the label on the top left denotes the index of the sample. Too abstract? Imagine that sample \\\\(^{1}\\boldsymbol{x}\\\\) corresponds to model 1 while sample \\\\(^{2}\\boldsymbol{x}\\\\) corresponds to model 2. Then the output can be either an array of two vectors indicating the probabilities of model 1 and model 2, for example, \\\\(\\{(1,0),(0,1)\\}\\\\), or a vector of two elements \\\\(\\{0,1\\}\\\\) where 0 indicates model 1 and 1 indicates model 2 or whatever you label them. The purpose is to use a huge amount of data sets to train the neural network, more precisely to compute the weight matrix among the adjacent pair of layers, to fit the output layer to the output vectors. Will see it later on.\n\nNow, let's feed one sample (the first one \\\\(^{1}\\boldsymbol{x}\\\\)) to the neural network to see how to compute the output layer. Later, we will see how to train the network with multiple samples. Assume the second layer has \\\\(L_{2}\\\\) neurons\n\n![fig](2019-02-08-Machinelearningseries2/3.png)   \n\nNote that the first layer is the input layer in which we feed one sample with \\\\(L_{1}\\\\) feature to the neural network at first. Then the weight matrix from the first layer to the second layer is defined as \n\n![fig](2019-02-08-Machinelearningseries2/4.png)   \n\nThus, multiplying the weight matrix Eq.4 with the neurons of the upper layer Eq.3 yields\n\n![fig](2019-02-08-Machinelearningseries2/5.png)   \n\nMore generally, a bias term (constant term) is incorporated as follows\n\n![fig](2019-02-08-Machinelearningseries2/6.png) \n\nwhich can also be written in the form if we absorb the constant vector \\\\(\\boldsymbol{b}_{1}\\\\) into the weight matrix  \n\n![fig](2019-02-08-Machinelearningseries2/78.png) \n\nAfter the transformation, an activation function is applied to \\\\(\\boldsymbol{z}^{(2)}\\\\) elementwisely. For a model classification problem, the sigmoid function and the hyperbolic tangent function are widely used. Here we use the sigmoid function for instance\n\n![fig](2019-02-08-Machinelearningseries2/9.png) \n\nTill now, the transformation from the first layer (the input layer) to the second layer (the first hidden layer) is done. This process can be generalized as .\n\n![fig](2019-02-08-Machinelearningseries2/9-1.png) \n\nwhere \\\\(\\boldsymbol{a}'\\\\) is the vector \\\\(\\boldsymbol{a}\\\\) absorbing 1 at the end as what I did in Eq.8. Note that \\\\(\\boldsymbol{a}^{(1)}=^{1}\\boldsymbol{x}\\\\).\n\nFinally, the neural network will return \\\\(\\boldsymbol{a}^{(k+1)}\\\\) with \\\\(L_{k+1}\\\\) elements from the output layer. Given the corresponding output label \\\\(\\boldsymbol{y}^{(1)}\\\\), we can compute the error between the feedback \\\\(\\boldsymbol{a}^{(k+1)}\\\\) and the output label \\\\(\\boldsymbol{y}^{(1)}\\\\). This is normally called the loss of the result to the output. There are several candidate loss functions for model classification like the least square, the cross-entropy function. We take the least square function as the example\n\n![fig](2019-02-08-Machinelearningseries2/10.png) \n\nSo far, we have computed out the loss of the neural network with a bunch of randomly initialized weight matrix. No doubt, the loss would be huge. Our aim is to minimized the loss \\\\(J\\\\) by tuning the weight matrix. How? Remember your advanced calculus in the high school or the university? \\\\(J\\\\) can be envisaged as a function of every entry in the weight matrix that we want to adjust. Thus, the derivative of \\\\(J\\\\) with respect to each entries of the weight matrix would tell us how to tune the weight matrix to minimize the loss. This method is called gradient descend. And the loss can also propagate backwards to determine the gradient of the entries of the weight matrix at each layer. The full process to tune the weight matrix is also called Backward Propagation. \n\n\n# 2 Backward propagation\n## From the \\\\(k+1\\\\)th layer to the \\\\(k\\\\)th layer\n\nLet us start from the final layer (the \\\\(k+1\\\\)th layer) to the previous one (the \\\\(k\\\\)th layer). Note that the variable of our concern is the entry of the weight matrix from the \\\\(k\\\\)th layer to the \\\\(k+1\\\\)th layer, \\\\(\\theta_{L_{k+1}\\times L_{k}}^{(k)}\\\\). Here I only consider tuning \\\\(\\theta\\\\) instead of \\\\(\\theta'\\\\), meaning that the bias terms are not tuned. Updating the bias terms is similar and you can practice it afterward. The derivative of the loss function with respect to the matrix is defined as \n\n![fig](2019-02-08-Machinelearningseries2/11.png) \n\nAt the mean time, from the loss function Eq.10, we obtain the derivative according to the chain rule\n\n![fig](2019-02-08-Machinelearningseries2/12.png) \n\nwhere\n\n![fig](2019-02-08-Machinelearningseries2/13-15.png) \n\nand\n\n![fig](2019-02-08-Machinelearningseries2/16.png) \n\nwhere \\\\(\\otimes\\\\) is the outer product. Simplifying the gradient Eq.12 yields\n\n![fig](2019-02-08-Machinelearningseries2/17.png) \n\nIf we define\n\n![fig](2019-02-08-Machinelearningseries2/18.png) \n\nThe gradient Eq.17 can be further simplified as\n\n![fig](2019-02-08-Machinelearningseries2/19.png) \n\nNote that we define the outer product of two column vectors is the element-wise produce of the corresponding elements\n\n![fig](2019-02-08-Machinelearningseries2/19-1.png) \n\nNow, as \\\\(\\boldsymbol{z}^{(k+1)}\\\\), \\\\(\\boldsymbol{a}^{(k+1)}\\\\) and \\\\(^{1}\\boldsymbol{y}\\\\) are known, we can update the weight matrix \\\\(\\theta^{(k)}\\\\) by\n\n![fig](2019-02-08-Machinelearningseries2/20.png) \n\nwhere \\\\(\\lambda\\\\) is a constant called the learning rate given in advance. \n\n## From the \\\\(k\\\\)th layer to the \\\\(k-1\\\\)th layer\n\nLet's do the calculation one more time from the \\\\(k\\\\)th layer to the \\\\(k-1\\\\)th layer. At the end of this section, we will get a general formula to update all weight matrix via which we can develop the algorithm for a deep neural network. \n\nNow we consider one more previous weight matrix \\\\(\\theta^{(k-1)}\\\\). The derivative of the loss function with respect to that matrix yields \n\n![fig](2019-02-08-Machinelearningseries2/21.png) \n\nThe first two terms on the right hand side are the same as Eq.12. The third term \\\\(\\frac{\\partial\\boldsymbol{z}^{(k+1)}}{\\partial\\boldsymbol{a}^{(k)}}\\\\) produces the weight matrix from the \\\\(k+1\\\\)th layer to the \\\\(k\\\\)th layer\n\n![fig](2019-02-08-Machinelearningseries2/22.png) \n\nThe last two terms on the right hand side are similar to what we have done above. Finally, we get\n\n![fig](2019-02-08-Machinelearningseries2/23.png) \n\nComparing Eq.19 and Eq.23 tells us that we can update the error\n\n![fig](2019-02-08-Machinelearningseries2/24.png) \n\nfor each transition among layers. This is how the error propagates backwards along the neural network and where the name comes from. Note that this expression is a slightly different from the formula in Chapter 9.2 of the [machine learning course](https://www.coursera.org/course/ml) by Andrew Ng of Standford. Do you see why is that? \n\n# 3 Training multiple samples\n\nWe have derived mathematically how to train one sample on a neural network. How about multiple samples? Easy. Because all the weight matrix of the neural network are shared for all samples, we can update the weight matrix by a fraction of error of each sample. Normally this fraction is \\\\(\\frac{1}{\\text{sample size}}\\\\) where in our example the sample size is \\\\(n\\\\)\n\n![fig](2019-02-08-Machinelearningseries2/25.png) \n\nThen, the weight matrix is tuned to minimize the error of samples. \n\n# 4 Program a neural network \n\nTill now, we have derived a general formula Eq.23 to allow us to update all weight matrix. After updating, the loss function is applied again to examine if the error is sufficiently small. If not, update the weight matrix again till meeting our criterion. Understanding the math behind is a huge step towards the expert level but not the final one. Whether you can equip it via code is essential. Here I attached my code in Python from my perspective as a reference. The user can add any number of hidden layers and deploy any number of neurons there. It is a bit like a minimalistic version of tensorflow. \n\n```Python\nimport numpy as np\nimport pandas as pd\n\nclass neuralnetwork:\n    # initialize parameters\n    def __init__(self,num_sample,num_hidden_layer_units,num_input_feature,num_output_feature,bias,learningrate):\n        self.learningrate=learningrate # learning rate\n        self.num_sample = num_sample  # number of the samples\n        self.num_hidden_layer_units = num_hidden_layer_units # a list indicating the number of hidden layers and how many neurons for each layer\n        self.num_hidden_layer = len(num_hidden_layer_units) # the number of the hidden layers\n        self.weight_layer = []  # initialize the weight matrix\n        self.bias = bias   # bias for the input layer and for the hidden layers\n        assert len(self.bias) == self.num_hidden_layer+1, \"The length of the biases should equal the length of the hidden layers plus 1!!!\"\n        self.units = [num_input_feature] + num_hidden_layer_units + [num_output_feature]\n        # randomly initialize the weight matrix\n        for num_layer in range(0,len(self.units)-1):\n            temp_bias = np.zeros((1,self.units[num_layer+1]))\n            temp_bias.fill(self.bias[num_layer])\n            self.weight_layer.append(np.concatenate((np.random.randn(self.units[num_layer+1],\n                                                self.units[num_layer]),temp_bias.T),axis = 1))\n\n    # the activation function: sigmoid\n    # could be replaced by whatever you want\n    def sigmoid(self,x):\n        y = 1/(1+np.exp(-x))\n        return y\n\n    # train function requires the training data, accuracy and iteration limit\n    def train(self, input, output,accuracy, iteration_limit):\n        go_on = True\n        i = 0 # iteration indicator\n        error = []  # error list recoding errors for all iterations\n        iteration = []\n        while go_on:\n            iteration.append(i)\n            error_acc = 0   # initialize error for every iteration\n            stderror = []   # standard error\n            sample_Z = []   # Z value of neurons for every sample\n            sample_a = []   # sigmoid value of Z for every sample\n            # loop all samples\n            for sample_iter in range(self.num_sample):\n                Zlayer = []\n                a = []\n                a.append(input[sample_iter])    # place the input as the first a value\n                # forward computing Z and a values for all layers\n                for forward_layer in range(len(self.units)-1):\n                    Zlayer.append(np.matmul(self.weight_layer[forward_layer], np.concatenate((a[forward_layer],[1]))))\n                    a.append(self.sigmoid(Zlayer[forward_layer]))\n                # standard error by computing the distance between output layer and true output\n                stderror.append(a[len(a)-1]-output[sample_iter])\n                # loss function defined as the sum of the least square\n                # can be replaced by other functions like cross-entropy\n                error_acc += (1/2*np.sum((stderror[sample_iter])**2))\n                sample_a.append(a)  # store a values for updating\n                sample_Z.append(Zlayer) # store Z values for updating\n            error.append(error_acc) # store error for this iteration\n\n            # backward propagate errors to update the weight matrix\n            i += 1\n            if error[i-1] <accuracy or i >iteration_limit:\n                go_on = False\n            else:\n                for sample_iter in range(self.num_sample):\n                    delta = stderror[sample_iter]  # delta: standard error defined as the derivative of the loss function\n                    # initialize g'(z): the derivative of the activation function at the output layer\n                    deriv_sigmoid =  sample_a[sample_iter][len(sample_a[sample_iter])-1] *\\\n                                 (1 - sample_a[sample_iter][len(sample_a[sample_iter])-1])\n                    # backward propagation\n                    for backward_layer in list(reversed(range(len(self.units)-1))):\n                        temp_weight = self.weight_layer[backward_layer] # store the temporary kth matrix\n                        # update the kth matrix\n                        self.weight_layer[backward_layer][:,:-1] += - self.learningrate/self.num_sample *\\\n                            np.outer(delta * deriv_sigmoid, sample_a[sample_iter][backward_layer])\n                        # update delta and derivative of the activation function\n                        delta = np.dot(temp_weight[:,:-1].T,delta * deriv_sigmoid)\n                        deriv_sigmoid = sample_a[sample_iter][backward_layer] *\\\n                                 (1 - sample_a[sample_iter][backward_layer])\n        self.learningspeed = {'error': error, 'iteration': iteration}\n        self.lsdf = pd.DataFrame(self.learningspeed)\n        return sample_a, self.weight_layer\n\n    # predict\n    def predict(self,input,weight):\n        out = input+[1]\n        for i in range(len(weight)):\n            out = np.dot(weight[i],np.array(out))\n            out = self.sigmoid(out)\n            out = np.concatenate((out,[1]))\n        return out[:-1]\n\n\n# test\ninputx = np.array(([0.2, 0.5, 0.5,0.3], [0.1, 0.2, 0.15,0.1],[0.7,0.9,0.4,0.8]))\noutputy = np.array(([0.5, 0.4], [0.9, 0.1],[0.3,0.5]))\n\n# build the neural network\nnn=neuralnetwork(num_sample=inputx.shape[0],num_hidden_layer_units=[15,14,15,16],num_input_feature=inputx.shape[1],\n             num_output_feature=outputy.shape[1], bias=[0.1,0.2,0.3,0.4,0.4],learningrate=0.2)\n\n# set the accuracy and the iteration limit\nacc = 1e-7\niter = 20000\n# train the neural network\nout_a,weight = nn.train(inputx,outputy,accuracy = acc,iteration_limit=iter)\n# plot the learning process\nnn.lsdf.plot(x='iteration',y='error')\n\n# predict\ninput = [0.19, 0.51, 0.49,0.29]\nout = nn.predict(input,weight)\n\n```\n\nYou can also clone it [on my Github](https://github.com/xl0418/Tensorflow/blob/master/NeuralNetwork.py) \n\n# The end\n\nMy code is obviously not the most efficient one as a lot of loops are used. But in another sense it is easy to read for a starter. As you see in the derivation and in the code, there are a huge amount of independent computing that can be parallelized to speed up. So the future plan is to parallelize the code on GPU which may substantially improve the efficiency of the neural network. It would also be a good practice for you to step into the machine learning field. \n\n# Reference\n\n- An introduction to the math used in Machine Learning:  [The Matrix Calculus You Need For Deep Learning\n](https://explained.ai/matrix-calculus/index.html).\n\n- A concrete derivation of backward propagation for a two-layer neural network in Chinese [here](http://www.cnblogs.com/charlotte77/p/5629865.html#!comments).","source":"_posts/2019-02-08-Machinelearningseries2.md","raw":"---\ntitle: \"Machine learning on biology S2: how does a neural network work mathematically?\"\ntop: 4\ncategories: [Research,Machine learning,Deep learning]\ntags: [Python, Machine learning, neural networks, Backward propagation, gradient descent]\n---\n\nThere are tons of documents out there to explain how neural network works in different angles. So I guess people don't mind me adding one more from my perspective. Hopefully, someone may get inspired from this post.\n\n<!--more-->\n\nIn short, the neural network is a kind of system that transforms the input data into its corresponding output (or labels), strictly speaking, for a supervised learning. This system is consisting of three types of layers, i.e. the input layer, the hidden layer and the output layer. Normally, the hidden layer may have multiple layers according to one's design. Among layers, the data from the upper layer are transformed to the data in the lower layer via a linear combination with an initialized weight matrix. After that, a nonlinear transformation that is generally called the activation function is applied to the data that would be converted to a form for the next round until reaching the output layer. To assure the neural network feedback the correct output, people need to train the neural network by adjusting the weight matrix. They are usually adjusted via minimizing the error between the output from the final layer and the known output from the data used for training. \n\nWithout loss of generality, here I consider a simplest neural network which only possesses fully connected layers, meaning that every neuron in each layer connects all neurons of the upper and lower layers. The mathematical logic behind is analogous to other structures of neural network. Firstly, I used a series of graphs to illustrate how the data flows to the final layer and introduce the notations used in the derivation. Then, a general formulation is present subsequently.\n\n# A 6-layer neural network \n\nThe following four graphs illustrate how the data evolves along the neural network and the notations used in the derivation.\n\n![fig](2019-02-08-Machinelearningseries2/step1.png)    \n\n![fig](2019-02-08-Machinelearningseries2/step2.png)    \n\n![fig](2019-02-08-Machinelearningseries2/step3.png)    \n\n![fig](2019-02-08-Machinelearningseries2/step4.png)   \n\n![fig](2019-02-08-Machinelearningseries2/step5.png) \n\n![fig](2019-02-08-Machinelearningseries2/step8.png)         \n\n![fig](2019-02-08-Machinelearningseries2/step7.png)     \n\n![fig](2019-02-08-Machinelearningseries2/step6.png)     \n\n\n\n# 1 Forward flow of the neural network\n\nConsider a neural network with \\\\(k+1\\\\) layers including the input layer and the output layer. The input data is consisting of \\\\(n\\\\) samples with \\\\(L_{1}\\\\) features\n\n![fig](2019-02-08-Machinelearningseries2/1.png)   \n\nThe output label is given accordingly\n\n![fig](2019-02-08-Machinelearningseries2/2.png)   \n\nwhere the label on the top left denotes the index of the sample. Too abstract? Imagine that sample \\\\(^{1}\\boldsymbol{x}\\\\) corresponds to model 1 while sample \\\\(^{2}\\boldsymbol{x}\\\\) corresponds to model 2. Then the output can be either an array of two vectors indicating the probabilities of model 1 and model 2, for example, \\\\(\\{(1,0),(0,1)\\}\\\\), or a vector of two elements \\\\(\\{0,1\\}\\\\) where 0 indicates model 1 and 1 indicates model 2 or whatever you label them. The purpose is to use a huge amount of data sets to train the neural network, more precisely to compute the weight matrix among the adjacent pair of layers, to fit the output layer to the output vectors. Will see it later on.\n\nNow, let's feed one sample (the first one \\\\(^{1}\\boldsymbol{x}\\\\)) to the neural network to see how to compute the output layer. Later, we will see how to train the network with multiple samples. Assume the second layer has \\\\(L_{2}\\\\) neurons\n\n![fig](2019-02-08-Machinelearningseries2/3.png)   \n\nNote that the first layer is the input layer in which we feed one sample with \\\\(L_{1}\\\\) feature to the neural network at first. Then the weight matrix from the first layer to the second layer is defined as \n\n![fig](2019-02-08-Machinelearningseries2/4.png)   \n\nThus, multiplying the weight matrix Eq.4 with the neurons of the upper layer Eq.3 yields\n\n![fig](2019-02-08-Machinelearningseries2/5.png)   \n\nMore generally, a bias term (constant term) is incorporated as follows\n\n![fig](2019-02-08-Machinelearningseries2/6.png) \n\nwhich can also be written in the form if we absorb the constant vector \\\\(\\boldsymbol{b}_{1}\\\\) into the weight matrix  \n\n![fig](2019-02-08-Machinelearningseries2/78.png) \n\nAfter the transformation, an activation function is applied to \\\\(\\boldsymbol{z}^{(2)}\\\\) elementwisely. For a model classification problem, the sigmoid function and the hyperbolic tangent function are widely used. Here we use the sigmoid function for instance\n\n![fig](2019-02-08-Machinelearningseries2/9.png) \n\nTill now, the transformation from the first layer (the input layer) to the second layer (the first hidden layer) is done. This process can be generalized as .\n\n![fig](2019-02-08-Machinelearningseries2/9-1.png) \n\nwhere \\\\(\\boldsymbol{a}'\\\\) is the vector \\\\(\\boldsymbol{a}\\\\) absorbing 1 at the end as what I did in Eq.8. Note that \\\\(\\boldsymbol{a}^{(1)}=^{1}\\boldsymbol{x}\\\\).\n\nFinally, the neural network will return \\\\(\\boldsymbol{a}^{(k+1)}\\\\) with \\\\(L_{k+1}\\\\) elements from the output layer. Given the corresponding output label \\\\(\\boldsymbol{y}^{(1)}\\\\), we can compute the error between the feedback \\\\(\\boldsymbol{a}^{(k+1)}\\\\) and the output label \\\\(\\boldsymbol{y}^{(1)}\\\\). This is normally called the loss of the result to the output. There are several candidate loss functions for model classification like the least square, the cross-entropy function. We take the least square function as the example\n\n![fig](2019-02-08-Machinelearningseries2/10.png) \n\nSo far, we have computed out the loss of the neural network with a bunch of randomly initialized weight matrix. No doubt, the loss would be huge. Our aim is to minimized the loss \\\\(J\\\\) by tuning the weight matrix. How? Remember your advanced calculus in the high school or the university? \\\\(J\\\\) can be envisaged as a function of every entry in the weight matrix that we want to adjust. Thus, the derivative of \\\\(J\\\\) with respect to each entries of the weight matrix would tell us how to tune the weight matrix to minimize the loss. This method is called gradient descend. And the loss can also propagate backwards to determine the gradient of the entries of the weight matrix at each layer. The full process to tune the weight matrix is also called Backward Propagation. \n\n\n# 2 Backward propagation\n## From the \\\\(k+1\\\\)th layer to the \\\\(k\\\\)th layer\n\nLet us start from the final layer (the \\\\(k+1\\\\)th layer) to the previous one (the \\\\(k\\\\)th layer). Note that the variable of our concern is the entry of the weight matrix from the \\\\(k\\\\)th layer to the \\\\(k+1\\\\)th layer, \\\\(\\theta_{L_{k+1}\\times L_{k}}^{(k)}\\\\). Here I only consider tuning \\\\(\\theta\\\\) instead of \\\\(\\theta'\\\\), meaning that the bias terms are not tuned. Updating the bias terms is similar and you can practice it afterward. The derivative of the loss function with respect to the matrix is defined as \n\n![fig](2019-02-08-Machinelearningseries2/11.png) \n\nAt the mean time, from the loss function Eq.10, we obtain the derivative according to the chain rule\n\n![fig](2019-02-08-Machinelearningseries2/12.png) \n\nwhere\n\n![fig](2019-02-08-Machinelearningseries2/13-15.png) \n\nand\n\n![fig](2019-02-08-Machinelearningseries2/16.png) \n\nwhere \\\\(\\otimes\\\\) is the outer product. Simplifying the gradient Eq.12 yields\n\n![fig](2019-02-08-Machinelearningseries2/17.png) \n\nIf we define\n\n![fig](2019-02-08-Machinelearningseries2/18.png) \n\nThe gradient Eq.17 can be further simplified as\n\n![fig](2019-02-08-Machinelearningseries2/19.png) \n\nNote that we define the outer product of two column vectors is the element-wise produce of the corresponding elements\n\n![fig](2019-02-08-Machinelearningseries2/19-1.png) \n\nNow, as \\\\(\\boldsymbol{z}^{(k+1)}\\\\), \\\\(\\boldsymbol{a}^{(k+1)}\\\\) and \\\\(^{1}\\boldsymbol{y}\\\\) are known, we can update the weight matrix \\\\(\\theta^{(k)}\\\\) by\n\n![fig](2019-02-08-Machinelearningseries2/20.png) \n\nwhere \\\\(\\lambda\\\\) is a constant called the learning rate given in advance. \n\n## From the \\\\(k\\\\)th layer to the \\\\(k-1\\\\)th layer\n\nLet's do the calculation one more time from the \\\\(k\\\\)th layer to the \\\\(k-1\\\\)th layer. At the end of this section, we will get a general formula to update all weight matrix via which we can develop the algorithm for a deep neural network. \n\nNow we consider one more previous weight matrix \\\\(\\theta^{(k-1)}\\\\). The derivative of the loss function with respect to that matrix yields \n\n![fig](2019-02-08-Machinelearningseries2/21.png) \n\nThe first two terms on the right hand side are the same as Eq.12. The third term \\\\(\\frac{\\partial\\boldsymbol{z}^{(k+1)}}{\\partial\\boldsymbol{a}^{(k)}}\\\\) produces the weight matrix from the \\\\(k+1\\\\)th layer to the \\\\(k\\\\)th layer\n\n![fig](2019-02-08-Machinelearningseries2/22.png) \n\nThe last two terms on the right hand side are similar to what we have done above. Finally, we get\n\n![fig](2019-02-08-Machinelearningseries2/23.png) \n\nComparing Eq.19 and Eq.23 tells us that we can update the error\n\n![fig](2019-02-08-Machinelearningseries2/24.png) \n\nfor each transition among layers. This is how the error propagates backwards along the neural network and where the name comes from. Note that this expression is a slightly different from the formula in Chapter 9.2 of the [machine learning course](https://www.coursera.org/course/ml) by Andrew Ng of Standford. Do you see why is that? \n\n# 3 Training multiple samples\n\nWe have derived mathematically how to train one sample on a neural network. How about multiple samples? Easy. Because all the weight matrix of the neural network are shared for all samples, we can update the weight matrix by a fraction of error of each sample. Normally this fraction is \\\\(\\frac{1}{\\text{sample size}}\\\\) where in our example the sample size is \\\\(n\\\\)\n\n![fig](2019-02-08-Machinelearningseries2/25.png) \n\nThen, the weight matrix is tuned to minimize the error of samples. \n\n# 4 Program a neural network \n\nTill now, we have derived a general formula Eq.23 to allow us to update all weight matrix. After updating, the loss function is applied again to examine if the error is sufficiently small. If not, update the weight matrix again till meeting our criterion. Understanding the math behind is a huge step towards the expert level but not the final one. Whether you can equip it via code is essential. Here I attached my code in Python from my perspective as a reference. The user can add any number of hidden layers and deploy any number of neurons there. It is a bit like a minimalistic version of tensorflow. \n\n```Python\nimport numpy as np\nimport pandas as pd\n\nclass neuralnetwork:\n    # initialize parameters\n    def __init__(self,num_sample,num_hidden_layer_units,num_input_feature,num_output_feature,bias,learningrate):\n        self.learningrate=learningrate # learning rate\n        self.num_sample = num_sample  # number of the samples\n        self.num_hidden_layer_units = num_hidden_layer_units # a list indicating the number of hidden layers and how many neurons for each layer\n        self.num_hidden_layer = len(num_hidden_layer_units) # the number of the hidden layers\n        self.weight_layer = []  # initialize the weight matrix\n        self.bias = bias   # bias for the input layer and for the hidden layers\n        assert len(self.bias) == self.num_hidden_layer+1, \"The length of the biases should equal the length of the hidden layers plus 1!!!\"\n        self.units = [num_input_feature] + num_hidden_layer_units + [num_output_feature]\n        # randomly initialize the weight matrix\n        for num_layer in range(0,len(self.units)-1):\n            temp_bias = np.zeros((1,self.units[num_layer+1]))\n            temp_bias.fill(self.bias[num_layer])\n            self.weight_layer.append(np.concatenate((np.random.randn(self.units[num_layer+1],\n                                                self.units[num_layer]),temp_bias.T),axis = 1))\n\n    # the activation function: sigmoid\n    # could be replaced by whatever you want\n    def sigmoid(self,x):\n        y = 1/(1+np.exp(-x))\n        return y\n\n    # train function requires the training data, accuracy and iteration limit\n    def train(self, input, output,accuracy, iteration_limit):\n        go_on = True\n        i = 0 # iteration indicator\n        error = []  # error list recoding errors for all iterations\n        iteration = []\n        while go_on:\n            iteration.append(i)\n            error_acc = 0   # initialize error for every iteration\n            stderror = []   # standard error\n            sample_Z = []   # Z value of neurons for every sample\n            sample_a = []   # sigmoid value of Z for every sample\n            # loop all samples\n            for sample_iter in range(self.num_sample):\n                Zlayer = []\n                a = []\n                a.append(input[sample_iter])    # place the input as the first a value\n                # forward computing Z and a values for all layers\n                for forward_layer in range(len(self.units)-1):\n                    Zlayer.append(np.matmul(self.weight_layer[forward_layer], np.concatenate((a[forward_layer],[1]))))\n                    a.append(self.sigmoid(Zlayer[forward_layer]))\n                # standard error by computing the distance between output layer and true output\n                stderror.append(a[len(a)-1]-output[sample_iter])\n                # loss function defined as the sum of the least square\n                # can be replaced by other functions like cross-entropy\n                error_acc += (1/2*np.sum((stderror[sample_iter])**2))\n                sample_a.append(a)  # store a values for updating\n                sample_Z.append(Zlayer) # store Z values for updating\n            error.append(error_acc) # store error for this iteration\n\n            # backward propagate errors to update the weight matrix\n            i += 1\n            if error[i-1] <accuracy or i >iteration_limit:\n                go_on = False\n            else:\n                for sample_iter in range(self.num_sample):\n                    delta = stderror[sample_iter]  # delta: standard error defined as the derivative of the loss function\n                    # initialize g'(z): the derivative of the activation function at the output layer\n                    deriv_sigmoid =  sample_a[sample_iter][len(sample_a[sample_iter])-1] *\\\n                                 (1 - sample_a[sample_iter][len(sample_a[sample_iter])-1])\n                    # backward propagation\n                    for backward_layer in list(reversed(range(len(self.units)-1))):\n                        temp_weight = self.weight_layer[backward_layer] # store the temporary kth matrix\n                        # update the kth matrix\n                        self.weight_layer[backward_layer][:,:-1] += - self.learningrate/self.num_sample *\\\n                            np.outer(delta * deriv_sigmoid, sample_a[sample_iter][backward_layer])\n                        # update delta and derivative of the activation function\n                        delta = np.dot(temp_weight[:,:-1].T,delta * deriv_sigmoid)\n                        deriv_sigmoid = sample_a[sample_iter][backward_layer] *\\\n                                 (1 - sample_a[sample_iter][backward_layer])\n        self.learningspeed = {'error': error, 'iteration': iteration}\n        self.lsdf = pd.DataFrame(self.learningspeed)\n        return sample_a, self.weight_layer\n\n    # predict\n    def predict(self,input,weight):\n        out = input+[1]\n        for i in range(len(weight)):\n            out = np.dot(weight[i],np.array(out))\n            out = self.sigmoid(out)\n            out = np.concatenate((out,[1]))\n        return out[:-1]\n\n\n# test\ninputx = np.array(([0.2, 0.5, 0.5,0.3], [0.1, 0.2, 0.15,0.1],[0.7,0.9,0.4,0.8]))\noutputy = np.array(([0.5, 0.4], [0.9, 0.1],[0.3,0.5]))\n\n# build the neural network\nnn=neuralnetwork(num_sample=inputx.shape[0],num_hidden_layer_units=[15,14,15,16],num_input_feature=inputx.shape[1],\n             num_output_feature=outputy.shape[1], bias=[0.1,0.2,0.3,0.4,0.4],learningrate=0.2)\n\n# set the accuracy and the iteration limit\nacc = 1e-7\niter = 20000\n# train the neural network\nout_a,weight = nn.train(inputx,outputy,accuracy = acc,iteration_limit=iter)\n# plot the learning process\nnn.lsdf.plot(x='iteration',y='error')\n\n# predict\ninput = [0.19, 0.51, 0.49,0.29]\nout = nn.predict(input,weight)\n\n```\n\nYou can also clone it [on my Github](https://github.com/xl0418/Tensorflow/blob/master/NeuralNetwork.py) \n\n# The end\n\nMy code is obviously not the most efficient one as a lot of loops are used. But in another sense it is easy to read for a starter. As you see in the derivation and in the code, there are a huge amount of independent computing that can be parallelized to speed up. So the future plan is to parallelize the code on GPU which may substantially improve the efficiency of the neural network. It would also be a good practice for you to step into the machine learning field. \n\n# Reference\n\n- An introduction to the math used in Machine Learning:  [The Matrix Calculus You Need For Deep Learning\n](https://explained.ai/matrix-calculus/index.html).\n\n- A concrete derivation of backward propagation for a two-layer neural network in Chinese [here](http://www.cnblogs.com/charlotte77/p/5629865.html#!comments).","slug":"2019-02-08-Machinelearningseries2","published":1,"date":"2020-10-05T11:30:58.241Z","updated":"2020-10-05T11:30:58.242Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy1380013nj399avzbwoy","content":"<p>There are tons of documents out there to explain how neural network works in different angles. So I guess people don’t mind me adding one more from my perspective. Hopefully, someone may get inspired from this post.</p>\n<a id=\"more\"></a>\n<p>In short, the neural network is a kind of system that transforms the input data into its corresponding output (or labels), strictly speaking, for a supervised learning. This system is consisting of three types of layers, i.e. the input layer, the hidden layer and the output layer. Normally, the hidden layer may have multiple layers according to one’s design. Among layers, the data from the upper layer are transformed to the data in the lower layer via a linear combination with an initialized weight matrix. After that, a nonlinear transformation that is generally called the activation function is applied to the data that would be converted to a form for the next round until reaching the output layer. To assure the neural network feedback the correct output, people need to train the neural network by adjusting the weight matrix. They are usually adjusted via minimizing the error between the output from the final layer and the known output from the data used for training. </p>\n<p>Without loss of generality, here I consider a simplest neural network which only possesses fully connected layers, meaning that every neuron in each layer connects all neurons of the upper and lower layers. The mathematical logic behind is analogous to other structures of neural network. Firstly, I used a series of graphs to illustrate how the data flows to the final layer and introduce the notations used in the derivation. Then, a general formulation is present subsequently.</p>\n<h1 id=\"A-6-layer-neural-network\"><a href=\"#A-6-layer-neural-network\" class=\"headerlink\" title=\"A 6-layer neural network\"></a>A 6-layer neural network</h1><p>The following four graphs illustrate how the data evolves along the neural network and the notations used in the derivation.</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/step1.png\" alt=\"fig\">    </p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/step2.png\" alt=\"fig\">    </p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/step3.png\" alt=\"fig\">    </p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/step4.png\" alt=\"fig\">   </p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/step5.png\" alt=\"fig\"> </p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/step8.png\" alt=\"fig\">         </p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/step7.png\" alt=\"fig\">     </p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/step6.png\" alt=\"fig\">     </p>\n<h1 id=\"1-Forward-flow-of-the-neural-network\"><a href=\"#1-Forward-flow-of-the-neural-network\" class=\"headerlink\" title=\"1 Forward flow of the neural network\"></a>1 Forward flow of the neural network</h1><p>Consider a neural network with \\(k+1\\) layers including the input layer and the output layer. The input data is consisting of \\(n\\) samples with \\(L_{1}\\) features</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/1.png\" alt=\"fig\">   </p>\n<p>The output label is given accordingly</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/2.png\" alt=\"fig\">   </p>\n<p>where the label on the top left denotes the index of the sample. Too abstract? Imagine that sample \\(^{1}\\boldsymbol{x}\\) corresponds to model 1 while sample \\(^{2}\\boldsymbol{x}\\) corresponds to model 2. Then the output can be either an array of two vectors indicating the probabilities of model 1 and model 2, for example, \\({(1,0),(0,1)}\\), or a vector of two elements \\({0,1}\\) where 0 indicates model 1 and 1 indicates model 2 or whatever you label them. The purpose is to use a huge amount of data sets to train the neural network, more precisely to compute the weight matrix among the adjacent pair of layers, to fit the output layer to the output vectors. Will see it later on.</p>\n<p>Now, let’s feed one sample (the first one \\(^{1}\\boldsymbol{x}\\)) to the neural network to see how to compute the output layer. Later, we will see how to train the network with multiple samples. Assume the second layer has \\(L_{2}\\) neurons</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/3.png\" alt=\"fig\">   </p>\n<p>Note that the first layer is the input layer in which we feed one sample with \\(L_{1}\\) feature to the neural network at first. Then the weight matrix from the first layer to the second layer is defined as </p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/4.png\" alt=\"fig\">   </p>\n<p>Thus, multiplying the weight matrix Eq.4 with the neurons of the upper layer Eq.3 yields</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/5.png\" alt=\"fig\">   </p>\n<p>More generally, a bias term (constant term) is incorporated as follows</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/6.png\" alt=\"fig\"> </p>\n<p>which can also be written in the form if we absorb the constant vector \\(\\boldsymbol{b}_{1}\\) into the weight matrix  </p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/78.png\" alt=\"fig\"> </p>\n<p>After the transformation, an activation function is applied to \\(\\boldsymbol{z}^{(2)}\\) elementwisely. For a model classification problem, the sigmoid function and the hyperbolic tangent function are widely used. Here we use the sigmoid function for instance</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/9.png\" alt=\"fig\"> </p>\n<p>Till now, the transformation from the first layer (the input layer) to the second layer (the first hidden layer) is done. This process can be generalized as .</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/9-1.png\" alt=\"fig\"> </p>\n<p>where \\(\\boldsymbol{a}’\\) is the vector \\(\\boldsymbol{a}\\) absorbing 1 at the end as what I did in Eq.8. Note that \\(\\boldsymbol{a}^{(1)}=^{1}\\boldsymbol{x}\\).</p>\n<p>Finally, the neural network will return \\(\\boldsymbol{a}^{(k+1)}\\) with \\(L_{k+1}\\) elements from the output layer. Given the corresponding output label \\(\\boldsymbol{y}^{(1)}\\), we can compute the error between the feedback \\(\\boldsymbol{a}^{(k+1)}\\) and the output label \\(\\boldsymbol{y}^{(1)}\\). This is normally called the loss of the result to the output. There are several candidate loss functions for model classification like the least square, the cross-entropy function. We take the least square function as the example</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/10.png\" alt=\"fig\"> </p>\n<p>So far, we have computed out the loss of the neural network with a bunch of randomly initialized weight matrix. No doubt, the loss would be huge. Our aim is to minimized the loss \\(J\\) by tuning the weight matrix. How? Remember your advanced calculus in the high school or the university? \\(J\\) can be envisaged as a function of every entry in the weight matrix that we want to adjust. Thus, the derivative of \\(J\\) with respect to each entries of the weight matrix would tell us how to tune the weight matrix to minimize the loss. This method is called gradient descend. And the loss can also propagate backwards to determine the gradient of the entries of the weight matrix at each layer. The full process to tune the weight matrix is also called Backward Propagation. </p>\n<h1 id=\"2-Backward-propagation\"><a href=\"#2-Backward-propagation\" class=\"headerlink\" title=\"2 Backward propagation\"></a>2 Backward propagation</h1><h2 id=\"From-the-k-1-th-layer-to-the-k-th-layer\"><a href=\"#From-the-k-1-th-layer-to-the-k-th-layer\" class=\"headerlink\" title=\"From the \\(k+1\\)th layer to the \\(k\\)th layer\"></a>From the \\(k+1\\)th layer to the \\(k\\)th layer</h2><p>Let us start from the final layer (the \\(k+1\\)th layer) to the previous one (the \\(k\\)th layer). Note that the variable of our concern is the entry of the weight matrix from the \\(k\\)th layer to the \\(k+1\\)th layer, \\(\\theta_{L_{k+1}\\times L_{k}}^{(k)}\\). Here I only consider tuning \\(\\theta\\) instead of \\(\\theta’\\), meaning that the bias terms are not tuned. Updating the bias terms is similar and you can practice it afterward. The derivative of the loss function with respect to the matrix is defined as </p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/11.png\" alt=\"fig\"> </p>\n<p>At the mean time, from the loss function Eq.10, we obtain the derivative according to the chain rule</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/12.png\" alt=\"fig\"> </p>\n<p>where</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/13-15.png\" alt=\"fig\"> </p>\n<p>and</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/16.png\" alt=\"fig\"> </p>\n<p>where \\(\\otimes\\) is the outer product. Simplifying the gradient Eq.12 yields</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/17.png\" alt=\"fig\"> </p>\n<p>If we define</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/18.png\" alt=\"fig\"> </p>\n<p>The gradient Eq.17 can be further simplified as</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/19.png\" alt=\"fig\"> </p>\n<p>Note that we define the outer product of two column vectors is the element-wise produce of the corresponding elements</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/19-1.png\" alt=\"fig\"> </p>\n<p>Now, as \\(\\boldsymbol{z}^{(k+1)}\\), \\(\\boldsymbol{a}^{(k+1)}\\) and \\(^{1}\\boldsymbol{y}\\) are known, we can update the weight matrix \\(\\theta^{(k)}\\) by</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/20.png\" alt=\"fig\"> </p>\n<p>where \\(\\lambda\\) is a constant called the learning rate given in advance. </p>\n<h2 id=\"From-the-k-th-layer-to-the-k-1-th-layer\"><a href=\"#From-the-k-th-layer-to-the-k-1-th-layer\" class=\"headerlink\" title=\"From the \\(k\\)th layer to the \\(k-1\\)th layer\"></a>From the \\(k\\)th layer to the \\(k-1\\)th layer</h2><p>Let’s do the calculation one more time from the \\(k\\)th layer to the \\(k-1\\)th layer. At the end of this section, we will get a general formula to update all weight matrix via which we can develop the algorithm for a deep neural network. </p>\n<p>Now we consider one more previous weight matrix \\(\\theta^{(k-1)}\\). The derivative of the loss function with respect to that matrix yields </p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/21.png\" alt=\"fig\"> </p>\n<p>The first two terms on the right hand side are the same as Eq.12. The third term \\(\\frac{\\partial\\boldsymbol{z}^{(k+1)}}{\\partial\\boldsymbol{a}^{(k)}}\\) produces the weight matrix from the \\(k+1\\)th layer to the \\(k\\)th layer</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/22.png\" alt=\"fig\"> </p>\n<p>The last two terms on the right hand side are similar to what we have done above. Finally, we get</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/23.png\" alt=\"fig\"> </p>\n<p>Comparing Eq.19 and Eq.23 tells us that we can update the error</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/24.png\" alt=\"fig\"> </p>\n<p>for each transition among layers. This is how the error propagates backwards along the neural network and where the name comes from. Note that this expression is a slightly different from the formula in Chapter 9.2 of the <a href=\"https://www.coursera.org/course/ml\" target=\"_blank\" rel=\"noopener\">machine learning course</a> by Andrew Ng of Standford. Do you see why is that? </p>\n<h1 id=\"3-Training-multiple-samples\"><a href=\"#3-Training-multiple-samples\" class=\"headerlink\" title=\"3 Training multiple samples\"></a>3 Training multiple samples</h1><p>We have derived mathematically how to train one sample on a neural network. How about multiple samples? Easy. Because all the weight matrix of the neural network are shared for all samples, we can update the weight matrix by a fraction of error of each sample. Normally this fraction is \\(\\frac{1}{\\text{sample size}}\\) where in our example the sample size is \\(n\\)</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/25.png\" alt=\"fig\"> </p>\n<p>Then, the weight matrix is tuned to minimize the error of samples. </p>\n<h1 id=\"4-Program-a-neural-network\"><a href=\"#4-Program-a-neural-network\" class=\"headerlink\" title=\"4 Program a neural network\"></a>4 Program a neural network</h1><p>Till now, we have derived a general formula Eq.23 to allow us to update all weight matrix. After updating, the loss function is applied again to examine if the error is sufficiently small. If not, update the weight matrix again till meeting our criterion. Understanding the math behind is a huge step towards the expert level but not the final one. Whether you can equip it via code is essential. Here I attached my code in Python from my perspective as a reference. The user can add any number of hidden layers and deploy any number of neurons there. It is a bit like a minimalistic version of tensorflow. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">neuralnetwork</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># initialize parameters</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self,num_sample,num_hidden_layer_units,num_input_feature,num_output_feature,bias,learningrate)</span>:</span></span><br><span class=\"line\">        self.learningrate=learningrate <span class=\"comment\"># learning rate</span></span><br><span class=\"line\">        self.num_sample = num_sample  <span class=\"comment\"># number of the samples</span></span><br><span class=\"line\">        self.num_hidden_layer_units = num_hidden_layer_units <span class=\"comment\"># a list indicating the number of hidden layers and how many neurons for each layer</span></span><br><span class=\"line\">        self.num_hidden_layer = len(num_hidden_layer_units) <span class=\"comment\"># the number of the hidden layers</span></span><br><span class=\"line\">        self.weight_layer = []  <span class=\"comment\"># initialize the weight matrix</span></span><br><span class=\"line\">        self.bias = bias   <span class=\"comment\"># bias for the input layer and for the hidden layers</span></span><br><span class=\"line\">        <span class=\"keyword\">assert</span> len(self.bias) == self.num_hidden_layer+<span class=\"number\">1</span>, <span class=\"string\">\"The length of the biases should equal the length of the hidden layers plus 1!!!\"</span></span><br><span class=\"line\">        self.units = [num_input_feature] + num_hidden_layer_units + [num_output_feature]</span><br><span class=\"line\">        <span class=\"comment\"># randomly initialize the weight matrix</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> num_layer <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(self.units)<span class=\"number\">-1</span>):</span><br><span class=\"line\">            temp_bias = np.zeros((<span class=\"number\">1</span>,self.units[num_layer+<span class=\"number\">1</span>]))</span><br><span class=\"line\">            temp_bias.fill(self.bias[num_layer])</span><br><span class=\"line\">            self.weight_layer.append(np.concatenate((np.random.randn(self.units[num_layer+<span class=\"number\">1</span>],</span><br><span class=\"line\">                                                self.units[num_layer]),temp_bias.T),axis = <span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># the activation function: sigmoid</span></span><br><span class=\"line\">    <span class=\"comment\"># could be replaced by whatever you want</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sigmoid</span><span class=\"params\">(self,x)</span>:</span></span><br><span class=\"line\">        y = <span class=\"number\">1</span>/(<span class=\"number\">1</span>+np.exp(-x))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> y</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># train function requires the training data, accuracy and iteration limit</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span><span class=\"params\">(self, input, output,accuracy, iteration_limit)</span>:</span></span><br><span class=\"line\">        go_on = <span class=\"keyword\">True</span></span><br><span class=\"line\">        i = <span class=\"number\">0</span> <span class=\"comment\"># iteration indicator</span></span><br><span class=\"line\">        error = []  <span class=\"comment\"># error list recoding errors for all iterations</span></span><br><span class=\"line\">        iteration = []</span><br><span class=\"line\">        <span class=\"keyword\">while</span> go_on:</span><br><span class=\"line\">            iteration.append(i)</span><br><span class=\"line\">            error_acc = <span class=\"number\">0</span>   <span class=\"comment\"># initialize error for every iteration</span></span><br><span class=\"line\">            stderror = []   <span class=\"comment\"># standard error</span></span><br><span class=\"line\">            sample_Z = []   <span class=\"comment\"># Z value of neurons for every sample</span></span><br><span class=\"line\">            sample_a = []   <span class=\"comment\"># sigmoid value of Z for every sample</span></span><br><span class=\"line\">            <span class=\"comment\"># loop all samples</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> sample_iter <span class=\"keyword\">in</span> range(self.num_sample):</span><br><span class=\"line\">                Zlayer = []</span><br><span class=\"line\">                a = []</span><br><span class=\"line\">                a.append(input[sample_iter])    <span class=\"comment\"># place the input as the first a value</span></span><br><span class=\"line\">                <span class=\"comment\"># forward computing Z and a values for all layers</span></span><br><span class=\"line\">                <span class=\"keyword\">for</span> forward_layer <span class=\"keyword\">in</span> range(len(self.units)<span class=\"number\">-1</span>):</span><br><span class=\"line\">                    Zlayer.append(np.matmul(self.weight_layer[forward_layer], np.concatenate((a[forward_layer],[<span class=\"number\">1</span>]))))</span><br><span class=\"line\">                    a.append(self.sigmoid(Zlayer[forward_layer]))</span><br><span class=\"line\">                <span class=\"comment\"># standard error by computing the distance between output layer and true output</span></span><br><span class=\"line\">                stderror.append(a[len(a)<span class=\"number\">-1</span>]-output[sample_iter])</span><br><span class=\"line\">                <span class=\"comment\"># loss function defined as the sum of the least square</span></span><br><span class=\"line\">                <span class=\"comment\"># can be replaced by other functions like cross-entropy</span></span><br><span class=\"line\">                error_acc += (<span class=\"number\">1</span>/<span class=\"number\">2</span>*np.sum((stderror[sample_iter])**<span class=\"number\">2</span>))</span><br><span class=\"line\">                sample_a.append(a)  <span class=\"comment\"># store a values for updating</span></span><br><span class=\"line\">                sample_Z.append(Zlayer) <span class=\"comment\"># store Z values for updating</span></span><br><span class=\"line\">            error.append(error_acc) <span class=\"comment\"># store error for this iteration</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># backward propagate errors to update the weight matrix</span></span><br><span class=\"line\">            i += <span class=\"number\">1</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> error[i<span class=\"number\">-1</span>] &lt;accuracy <span class=\"keyword\">or</span> i &gt;iteration_limit:</span><br><span class=\"line\">                go_on = <span class=\"keyword\">False</span></span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                <span class=\"keyword\">for</span> sample_iter <span class=\"keyword\">in</span> range(self.num_sample):</span><br><span class=\"line\">                    delta = stderror[sample_iter]  <span class=\"comment\"># delta: standard error defined as the derivative of the loss function</span></span><br><span class=\"line\">                    <span class=\"comment\"># initialize g'(z): the derivative of the activation function at the output layer</span></span><br><span class=\"line\">                    deriv_sigmoid =  sample_a[sample_iter][len(sample_a[sample_iter])<span class=\"number\">-1</span>] *\\</span><br><span class=\"line\">                                 (<span class=\"number\">1</span> - sample_a[sample_iter][len(sample_a[sample_iter])<span class=\"number\">-1</span>])</span><br><span class=\"line\">                    <span class=\"comment\"># backward propagation</span></span><br><span class=\"line\">                    <span class=\"keyword\">for</span> backward_layer <span class=\"keyword\">in</span> list(reversed(range(len(self.units)<span class=\"number\">-1</span>))):</span><br><span class=\"line\">                        temp_weight = self.weight_layer[backward_layer] <span class=\"comment\"># store the temporary kth matrix</span></span><br><span class=\"line\">                        <span class=\"comment\"># update the kth matrix</span></span><br><span class=\"line\">                        self.weight_layer[backward_layer][:,:<span class=\"number\">-1</span>] += - self.learningrate/self.num_sample *\\</span><br><span class=\"line\">                            np.outer(delta * deriv_sigmoid, sample_a[sample_iter][backward_layer])</span><br><span class=\"line\">                        <span class=\"comment\"># update delta and derivative of the activation function</span></span><br><span class=\"line\">                        delta = np.dot(temp_weight[:,:<span class=\"number\">-1</span>].T,delta * deriv_sigmoid)</span><br><span class=\"line\">                        deriv_sigmoid = sample_a[sample_iter][backward_layer] *\\</span><br><span class=\"line\">                                 (<span class=\"number\">1</span> - sample_a[sample_iter][backward_layer])</span><br><span class=\"line\">        self.learningspeed = &#123;<span class=\"string\">'error'</span>: error, <span class=\"string\">'iteration'</span>: iteration&#125;</span><br><span class=\"line\">        self.lsdf = pd.DataFrame(self.learningspeed)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> sample_a, self.weight_layer</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># predict</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(self,input,weight)</span>:</span></span><br><span class=\"line\">        out = input+[<span class=\"number\">1</span>]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(weight)):</span><br><span class=\"line\">            out = np.dot(weight[i],np.array(out))</span><br><span class=\"line\">            out = self.sigmoid(out)</span><br><span class=\"line\">            out = np.concatenate((out,[<span class=\"number\">1</span>]))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out[:<span class=\"number\">-1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># test</span></span><br><span class=\"line\">inputx = np.array(([<span class=\"number\">0.2</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>,<span class=\"number\">0.3</span>], [<span class=\"number\">0.1</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.15</span>,<span class=\"number\">0.1</span>],[<span class=\"number\">0.7</span>,<span class=\"number\">0.9</span>,<span class=\"number\">0.4</span>,<span class=\"number\">0.8</span>]))</span><br><span class=\"line\">outputy = np.array(([<span class=\"number\">0.5</span>, <span class=\"number\">0.4</span>], [<span class=\"number\">0.9</span>, <span class=\"number\">0.1</span>],[<span class=\"number\">0.3</span>,<span class=\"number\">0.5</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># build the neural network</span></span><br><span class=\"line\">nn=neuralnetwork(num_sample=inputx.shape[<span class=\"number\">0</span>],num_hidden_layer_units=[<span class=\"number\">15</span>,<span class=\"number\">14</span>,<span class=\"number\">15</span>,<span class=\"number\">16</span>],num_input_feature=inputx.shape[<span class=\"number\">1</span>],</span><br><span class=\"line\">             num_output_feature=outputy.shape[<span class=\"number\">1</span>], bias=[<span class=\"number\">0.1</span>,<span class=\"number\">0.2</span>,<span class=\"number\">0.3</span>,<span class=\"number\">0.4</span>,<span class=\"number\">0.4</span>],learningrate=<span class=\"number\">0.2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># set the accuracy and the iteration limit</span></span><br><span class=\"line\">acc = <span class=\"number\">1e-7</span></span><br><span class=\"line\">iter = <span class=\"number\">20000</span></span><br><span class=\"line\"><span class=\"comment\"># train the neural network</span></span><br><span class=\"line\">out_a,weight = nn.train(inputx,outputy,accuracy = acc,iteration_limit=iter)</span><br><span class=\"line\"><span class=\"comment\"># plot the learning process</span></span><br><span class=\"line\">nn.lsdf.plot(x=<span class=\"string\">'iteration'</span>,y=<span class=\"string\">'error'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># predict</span></span><br><span class=\"line\">input = [<span class=\"number\">0.19</span>, <span class=\"number\">0.51</span>, <span class=\"number\">0.49</span>,<span class=\"number\">0.29</span>]</span><br><span class=\"line\">out = nn.predict(input,weight)</span><br></pre></td></tr></table></figure>\n<p>You can also clone it <a href=\"https://github.com/xl0418/Tensorflow/blob/master/NeuralNetwork.py\" target=\"_blank\" rel=\"noopener\">on my Github</a> </p>\n<h1 id=\"The-end\"><a href=\"#The-end\" class=\"headerlink\" title=\"The end\"></a>The end</h1><p>My code is obviously not the most efficient one as a lot of loops are used. But in another sense it is easy to read for a starter. As you see in the derivation and in the code, there are a huge amount of independent computing that can be parallelized to speed up. So the future plan is to parallelize the code on GPU which may substantially improve the efficiency of the neural network. It would also be a good practice for you to step into the machine learning field. </p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><ul>\n<li><p>An introduction to the math used in Machine Learning:  <a href=\"https://explained.ai/matrix-calculus/index.html\" target=\"_blank\" rel=\"noopener\">The Matrix Calculus You Need For Deep Learning\n</a>.</p>\n</li>\n<li><p>A concrete derivation of backward propagation for a two-layer neural network in Chinese <a href=\"http://www.cnblogs.com/charlotte77/p/5629865.html#!comments\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>There are tons of documents out there to explain how neural network works in different angles. So I guess people don’t mind me adding one more from my perspective. Hopefully, someone may get inspired from this post.</p>","more":"<p>In short, the neural network is a kind of system that transforms the input data into its corresponding output (or labels), strictly speaking, for a supervised learning. This system is consisting of three types of layers, i.e. the input layer, the hidden layer and the output layer. Normally, the hidden layer may have multiple layers according to one’s design. Among layers, the data from the upper layer are transformed to the data in the lower layer via a linear combination with an initialized weight matrix. After that, a nonlinear transformation that is generally called the activation function is applied to the data that would be converted to a form for the next round until reaching the output layer. To assure the neural network feedback the correct output, people need to train the neural network by adjusting the weight matrix. They are usually adjusted via minimizing the error between the output from the final layer and the known output from the data used for training. </p>\n<p>Without loss of generality, here I consider a simplest neural network which only possesses fully connected layers, meaning that every neuron in each layer connects all neurons of the upper and lower layers. The mathematical logic behind is analogous to other structures of neural network. Firstly, I used a series of graphs to illustrate how the data flows to the final layer and introduce the notations used in the derivation. Then, a general formulation is present subsequently.</p>\n<h1 id=\"A-6-layer-neural-network\"><a href=\"#A-6-layer-neural-network\" class=\"headerlink\" title=\"A 6-layer neural network\"></a>A 6-layer neural network</h1><p>The following four graphs illustrate how the data evolves along the neural network and the notations used in the derivation.</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/step1.png\" alt=\"fig\">    </p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/step2.png\" alt=\"fig\">    </p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/step3.png\" alt=\"fig\">    </p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/step4.png\" alt=\"fig\">   </p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/step5.png\" alt=\"fig\"> </p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/step8.png\" alt=\"fig\">         </p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/step7.png\" alt=\"fig\">     </p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/step6.png\" alt=\"fig\">     </p>\n<h1 id=\"1-Forward-flow-of-the-neural-network\"><a href=\"#1-Forward-flow-of-the-neural-network\" class=\"headerlink\" title=\"1 Forward flow of the neural network\"></a>1 Forward flow of the neural network</h1><p>Consider a neural network with \\(k+1\\) layers including the input layer and the output layer. The input data is consisting of \\(n\\) samples with \\(L_{1}\\) features</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/1.png\" alt=\"fig\">   </p>\n<p>The output label is given accordingly</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/2.png\" alt=\"fig\">   </p>\n<p>where the label on the top left denotes the index of the sample. Too abstract? Imagine that sample \\(^{1}\\boldsymbol{x}\\) corresponds to model 1 while sample \\(^{2}\\boldsymbol{x}\\) corresponds to model 2. Then the output can be either an array of two vectors indicating the probabilities of model 1 and model 2, for example, \\({(1,0),(0,1)}\\), or a vector of two elements \\({0,1}\\) where 0 indicates model 1 and 1 indicates model 2 or whatever you label them. The purpose is to use a huge amount of data sets to train the neural network, more precisely to compute the weight matrix among the adjacent pair of layers, to fit the output layer to the output vectors. Will see it later on.</p>\n<p>Now, let’s feed one sample (the first one \\(^{1}\\boldsymbol{x}\\)) to the neural network to see how to compute the output layer. Later, we will see how to train the network with multiple samples. Assume the second layer has \\(L_{2}\\) neurons</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/3.png\" alt=\"fig\">   </p>\n<p>Note that the first layer is the input layer in which we feed one sample with \\(L_{1}\\) feature to the neural network at first. Then the weight matrix from the first layer to the second layer is defined as </p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/4.png\" alt=\"fig\">   </p>\n<p>Thus, multiplying the weight matrix Eq.4 with the neurons of the upper layer Eq.3 yields</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/5.png\" alt=\"fig\">   </p>\n<p>More generally, a bias term (constant term) is incorporated as follows</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/6.png\" alt=\"fig\"> </p>\n<p>which can also be written in the form if we absorb the constant vector \\(\\boldsymbol{b}_{1}\\) into the weight matrix  </p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/78.png\" alt=\"fig\"> </p>\n<p>After the transformation, an activation function is applied to \\(\\boldsymbol{z}^{(2)}\\) elementwisely. For a model classification problem, the sigmoid function and the hyperbolic tangent function are widely used. Here we use the sigmoid function for instance</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/9.png\" alt=\"fig\"> </p>\n<p>Till now, the transformation from the first layer (the input layer) to the second layer (the first hidden layer) is done. This process can be generalized as .</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/9-1.png\" alt=\"fig\"> </p>\n<p>where \\(\\boldsymbol{a}’\\) is the vector \\(\\boldsymbol{a}\\) absorbing 1 at the end as what I did in Eq.8. Note that \\(\\boldsymbol{a}^{(1)}=^{1}\\boldsymbol{x}\\).</p>\n<p>Finally, the neural network will return \\(\\boldsymbol{a}^{(k+1)}\\) with \\(L_{k+1}\\) elements from the output layer. Given the corresponding output label \\(\\boldsymbol{y}^{(1)}\\), we can compute the error between the feedback \\(\\boldsymbol{a}^{(k+1)}\\) and the output label \\(\\boldsymbol{y}^{(1)}\\). This is normally called the loss of the result to the output. There are several candidate loss functions for model classification like the least square, the cross-entropy function. We take the least square function as the example</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/10.png\" alt=\"fig\"> </p>\n<p>So far, we have computed out the loss of the neural network with a bunch of randomly initialized weight matrix. No doubt, the loss would be huge. Our aim is to minimized the loss \\(J\\) by tuning the weight matrix. How? Remember your advanced calculus in the high school or the university? \\(J\\) can be envisaged as a function of every entry in the weight matrix that we want to adjust. Thus, the derivative of \\(J\\) with respect to each entries of the weight matrix would tell us how to tune the weight matrix to minimize the loss. This method is called gradient descend. And the loss can also propagate backwards to determine the gradient of the entries of the weight matrix at each layer. The full process to tune the weight matrix is also called Backward Propagation. </p>\n<h1 id=\"2-Backward-propagation\"><a href=\"#2-Backward-propagation\" class=\"headerlink\" title=\"2 Backward propagation\"></a>2 Backward propagation</h1><h2 id=\"From-the-k-1-th-layer-to-the-k-th-layer\"><a href=\"#From-the-k-1-th-layer-to-the-k-th-layer\" class=\"headerlink\" title=\"From the \\(k+1\\)th layer to the \\(k\\)th layer\"></a>From the \\(k+1\\)th layer to the \\(k\\)th layer</h2><p>Let us start from the final layer (the \\(k+1\\)th layer) to the previous one (the \\(k\\)th layer). Note that the variable of our concern is the entry of the weight matrix from the \\(k\\)th layer to the \\(k+1\\)th layer, \\(\\theta_{L_{k+1}\\times L_{k}}^{(k)}\\). Here I only consider tuning \\(\\theta\\) instead of \\(\\theta’\\), meaning that the bias terms are not tuned. Updating the bias terms is similar and you can practice it afterward. The derivative of the loss function with respect to the matrix is defined as </p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/11.png\" alt=\"fig\"> </p>\n<p>At the mean time, from the loss function Eq.10, we obtain the derivative according to the chain rule</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/12.png\" alt=\"fig\"> </p>\n<p>where</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/13-15.png\" alt=\"fig\"> </p>\n<p>and</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/16.png\" alt=\"fig\"> </p>\n<p>where \\(\\otimes\\) is the outer product. Simplifying the gradient Eq.12 yields</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/17.png\" alt=\"fig\"> </p>\n<p>If we define</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/18.png\" alt=\"fig\"> </p>\n<p>The gradient Eq.17 can be further simplified as</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/19.png\" alt=\"fig\"> </p>\n<p>Note that we define the outer product of two column vectors is the element-wise produce of the corresponding elements</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/19-1.png\" alt=\"fig\"> </p>\n<p>Now, as \\(\\boldsymbol{z}^{(k+1)}\\), \\(\\boldsymbol{a}^{(k+1)}\\) and \\(^{1}\\boldsymbol{y}\\) are known, we can update the weight matrix \\(\\theta^{(k)}\\) by</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/20.png\" alt=\"fig\"> </p>\n<p>where \\(\\lambda\\) is a constant called the learning rate given in advance. </p>\n<h2 id=\"From-the-k-th-layer-to-the-k-1-th-layer\"><a href=\"#From-the-k-th-layer-to-the-k-1-th-layer\" class=\"headerlink\" title=\"From the \\(k\\)th layer to the \\(k-1\\)th layer\"></a>From the \\(k\\)th layer to the \\(k-1\\)th layer</h2><p>Let’s do the calculation one more time from the \\(k\\)th layer to the \\(k-1\\)th layer. At the end of this section, we will get a general formula to update all weight matrix via which we can develop the algorithm for a deep neural network. </p>\n<p>Now we consider one more previous weight matrix \\(\\theta^{(k-1)}\\). The derivative of the loss function with respect to that matrix yields </p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/21.png\" alt=\"fig\"> </p>\n<p>The first two terms on the right hand side are the same as Eq.12. The third term \\(\\frac{\\partial\\boldsymbol{z}^{(k+1)}}{\\partial\\boldsymbol{a}^{(k)}}\\) produces the weight matrix from the \\(k+1\\)th layer to the \\(k\\)th layer</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/22.png\" alt=\"fig\"> </p>\n<p>The last two terms on the right hand side are similar to what we have done above. Finally, we get</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/23.png\" alt=\"fig\"> </p>\n<p>Comparing Eq.19 and Eq.23 tells us that we can update the error</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/24.png\" alt=\"fig\"> </p>\n<p>for each transition among layers. This is how the error propagates backwards along the neural network and where the name comes from. Note that this expression is a slightly different from the formula in Chapter 9.2 of the <a href=\"https://www.coursera.org/course/ml\" target=\"_blank\" rel=\"noopener\">machine learning course</a> by Andrew Ng of Standford. Do you see why is that? </p>\n<h1 id=\"3-Training-multiple-samples\"><a href=\"#3-Training-multiple-samples\" class=\"headerlink\" title=\"3 Training multiple samples\"></a>3 Training multiple samples</h1><p>We have derived mathematically how to train one sample on a neural network. How about multiple samples? Easy. Because all the weight matrix of the neural network are shared for all samples, we can update the weight matrix by a fraction of error of each sample. Normally this fraction is \\(\\frac{1}{\\text{sample size}}\\) where in our example the sample size is \\(n\\)</p>\n<p><img src=\"/2020/10/05/2019-02-08-Machinelearningseries2/25.png\" alt=\"fig\"> </p>\n<p>Then, the weight matrix is tuned to minimize the error of samples. </p>\n<h1 id=\"4-Program-a-neural-network\"><a href=\"#4-Program-a-neural-network\" class=\"headerlink\" title=\"4 Program a neural network\"></a>4 Program a neural network</h1><p>Till now, we have derived a general formula Eq.23 to allow us to update all weight matrix. After updating, the loss function is applied again to examine if the error is sufficiently small. If not, update the weight matrix again till meeting our criterion. Understanding the math behind is a huge step towards the expert level but not the final one. Whether you can equip it via code is essential. Here I attached my code in Python from my perspective as a reference. The user can add any number of hidden layers and deploy any number of neurons there. It is a bit like a minimalistic version of tensorflow. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">neuralnetwork</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># initialize parameters</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self,num_sample,num_hidden_layer_units,num_input_feature,num_output_feature,bias,learningrate)</span>:</span></span><br><span class=\"line\">        self.learningrate=learningrate <span class=\"comment\"># learning rate</span></span><br><span class=\"line\">        self.num_sample = num_sample  <span class=\"comment\"># number of the samples</span></span><br><span class=\"line\">        self.num_hidden_layer_units = num_hidden_layer_units <span class=\"comment\"># a list indicating the number of hidden layers and how many neurons for each layer</span></span><br><span class=\"line\">        self.num_hidden_layer = len(num_hidden_layer_units) <span class=\"comment\"># the number of the hidden layers</span></span><br><span class=\"line\">        self.weight_layer = []  <span class=\"comment\"># initialize the weight matrix</span></span><br><span class=\"line\">        self.bias = bias   <span class=\"comment\"># bias for the input layer and for the hidden layers</span></span><br><span class=\"line\">        <span class=\"keyword\">assert</span> len(self.bias) == self.num_hidden_layer+<span class=\"number\">1</span>, <span class=\"string\">\"The length of the biases should equal the length of the hidden layers plus 1!!!\"</span></span><br><span class=\"line\">        self.units = [num_input_feature] + num_hidden_layer_units + [num_output_feature]</span><br><span class=\"line\">        <span class=\"comment\"># randomly initialize the weight matrix</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> num_layer <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(self.units)<span class=\"number\">-1</span>):</span><br><span class=\"line\">            temp_bias = np.zeros((<span class=\"number\">1</span>,self.units[num_layer+<span class=\"number\">1</span>]))</span><br><span class=\"line\">            temp_bias.fill(self.bias[num_layer])</span><br><span class=\"line\">            self.weight_layer.append(np.concatenate((np.random.randn(self.units[num_layer+<span class=\"number\">1</span>],</span><br><span class=\"line\">                                                self.units[num_layer]),temp_bias.T),axis = <span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># the activation function: sigmoid</span></span><br><span class=\"line\">    <span class=\"comment\"># could be replaced by whatever you want</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sigmoid</span><span class=\"params\">(self,x)</span>:</span></span><br><span class=\"line\">        y = <span class=\"number\">1</span>/(<span class=\"number\">1</span>+np.exp(-x))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> y</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># train function requires the training data, accuracy and iteration limit</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span><span class=\"params\">(self, input, output,accuracy, iteration_limit)</span>:</span></span><br><span class=\"line\">        go_on = <span class=\"keyword\">True</span></span><br><span class=\"line\">        i = <span class=\"number\">0</span> <span class=\"comment\"># iteration indicator</span></span><br><span class=\"line\">        error = []  <span class=\"comment\"># error list recoding errors for all iterations</span></span><br><span class=\"line\">        iteration = []</span><br><span class=\"line\">        <span class=\"keyword\">while</span> go_on:</span><br><span class=\"line\">            iteration.append(i)</span><br><span class=\"line\">            error_acc = <span class=\"number\">0</span>   <span class=\"comment\"># initialize error for every iteration</span></span><br><span class=\"line\">            stderror = []   <span class=\"comment\"># standard error</span></span><br><span class=\"line\">            sample_Z = []   <span class=\"comment\"># Z value of neurons for every sample</span></span><br><span class=\"line\">            sample_a = []   <span class=\"comment\"># sigmoid value of Z for every sample</span></span><br><span class=\"line\">            <span class=\"comment\"># loop all samples</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> sample_iter <span class=\"keyword\">in</span> range(self.num_sample):</span><br><span class=\"line\">                Zlayer = []</span><br><span class=\"line\">                a = []</span><br><span class=\"line\">                a.append(input[sample_iter])    <span class=\"comment\"># place the input as the first a value</span></span><br><span class=\"line\">                <span class=\"comment\"># forward computing Z and a values for all layers</span></span><br><span class=\"line\">                <span class=\"keyword\">for</span> forward_layer <span class=\"keyword\">in</span> range(len(self.units)<span class=\"number\">-1</span>):</span><br><span class=\"line\">                    Zlayer.append(np.matmul(self.weight_layer[forward_layer], np.concatenate((a[forward_layer],[<span class=\"number\">1</span>]))))</span><br><span class=\"line\">                    a.append(self.sigmoid(Zlayer[forward_layer]))</span><br><span class=\"line\">                <span class=\"comment\"># standard error by computing the distance between output layer and true output</span></span><br><span class=\"line\">                stderror.append(a[len(a)<span class=\"number\">-1</span>]-output[sample_iter])</span><br><span class=\"line\">                <span class=\"comment\"># loss function defined as the sum of the least square</span></span><br><span class=\"line\">                <span class=\"comment\"># can be replaced by other functions like cross-entropy</span></span><br><span class=\"line\">                error_acc += (<span class=\"number\">1</span>/<span class=\"number\">2</span>*np.sum((stderror[sample_iter])**<span class=\"number\">2</span>))</span><br><span class=\"line\">                sample_a.append(a)  <span class=\"comment\"># store a values for updating</span></span><br><span class=\"line\">                sample_Z.append(Zlayer) <span class=\"comment\"># store Z values for updating</span></span><br><span class=\"line\">            error.append(error_acc) <span class=\"comment\"># store error for this iteration</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># backward propagate errors to update the weight matrix</span></span><br><span class=\"line\">            i += <span class=\"number\">1</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> error[i<span class=\"number\">-1</span>] &lt;accuracy <span class=\"keyword\">or</span> i &gt;iteration_limit:</span><br><span class=\"line\">                go_on = <span class=\"keyword\">False</span></span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                <span class=\"keyword\">for</span> sample_iter <span class=\"keyword\">in</span> range(self.num_sample):</span><br><span class=\"line\">                    delta = stderror[sample_iter]  <span class=\"comment\"># delta: standard error defined as the derivative of the loss function</span></span><br><span class=\"line\">                    <span class=\"comment\"># initialize g'(z): the derivative of the activation function at the output layer</span></span><br><span class=\"line\">                    deriv_sigmoid =  sample_a[sample_iter][len(sample_a[sample_iter])<span class=\"number\">-1</span>] *\\</span><br><span class=\"line\">                                 (<span class=\"number\">1</span> - sample_a[sample_iter][len(sample_a[sample_iter])<span class=\"number\">-1</span>])</span><br><span class=\"line\">                    <span class=\"comment\"># backward propagation</span></span><br><span class=\"line\">                    <span class=\"keyword\">for</span> backward_layer <span class=\"keyword\">in</span> list(reversed(range(len(self.units)<span class=\"number\">-1</span>))):</span><br><span class=\"line\">                        temp_weight = self.weight_layer[backward_layer] <span class=\"comment\"># store the temporary kth matrix</span></span><br><span class=\"line\">                        <span class=\"comment\"># update the kth matrix</span></span><br><span class=\"line\">                        self.weight_layer[backward_layer][:,:<span class=\"number\">-1</span>] += - self.learningrate/self.num_sample *\\</span><br><span class=\"line\">                            np.outer(delta * deriv_sigmoid, sample_a[sample_iter][backward_layer])</span><br><span class=\"line\">                        <span class=\"comment\"># update delta and derivative of the activation function</span></span><br><span class=\"line\">                        delta = np.dot(temp_weight[:,:<span class=\"number\">-1</span>].T,delta * deriv_sigmoid)</span><br><span class=\"line\">                        deriv_sigmoid = sample_a[sample_iter][backward_layer] *\\</span><br><span class=\"line\">                                 (<span class=\"number\">1</span> - sample_a[sample_iter][backward_layer])</span><br><span class=\"line\">        self.learningspeed = &#123;<span class=\"string\">'error'</span>: error, <span class=\"string\">'iteration'</span>: iteration&#125;</span><br><span class=\"line\">        self.lsdf = pd.DataFrame(self.learningspeed)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> sample_a, self.weight_layer</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># predict</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(self,input,weight)</span>:</span></span><br><span class=\"line\">        out = input+[<span class=\"number\">1</span>]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(weight)):</span><br><span class=\"line\">            out = np.dot(weight[i],np.array(out))</span><br><span class=\"line\">            out = self.sigmoid(out)</span><br><span class=\"line\">            out = np.concatenate((out,[<span class=\"number\">1</span>]))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out[:<span class=\"number\">-1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># test</span></span><br><span class=\"line\">inputx = np.array(([<span class=\"number\">0.2</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>,<span class=\"number\">0.3</span>], [<span class=\"number\">0.1</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.15</span>,<span class=\"number\">0.1</span>],[<span class=\"number\">0.7</span>,<span class=\"number\">0.9</span>,<span class=\"number\">0.4</span>,<span class=\"number\">0.8</span>]))</span><br><span class=\"line\">outputy = np.array(([<span class=\"number\">0.5</span>, <span class=\"number\">0.4</span>], [<span class=\"number\">0.9</span>, <span class=\"number\">0.1</span>],[<span class=\"number\">0.3</span>,<span class=\"number\">0.5</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># build the neural network</span></span><br><span class=\"line\">nn=neuralnetwork(num_sample=inputx.shape[<span class=\"number\">0</span>],num_hidden_layer_units=[<span class=\"number\">15</span>,<span class=\"number\">14</span>,<span class=\"number\">15</span>,<span class=\"number\">16</span>],num_input_feature=inputx.shape[<span class=\"number\">1</span>],</span><br><span class=\"line\">             num_output_feature=outputy.shape[<span class=\"number\">1</span>], bias=[<span class=\"number\">0.1</span>,<span class=\"number\">0.2</span>,<span class=\"number\">0.3</span>,<span class=\"number\">0.4</span>,<span class=\"number\">0.4</span>],learningrate=<span class=\"number\">0.2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># set the accuracy and the iteration limit</span></span><br><span class=\"line\">acc = <span class=\"number\">1e-7</span></span><br><span class=\"line\">iter = <span class=\"number\">20000</span></span><br><span class=\"line\"><span class=\"comment\"># train the neural network</span></span><br><span class=\"line\">out_a,weight = nn.train(inputx,outputy,accuracy = acc,iteration_limit=iter)</span><br><span class=\"line\"><span class=\"comment\"># plot the learning process</span></span><br><span class=\"line\">nn.lsdf.plot(x=<span class=\"string\">'iteration'</span>,y=<span class=\"string\">'error'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># predict</span></span><br><span class=\"line\">input = [<span class=\"number\">0.19</span>, <span class=\"number\">0.51</span>, <span class=\"number\">0.49</span>,<span class=\"number\">0.29</span>]</span><br><span class=\"line\">out = nn.predict(input,weight)</span><br></pre></td></tr></table></figure>\n<p>You can also clone it <a href=\"https://github.com/xl0418/Tensorflow/blob/master/NeuralNetwork.py\" target=\"_blank\" rel=\"noopener\">on my Github</a> </p>\n<h1 id=\"The-end\"><a href=\"#The-end\" class=\"headerlink\" title=\"The end\"></a>The end</h1><p>My code is obviously not the most efficient one as a lot of loops are used. But in another sense it is easy to read for a starter. As you see in the derivation and in the code, there are a huge amount of independent computing that can be parallelized to speed up. So the future plan is to parallelize the code on GPU which may substantially improve the efficiency of the neural network. It would also be a good practice for you to step into the machine learning field. </p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><ul>\n<li><p>An introduction to the math used in Machine Learning:  <a href=\"https://explained.ai/matrix-calculus/index.html\" target=\"_blank\" rel=\"noopener\">The Matrix Calculus You Need For Deep Learning\n</a>.</p>\n</li>\n<li><p>A concrete derivation of backward propagation for a two-layer neural network in Chinese <a href=\"http://www.cnblogs.com/charlotte77/p/5629865.html#!comments\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n</li>\n</ul>"},{"title":"Machine learning on biology S1: model classification via ML","_content":"\nOn my 2nd Phd project, I have constructed a complex trait-population model to describe how species evolve and assemble in a community under natural selection. However, people may naturally ask if such complexity is really needed. What is the discrepancy between the model with and without population dynamics and variable trait variance? To answer this question, a model selection/classification should be performed on the data generated under those candidate models. As what we used in parameter inference, an [Approximate Bayesian Computation-Sequential Monte Carlo method](https://xl0418.github.io/2018/11/30/2018-11-30-SMCplots/) is able to handle this work. However, a huge computational demanding is the bottle neck.  To avoid to waste time on fitting each empirical data set by ABC-SMC, a method that only uses the feature of data sets would be a better choice. This reminds me the existing and fast-developed took, i.e. machine learning. So, from this post on, I would like to share my experience of learning Machine Leaning, how to construct a neural network, how to derive backwards propagation algorithm for leaning parameters of a neural network and how to use tensorflow to simply use machine learning, or even deep learning (a multiple-layer neural network algorithm), to do model selection/classification on the topic I mentioned above. \n\n<!--more-->\n \n# A fast illustration\n\nPresenting application at first would be concrete for people understand what I am talking about. Here in this simple example, I generated 200K data sets for each of the four models, i.e. Brownian Motion model, Ornstein Uhlenbeck (OU) model, Trait-population model, anti-Ornstein Uhlenbeck model. As a preliminary test, I just adjusted the parameter values for \\\\(\\\\gamma\\\\) and \\\\(\\\\alpha\\\\) to stand for 4 models, i.e. when \\\\(\\\\gamma=0,\\\\alpha=0\\\\) it stands for a Brownian Motion model; when \\\\(\\\\gamma>0,\\\\alpha=0\\\\) it stands for an OU model; when \\\\(\\\\gamma=0,\\\\alpha>0\\\\) it stands for an anti-OU model; when \\\\(\\\\gamma>0,\\\\alpha>0\\\\) it stands for a trait-population evolution (TP) model. I didn't remove population dynamics from the BM and OU model. Therefore, the data generated by those two processes would be more alike to TP model than by the standard BM and OU model. The logic behind is that if the learning neural network can distinguish TP from the other 3 models, it is able to pick TP out from model pile with standard BM and OU model. Note that the anti-OU model is similar to Drury et al. model in 2018. \n\nWhen the parameter is nonzero, I simply randomly chose values as I assume we don't know any prior information from the empirical data. \n\nI used in total around 1000K data sets that are randomly generated under 4 models but labeled with correct model names to train a simply three-layer neural network. It is super fast to train such small network and can be done within few minutes. Note that fitting data via ABC-SMC may coat a few days with 30 iterations and 200K particles for each iteration. Then I generated 100 data sets to feed the trained neural network. The generating models are known in advance. The purpose of this test is to check if the trained network is able to recognize the generating models. \n\n# A preliminary result\n\nThe result is shown below:\n\n![fig](2019-02-01-Machinelearningseries1/modelselection1.png)   \n\nEach chart contains four bars indicating the probability of each model (B: BM; O: OU; T: TP; A: Anti-OU). Color red denotes that the prediction of the network is wrong. Otherwise, the highest bar correctly implies the generating model. The results reveal that BM and OU model are confusing to the trained neural network. This makes sense that because of the normalization of traits the output of these two models show no significant discrepancy, i.e. almost all traits are evenly distributed in the trait space. Only the range of the traits would provide some information, i.e. BM model has wider range for the traits than OU model. But the normalization neglects it. The other two models, TP and Anti-OU are nicely distinguishable from the model pile. This indicates that the trait-population model do have significant pattern to express itself. Thus our complexity plays a role in describing trait patterns. \n\n# Future plan\n\nHere in the example, I only exploited a 3-layer neural network but already got a nice result. The issues of confusing BM and OU model may be resolved if a deep neural network is applied. Anyway, the power of machine learning on biology is shown and there are more improvement space waiting for further study.\n\nI do believe that machine learning can be useful in biology. But so far as I know, few literature present this tech on biology field. In fact, data analysis and pattern recognition are also key component in biological research besides modeling. I hope machine learning could attract biologist's attention and thrive here like what it develops in industry and computer science.\n\n# The end\n\nAs planed at the beginning, the following posts would be on machine learning. Coming soon!\n\n","source":"_posts/2019-02-01-Machinelearningseries1.md","raw":"---\ntitle: \"Machine learning on biology S1: model classification via ML\"\ncategories: [Research,Machine learning,Deep learning]\ntags: [Python, Machine learning, neural networks, GPU programming]\n---\n\nOn my 2nd Phd project, I have constructed a complex trait-population model to describe how species evolve and assemble in a community under natural selection. However, people may naturally ask if such complexity is really needed. What is the discrepancy between the model with and without population dynamics and variable trait variance? To answer this question, a model selection/classification should be performed on the data generated under those candidate models. As what we used in parameter inference, an [Approximate Bayesian Computation-Sequential Monte Carlo method](https://xl0418.github.io/2018/11/30/2018-11-30-SMCplots/) is able to handle this work. However, a huge computational demanding is the bottle neck.  To avoid to waste time on fitting each empirical data set by ABC-SMC, a method that only uses the feature of data sets would be a better choice. This reminds me the existing and fast-developed took, i.e. machine learning. So, from this post on, I would like to share my experience of learning Machine Leaning, how to construct a neural network, how to derive backwards propagation algorithm for leaning parameters of a neural network and how to use tensorflow to simply use machine learning, or even deep learning (a multiple-layer neural network algorithm), to do model selection/classification on the topic I mentioned above. \n\n<!--more-->\n \n# A fast illustration\n\nPresenting application at first would be concrete for people understand what I am talking about. Here in this simple example, I generated 200K data sets for each of the four models, i.e. Brownian Motion model, Ornstein Uhlenbeck (OU) model, Trait-population model, anti-Ornstein Uhlenbeck model. As a preliminary test, I just adjusted the parameter values for \\\\(\\\\gamma\\\\) and \\\\(\\\\alpha\\\\) to stand for 4 models, i.e. when \\\\(\\\\gamma=0,\\\\alpha=0\\\\) it stands for a Brownian Motion model; when \\\\(\\\\gamma>0,\\\\alpha=0\\\\) it stands for an OU model; when \\\\(\\\\gamma=0,\\\\alpha>0\\\\) it stands for an anti-OU model; when \\\\(\\\\gamma>0,\\\\alpha>0\\\\) it stands for a trait-population evolution (TP) model. I didn't remove population dynamics from the BM and OU model. Therefore, the data generated by those two processes would be more alike to TP model than by the standard BM and OU model. The logic behind is that if the learning neural network can distinguish TP from the other 3 models, it is able to pick TP out from model pile with standard BM and OU model. Note that the anti-OU model is similar to Drury et al. model in 2018. \n\nWhen the parameter is nonzero, I simply randomly chose values as I assume we don't know any prior information from the empirical data. \n\nI used in total around 1000K data sets that are randomly generated under 4 models but labeled with correct model names to train a simply three-layer neural network. It is super fast to train such small network and can be done within few minutes. Note that fitting data via ABC-SMC may coat a few days with 30 iterations and 200K particles for each iteration. Then I generated 100 data sets to feed the trained neural network. The generating models are known in advance. The purpose of this test is to check if the trained network is able to recognize the generating models. \n\n# A preliminary result\n\nThe result is shown below:\n\n![fig](2019-02-01-Machinelearningseries1/modelselection1.png)   \n\nEach chart contains four bars indicating the probability of each model (B: BM; O: OU; T: TP; A: Anti-OU). Color red denotes that the prediction of the network is wrong. Otherwise, the highest bar correctly implies the generating model. The results reveal that BM and OU model are confusing to the trained neural network. This makes sense that because of the normalization of traits the output of these two models show no significant discrepancy, i.e. almost all traits are evenly distributed in the trait space. Only the range of the traits would provide some information, i.e. BM model has wider range for the traits than OU model. But the normalization neglects it. The other two models, TP and Anti-OU are nicely distinguishable from the model pile. This indicates that the trait-population model do have significant pattern to express itself. Thus our complexity plays a role in describing trait patterns. \n\n# Future plan\n\nHere in the example, I only exploited a 3-layer neural network but already got a nice result. The issues of confusing BM and OU model may be resolved if a deep neural network is applied. Anyway, the power of machine learning on biology is shown and there are more improvement space waiting for further study.\n\nI do believe that machine learning can be useful in biology. But so far as I know, few literature present this tech on biology field. In fact, data analysis and pattern recognition are also key component in biological research besides modeling. I hope machine learning could attract biologist's attention and thrive here like what it develops in industry and computer science.\n\n# The end\n\nAs planed at the beginning, the following posts would be on machine learning. Coming soon!\n\n","slug":"2019-02-01-Machinelearningseries1","published":1,"date":"2020-10-05T11:30:58.233Z","updated":"2020-10-05T11:30:58.238Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy13c0014nj39kc30bjoo","content":"<p>On my 2nd Phd project, I have constructed a complex trait-population model to describe how species evolve and assemble in a community under natural selection. However, people may naturally ask if such complexity is really needed. What is the discrepancy between the model with and without population dynamics and variable trait variance? To answer this question, a model selection/classification should be performed on the data generated under those candidate models. As what we used in parameter inference, an <a href=\"https://xl0418.github.io/2018/11/30/2018-11-30-SMCplots/\">Approximate Bayesian Computation-Sequential Monte Carlo method</a> is able to handle this work. However, a huge computational demanding is the bottle neck.  To avoid to waste time on fitting each empirical data set by ABC-SMC, a method that only uses the feature of data sets would be a better choice. This reminds me the existing and fast-developed took, i.e. machine learning. So, from this post on, I would like to share my experience of learning Machine Leaning, how to construct a neural network, how to derive backwards propagation algorithm for leaning parameters of a neural network and how to use tensorflow to simply use machine learning, or even deep learning (a multiple-layer neural network algorithm), to do model selection/classification on the topic I mentioned above. </p>\n<a id=\"more\"></a>\n<h1 id=\"A-fast-illustration\"><a href=\"#A-fast-illustration\" class=\"headerlink\" title=\"A fast illustration\"></a>A fast illustration</h1><p>Presenting application at first would be concrete for people understand what I am talking about. Here in this simple example, I generated 200K data sets for each of the four models, i.e. Brownian Motion model, Ornstein Uhlenbeck (OU) model, Trait-population model, anti-Ornstein Uhlenbeck model. As a preliminary test, I just adjusted the parameter values for \\(\\gamma\\) and \\(\\alpha\\) to stand for 4 models, i.e. when \\(\\gamma=0,\\alpha=0\\) it stands for a Brownian Motion model; when \\(\\gamma&gt;0,\\alpha=0\\) it stands for an OU model; when \\(\\gamma=0,\\alpha&gt;0\\) it stands for an anti-OU model; when \\(\\gamma&gt;0,\\alpha&gt;0\\) it stands for a trait-population evolution (TP) model. I didn’t remove population dynamics from the BM and OU model. Therefore, the data generated by those two processes would be more alike to TP model than by the standard BM and OU model. The logic behind is that if the learning neural network can distinguish TP from the other 3 models, it is able to pick TP out from model pile with standard BM and OU model. Note that the anti-OU model is similar to Drury et al. model in 2018. </p>\n<p>When the parameter is nonzero, I simply randomly chose values as I assume we don’t know any prior information from the empirical data. </p>\n<p>I used in total around 1000K data sets that are randomly generated under 4 models but labeled with correct model names to train a simply three-layer neural network. It is super fast to train such small network and can be done within few minutes. Note that fitting data via ABC-SMC may coat a few days with 30 iterations and 200K particles for each iteration. Then I generated 100 data sets to feed the trained neural network. The generating models are known in advance. The purpose of this test is to check if the trained network is able to recognize the generating models. </p>\n<h1 id=\"A-preliminary-result\"><a href=\"#A-preliminary-result\" class=\"headerlink\" title=\"A preliminary result\"></a>A preliminary result</h1><p>The result is shown below:</p>\n<p><img src=\"/2020/10/05/2019-02-01-Machinelearningseries1/modelselection1.png\" alt=\"fig\">   </p>\n<p>Each chart contains four bars indicating the probability of each model (B: BM; O: OU; T: TP; A: Anti-OU). Color red denotes that the prediction of the network is wrong. Otherwise, the highest bar correctly implies the generating model. The results reveal that BM and OU model are confusing to the trained neural network. This makes sense that because of the normalization of traits the output of these two models show no significant discrepancy, i.e. almost all traits are evenly distributed in the trait space. Only the range of the traits would provide some information, i.e. BM model has wider range for the traits than OU model. But the normalization neglects it. The other two models, TP and Anti-OU are nicely distinguishable from the model pile. This indicates that the trait-population model do have significant pattern to express itself. Thus our complexity plays a role in describing trait patterns. </p>\n<h1 id=\"Future-plan\"><a href=\"#Future-plan\" class=\"headerlink\" title=\"Future plan\"></a>Future plan</h1><p>Here in the example, I only exploited a 3-layer neural network but already got a nice result. The issues of confusing BM and OU model may be resolved if a deep neural network is applied. Anyway, the power of machine learning on biology is shown and there are more improvement space waiting for further study.</p>\n<p>I do believe that machine learning can be useful in biology. But so far as I know, few literature present this tech on biology field. In fact, data analysis and pattern recognition are also key component in biological research besides modeling. I hope machine learning could attract biologist’s attention and thrive here like what it develops in industry and computer science.</p>\n<h1 id=\"The-end\"><a href=\"#The-end\" class=\"headerlink\" title=\"The end\"></a>The end</h1><p>As planed at the beginning, the following posts would be on machine learning. Coming soon!</p>\n","site":{"data":{}},"excerpt":"<p>On my 2nd Phd project, I have constructed a complex trait-population model to describe how species evolve and assemble in a community under natural selection. However, people may naturally ask if such complexity is really needed. What is the discrepancy between the model with and without population dynamics and variable trait variance? To answer this question, a model selection/classification should be performed on the data generated under those candidate models. As what we used in parameter inference, an <a href=\"https://xl0418.github.io/2018/11/30/2018-11-30-SMCplots/\">Approximate Bayesian Computation-Sequential Monte Carlo method</a> is able to handle this work. However, a huge computational demanding is the bottle neck.  To avoid to waste time on fitting each empirical data set by ABC-SMC, a method that only uses the feature of data sets would be a better choice. This reminds me the existing and fast-developed took, i.e. machine learning. So, from this post on, I would like to share my experience of learning Machine Leaning, how to construct a neural network, how to derive backwards propagation algorithm for leaning parameters of a neural network and how to use tensorflow to simply use machine learning, or even deep learning (a multiple-layer neural network algorithm), to do model selection/classification on the topic I mentioned above. </p>","more":"<h1 id=\"A-fast-illustration\"><a href=\"#A-fast-illustration\" class=\"headerlink\" title=\"A fast illustration\"></a>A fast illustration</h1><p>Presenting application at first would be concrete for people understand what I am talking about. Here in this simple example, I generated 200K data sets for each of the four models, i.e. Brownian Motion model, Ornstein Uhlenbeck (OU) model, Trait-population model, anti-Ornstein Uhlenbeck model. As a preliminary test, I just adjusted the parameter values for \\(\\gamma\\) and \\(\\alpha\\) to stand for 4 models, i.e. when \\(\\gamma=0,\\alpha=0\\) it stands for a Brownian Motion model; when \\(\\gamma&gt;0,\\alpha=0\\) it stands for an OU model; when \\(\\gamma=0,\\alpha&gt;0\\) it stands for an anti-OU model; when \\(\\gamma&gt;0,\\alpha&gt;0\\) it stands for a trait-population evolution (TP) model. I didn’t remove population dynamics from the BM and OU model. Therefore, the data generated by those two processes would be more alike to TP model than by the standard BM and OU model. The logic behind is that if the learning neural network can distinguish TP from the other 3 models, it is able to pick TP out from model pile with standard BM and OU model. Note that the anti-OU model is similar to Drury et al. model in 2018. </p>\n<p>When the parameter is nonzero, I simply randomly chose values as I assume we don’t know any prior information from the empirical data. </p>\n<p>I used in total around 1000K data sets that are randomly generated under 4 models but labeled with correct model names to train a simply three-layer neural network. It is super fast to train such small network and can be done within few minutes. Note that fitting data via ABC-SMC may coat a few days with 30 iterations and 200K particles for each iteration. Then I generated 100 data sets to feed the trained neural network. The generating models are known in advance. The purpose of this test is to check if the trained network is able to recognize the generating models. </p>\n<h1 id=\"A-preliminary-result\"><a href=\"#A-preliminary-result\" class=\"headerlink\" title=\"A preliminary result\"></a>A preliminary result</h1><p>The result is shown below:</p>\n<p><img src=\"/2020/10/05/2019-02-01-Machinelearningseries1/modelselection1.png\" alt=\"fig\">   </p>\n<p>Each chart contains four bars indicating the probability of each model (B: BM; O: OU; T: TP; A: Anti-OU). Color red denotes that the prediction of the network is wrong. Otherwise, the highest bar correctly implies the generating model. The results reveal that BM and OU model are confusing to the trained neural network. This makes sense that because of the normalization of traits the output of these two models show no significant discrepancy, i.e. almost all traits are evenly distributed in the trait space. Only the range of the traits would provide some information, i.e. BM model has wider range for the traits than OU model. But the normalization neglects it. The other two models, TP and Anti-OU are nicely distinguishable from the model pile. This indicates that the trait-population model do have significant pattern to express itself. Thus our complexity plays a role in describing trait patterns. </p>\n<h1 id=\"Future-plan\"><a href=\"#Future-plan\" class=\"headerlink\" title=\"Future plan\"></a>Future plan</h1><p>Here in the example, I only exploited a 3-layer neural network but already got a nice result. The issues of confusing BM and OU model may be resolved if a deep neural network is applied. Anyway, the power of machine learning on biology is shown and there are more improvement space waiting for further study.</p>\n<p>I do believe that machine learning can be useful in biology. But so far as I know, few literature present this tech on biology field. In fact, data analysis and pattern recognition are also key component in biological research besides modeling. I hope machine learning could attract biologist’s attention and thrive here like what it develops in industry and computer science.</p>\n<h1 id=\"The-end\"><a href=\"#The-end\" class=\"headerlink\" title=\"The end\"></a>The end</h1><p>As planed at the beginning, the following posts would be on machine learning. Coming soon!</p>"},{"title":"Bash & R: Dealing with big data; read it! don't source it!","_content":"\nRecently, I was focusing on my 3rd PhD project with mathematical modeling. It was about the extension to the [neutral theory](https://www.nature.com/scitable/knowledge/library/neutral-theory-of-species-diversity-13259703). I am not gonna reveal it now but will bring back in the future.  So I was away from the blog for a while and have to put machine learning series aside, although the derivation of the recurrent neural network is done. It is coming soon as well. Finally, the mathematical model has been done. I have some time to talk something that I've been doing except the modeling when waiting for the simulation results. In this post, I am gonna record a silly issue about loading data in R after extracting the a subset of a big data by Bash.  \n\n<!--more-->\n\n# The right matrix format to load in R\n\nIf you have tracked my posts, you may remember that I used a bash script to extract a matrix out of a big and complex structured m file (see [here](https://xl0418.github.io/2018/10/29/2018-10-29-sed/)). But I mistakenly modified the matrix to fit the matrix format in R.\n\n![fig](2019-03-20-readdatadon/d.png) \n\nWhat's wrong with this format? It is a function to call instead of a data to load. Thus if the data in the `structure()` is very big, like 10000x10000, it is very time-consuming to run the function in R. But our aim is to read the matrix of this size or even larger. It is not supposed to be problematic for R. \n\nAfter a while of brainstorm, I kind of thought it might be due to the matrix format and the way I load the data in R. The command `source()` actually calls the function written in the sourced file.  Thus, to create a matrix by using `structure()` it is easy to exceed the memory size. \n\nThe solution is that we could use `read.csv()` to load the data into R, which is much faster and more efficient for large data sets. To achieve that, we need rewrite the raw data in a csv format. The following is how it looks like in the notepad \n\n![fig](2019-03-20-readdatadon/rewrittendata.png) \n\nNotice that the first line is left blank. It is supposed to put the headers for columns. Then we can directly read it in R by using \n\n```R\nx = read.csv(data,header = FALSE)\n```\n\nSuper fast and it's done. \n\nFinally, to extract the matrix from the raw data, you can just follow what I did before ([here](https://xl0418.github.io/2018/10/29/2018-10-29-sed/))). And replace the 6-15th lines with \n\n```Bash\nA=$(grep -c 'D'{'length(D)+1'}' = \\[' HDs\"$j$i\".Rdata)\nB=$[$A-1]\nsed '/D'{'length(D)+1'}'/{G;s/\\nX\\{'$B'\\}//;tend;x;s/^/X/;x;P;d};p;d;:end;s/D'{'length(D)+1'}'/D/;:a;n;ba' HDs\"$j$i\".Rdata>HDt\"$j$i\".Rdata\nsed -n '/D = \\[/,/\\];/p' HDt\"$j$i\".Rdata>HD\"$j$i\".csv\nsed -i -e 's/D = \\[//' -e 's/\\];//' -e 's/ /,/g' -e 's/,;//g' HD\"$j$i\".csv\n```\n\nwhere I just replace the space by the comma and delete the first and last line of the raw matrix. The `;` at the end of each line is also removed.\n\nEnjoy!\n\n","source":"_posts/2019-03-20-readdatadon.md","raw":"---\ntitle: \"Bash & R: Dealing with big data; read it! don't source it!\"\ncategories: [Research,Bash,sed]\ntags: [data analysis, bash, R]\n---\n\nRecently, I was focusing on my 3rd PhD project with mathematical modeling. It was about the extension to the [neutral theory](https://www.nature.com/scitable/knowledge/library/neutral-theory-of-species-diversity-13259703). I am not gonna reveal it now but will bring back in the future.  So I was away from the blog for a while and have to put machine learning series aside, although the derivation of the recurrent neural network is done. It is coming soon as well. Finally, the mathematical model has been done. I have some time to talk something that I've been doing except the modeling when waiting for the simulation results. In this post, I am gonna record a silly issue about loading data in R after extracting the a subset of a big data by Bash.  \n\n<!--more-->\n\n# The right matrix format to load in R\n\nIf you have tracked my posts, you may remember that I used a bash script to extract a matrix out of a big and complex structured m file (see [here](https://xl0418.github.io/2018/10/29/2018-10-29-sed/)). But I mistakenly modified the matrix to fit the matrix format in R.\n\n![fig](2019-03-20-readdatadon/d.png) \n\nWhat's wrong with this format? It is a function to call instead of a data to load. Thus if the data in the `structure()` is very big, like 10000x10000, it is very time-consuming to run the function in R. But our aim is to read the matrix of this size or even larger. It is not supposed to be problematic for R. \n\nAfter a while of brainstorm, I kind of thought it might be due to the matrix format and the way I load the data in R. The command `source()` actually calls the function written in the sourced file.  Thus, to create a matrix by using `structure()` it is easy to exceed the memory size. \n\nThe solution is that we could use `read.csv()` to load the data into R, which is much faster and more efficient for large data sets. To achieve that, we need rewrite the raw data in a csv format. The following is how it looks like in the notepad \n\n![fig](2019-03-20-readdatadon/rewrittendata.png) \n\nNotice that the first line is left blank. It is supposed to put the headers for columns. Then we can directly read it in R by using \n\n```R\nx = read.csv(data,header = FALSE)\n```\n\nSuper fast and it's done. \n\nFinally, to extract the matrix from the raw data, you can just follow what I did before ([here](https://xl0418.github.io/2018/10/29/2018-10-29-sed/))). And replace the 6-15th lines with \n\n```Bash\nA=$(grep -c 'D'{'length(D)+1'}' = \\[' HDs\"$j$i\".Rdata)\nB=$[$A-1]\nsed '/D'{'length(D)+1'}'/{G;s/\\nX\\{'$B'\\}//;tend;x;s/^/X/;x;P;d};p;d;:end;s/D'{'length(D)+1'}'/D/;:a;n;ba' HDs\"$j$i\".Rdata>HDt\"$j$i\".Rdata\nsed -n '/D = \\[/,/\\];/p' HDt\"$j$i\".Rdata>HD\"$j$i\".csv\nsed -i -e 's/D = \\[//' -e 's/\\];//' -e 's/ /,/g' -e 's/,;//g' HD\"$j$i\".csv\n```\n\nwhere I just replace the space by the comma and delete the first and last line of the raw matrix. The `;` at the end of each line is also removed.\n\nEnjoy!\n\n","slug":"2019-03-20-readdatadon","published":1,"date":"2020-10-05T11:30:58.303Z","updated":"2020-10-05T11:30:58.305Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy13d0017nj3905vi5u7n","content":"<p>Recently, I was focusing on my 3rd PhD project with mathematical modeling. It was about the extension to the <a href=\"https://www.nature.com/scitable/knowledge/library/neutral-theory-of-species-diversity-13259703\" target=\"_blank\" rel=\"noopener\">neutral theory</a>. I am not gonna reveal it now but will bring back in the future.  So I was away from the blog for a while and have to put machine learning series aside, although the derivation of the recurrent neural network is done. It is coming soon as well. Finally, the mathematical model has been done. I have some time to talk something that I’ve been doing except the modeling when waiting for the simulation results. In this post, I am gonna record a silly issue about loading data in R after extracting the a subset of a big data by Bash.  </p>\n<a id=\"more\"></a>\n<h1 id=\"The-right-matrix-format-to-load-in-R\"><a href=\"#The-right-matrix-format-to-load-in-R\" class=\"headerlink\" title=\"The right matrix format to load in R\"></a>The right matrix format to load in R</h1><p>If you have tracked my posts, you may remember that I used a bash script to extract a matrix out of a big and complex structured m file (see <a href=\"https://xl0418.github.io/2018/10/29/2018-10-29-sed/\">here</a>). But I mistakenly modified the matrix to fit the matrix format in R.</p>\n<p><img src=\"/2020/10/05/2019-03-20-readdatadon/d.png\" alt=\"fig\"> </p>\n<p>What’s wrong with this format? It is a function to call instead of a data to load. Thus if the data in the <code>structure()</code> is very big, like 10000x10000, it is very time-consuming to run the function in R. But our aim is to read the matrix of this size or even larger. It is not supposed to be problematic for R. </p>\n<p>After a while of brainstorm, I kind of thought it might be due to the matrix format and the way I load the data in R. The command <code>source()</code> actually calls the function written in the sourced file.  Thus, to create a matrix by using <code>structure()</code> it is easy to exceed the memory size. </p>\n<p>The solution is that we could use <code>read.csv()</code> to load the data into R, which is much faster and more efficient for large data sets. To achieve that, we need rewrite the raw data in a csv format. The following is how it looks like in the notepad </p>\n<p><img src=\"/2020/10/05/2019-03-20-readdatadon/rewrittendata.png\" alt=\"fig\"> </p>\n<p>Notice that the first line is left blank. It is supposed to put the headers for columns. Then we can directly read it in R by using </p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = read.csv(data,header = <span class=\"literal\">FALSE</span>)</span><br></pre></td></tr></table></figure>\n<p>Super fast and it’s done. </p>\n<p>Finally, to extract the matrix from the raw data, you can just follow what I did before (<a href=\"https://xl0418.github.io/2018/10/29/2018-10-29-sed/\">here</a>)). And replace the 6-15th lines with </p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=$(grep -c <span class=\"string\">'D'</span>&#123;<span class=\"string\">'length(D)+1'</span>&#125;<span class=\"string\">' = \\['</span> HDs<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata)</span><br><span class=\"line\">B=$[<span class=\"variable\">$A</span>-1]</span><br><span class=\"line\">sed <span class=\"string\">'/D'</span>&#123;<span class=\"string\">'length(D)+1'</span>&#125;<span class=\"string\">'/&#123;G;s/\\nX\\&#123;'</span><span class=\"variable\">$B</span><span class=\"string\">'\\&#125;//;tend;x;s/^/X/;x;P;d&#125;;p;d;:end;s/D'</span>&#123;<span class=\"string\">'length(D)+1'</span>&#125;<span class=\"string\">'/D/;:a;n;ba'</span> HDs<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata&gt;HDt<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata</span><br><span class=\"line\">sed -n <span class=\"string\">'/D = \\[/,/\\];/p'</span> HDt<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata&gt;HD<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.csv</span><br><span class=\"line\">sed -i -e <span class=\"string\">'s/D = \\[//'</span> -e <span class=\"string\">'s/\\];//'</span> -e <span class=\"string\">'s/ /,/g'</span> -e <span class=\"string\">'s/,;//g'</span> HD<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.csv</span><br></pre></td></tr></table></figure>\n<p>where I just replace the space by the comma and delete the first and last line of the raw matrix. The <code>;</code> at the end of each line is also removed.</p>\n<p>Enjoy!</p>\n","site":{"data":{}},"excerpt":"<p>Recently, I was focusing on my 3rd PhD project with mathematical modeling. It was about the extension to the <a href=\"https://www.nature.com/scitable/knowledge/library/neutral-theory-of-species-diversity-13259703\" target=\"_blank\" rel=\"noopener\">neutral theory</a>. I am not gonna reveal it now but will bring back in the future.  So I was away from the blog for a while and have to put machine learning series aside, although the derivation of the recurrent neural network is done. It is coming soon as well. Finally, the mathematical model has been done. I have some time to talk something that I’ve been doing except the modeling when waiting for the simulation results. In this post, I am gonna record a silly issue about loading data in R after extracting the a subset of a big data by Bash.  </p>","more":"<h1 id=\"The-right-matrix-format-to-load-in-R\"><a href=\"#The-right-matrix-format-to-load-in-R\" class=\"headerlink\" title=\"The right matrix format to load in R\"></a>The right matrix format to load in R</h1><p>If you have tracked my posts, you may remember that I used a bash script to extract a matrix out of a big and complex structured m file (see <a href=\"https://xl0418.github.io/2018/10/29/2018-10-29-sed/\">here</a>). But I mistakenly modified the matrix to fit the matrix format in R.</p>\n<p><img src=\"/2020/10/05/2019-03-20-readdatadon/d.png\" alt=\"fig\"> </p>\n<p>What’s wrong with this format? It is a function to call instead of a data to load. Thus if the data in the <code>structure()</code> is very big, like 10000x10000, it is very time-consuming to run the function in R. But our aim is to read the matrix of this size or even larger. It is not supposed to be problematic for R. </p>\n<p>After a while of brainstorm, I kind of thought it might be due to the matrix format and the way I load the data in R. The command <code>source()</code> actually calls the function written in the sourced file.  Thus, to create a matrix by using <code>structure()</code> it is easy to exceed the memory size. </p>\n<p>The solution is that we could use <code>read.csv()</code> to load the data into R, which is much faster and more efficient for large data sets. To achieve that, we need rewrite the raw data in a csv format. The following is how it looks like in the notepad </p>\n<p><img src=\"/2020/10/05/2019-03-20-readdatadon/rewrittendata.png\" alt=\"fig\"> </p>\n<p>Notice that the first line is left blank. It is supposed to put the headers for columns. Then we can directly read it in R by using </p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = read.csv(data,header = <span class=\"literal\">FALSE</span>)</span><br></pre></td></tr></table></figure>\n<p>Super fast and it’s done. </p>\n<p>Finally, to extract the matrix from the raw data, you can just follow what I did before (<a href=\"https://xl0418.github.io/2018/10/29/2018-10-29-sed/\">here</a>)). And replace the 6-15th lines with </p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=$(grep -c <span class=\"string\">'D'</span>&#123;<span class=\"string\">'length(D)+1'</span>&#125;<span class=\"string\">' = \\['</span> HDs<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata)</span><br><span class=\"line\">B=$[<span class=\"variable\">$A</span>-1]</span><br><span class=\"line\">sed <span class=\"string\">'/D'</span>&#123;<span class=\"string\">'length(D)+1'</span>&#125;<span class=\"string\">'/&#123;G;s/\\nX\\&#123;'</span><span class=\"variable\">$B</span><span class=\"string\">'\\&#125;//;tend;x;s/^/X/;x;P;d&#125;;p;d;:end;s/D'</span>&#123;<span class=\"string\">'length(D)+1'</span>&#125;<span class=\"string\">'/D/;:a;n;ba'</span> HDs<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata&gt;HDt<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata</span><br><span class=\"line\">sed -n <span class=\"string\">'/D = \\[/,/\\];/p'</span> HDt<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.Rdata&gt;HD<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.csv</span><br><span class=\"line\">sed -i -e <span class=\"string\">'s/D = \\[//'</span> -e <span class=\"string\">'s/\\];//'</span> -e <span class=\"string\">'s/ /,/g'</span> -e <span class=\"string\">'s/,;//g'</span> HD<span class=\"string\">\"<span class=\"variable\">$j</span><span class=\"variable\">$i</span>\"</span>.csv</span><br></pre></td></tr></table></figure>\n<p>where I just replace the space by the comma and delete the first and last line of the raw matrix. The <code>;</code> at the end of each line is also removed.</p>\n<p>Enjoy!</p>"},{"title":"Approximate Bayesian Computation: standard version and its variant ABC-SMC","_content":"\nIn the last few months of my PhD project, it is unimaginable that how busy it could be. You need to focus on the current work to finish them, analyzing data, writing paper, structuring the final thesis etc. whilst think about the future, going to industry or staying in academia. To me, I enjoy the process of researching, learning new techs, tackling with challenges. This is the essential need that I want. The outer environment is to support decorating the need to make the environment attractive. So it feels like clothes to human, no matter what kind of clothes they are their essential function is to cover the body that human wants, keeping warm and comfortable. But undoubtedly if available human is chasing after the most fancy clothes that meet human's additional requirements besides the basic need. Think through this, when I know clearly what I want, what kind of job I want to do is clear, meeting my basic need and seeking the most luxury that I fit. \n\nThe words above are more like a conclusion that I came up with in the last few months and an excuse/explanation of no posting. Here, I would like to record some evolutionary algorithms that I used in the projects I have done and the animations of them. I still have no time to go into the details and cannot post at a normal frequency for a while (at least before my graduation). But I promise I will concretize them in the future.     \n\n<!--more-->\n\n# The standard Approximate Bayesian Computation\n\nWith the development of computer science, people start to be able to solve more problems that are high computationally demanding. In 2013, Toni *et al*. overviewed the standard ABC algorithm and developed a sequential Monte Carlo variant to accelerate the convergence of parameters. This series of ABC-like methods is usually called \"likelihood-free\" method, or more precisely \"analytic likelihood-free method\". The basic idea is that it exploits the computational power of computer to generate large amount of data under the focal model and pick up the parameters that produce the data that is most analogous to the reality. Thus, people claim that the obtained parameters are the most likely parameters to reassemble the truth. \n\nIn my project of studying trait-population model, I modified this method a bit and successfully recovered the model parameters from simulation experiments. Later on, I applied our model to baleen whales and found the baleen whales are undergoing a weak environmental adaptation and a relatively strong competition (paper submitted soon).\n\nHere, I would like to show you an animation of how ABC algorithm works.  \n \n<video width=\"100%\" height=\"800\" src=\"MCMC3chains_test3.mp4\" controls=\"controls\">\nThe `<video>` tag is not supported by your browser.\n</video>\n\nIn the video, three Monte Carlo chains are deployed. They are independent of each other, exploring the parameter space from different initial conditions. Normally, it is hard to judge when the chain converges. So, you need to run a long time to check. One alternative way is to calculate the autocorrelation of the chain. You can google that if you want to know more details. \n\n# ABC-SMC for parameter estimation and model selection\n\nApproximate Bayesian Computation-Sequential Monte Carlo algorithm is an parallelized extension to the standard ABC algorithm. The basic idea is to do many sample simulations at one generation. Then, the best fit parameters are chosen to gain higher weight to be sampled in the next generation. With the generation proceeds, the best fit parameters converge to the \"true values\". To do model selection, the models are simply treated as one parameter but at a sequential level advanced to the other parameters. That means you need to chose models first and then sample parameters to complete the sampling process.\n\nThe following figure shows how the samples converges to the true value for parameter estimation. It started with a uniform prior information of &alpha;.\n![fig](2019-06-21-ABCalgorithm/Rplot2.png) \n\nThe following figure shows how model selection works under ABC-SMC algorithm. The details are coming in the future.\n![fig](2019-06-21-ABCalgorithm/modelseleSMC.png) \n\n","source":"_posts/2019-06-21-ABCalgorithm.md","raw":"---\ntitle: \"Approximate Bayesian Computation: standard version and its variant ABC-SMC\"\ncategories: [Research,Algorithm,ABC]\ntags: [data analysis, ABC, Animation]\n---\n\nIn the last few months of my PhD project, it is unimaginable that how busy it could be. You need to focus on the current work to finish them, analyzing data, writing paper, structuring the final thesis etc. whilst think about the future, going to industry or staying in academia. To me, I enjoy the process of researching, learning new techs, tackling with challenges. This is the essential need that I want. The outer environment is to support decorating the need to make the environment attractive. So it feels like clothes to human, no matter what kind of clothes they are their essential function is to cover the body that human wants, keeping warm and comfortable. But undoubtedly if available human is chasing after the most fancy clothes that meet human's additional requirements besides the basic need. Think through this, when I know clearly what I want, what kind of job I want to do is clear, meeting my basic need and seeking the most luxury that I fit. \n\nThe words above are more like a conclusion that I came up with in the last few months and an excuse/explanation of no posting. Here, I would like to record some evolutionary algorithms that I used in the projects I have done and the animations of them. I still have no time to go into the details and cannot post at a normal frequency for a while (at least before my graduation). But I promise I will concretize them in the future.     \n\n<!--more-->\n\n# The standard Approximate Bayesian Computation\n\nWith the development of computer science, people start to be able to solve more problems that are high computationally demanding. In 2013, Toni *et al*. overviewed the standard ABC algorithm and developed a sequential Monte Carlo variant to accelerate the convergence of parameters. This series of ABC-like methods is usually called \"likelihood-free\" method, or more precisely \"analytic likelihood-free method\". The basic idea is that it exploits the computational power of computer to generate large amount of data under the focal model and pick up the parameters that produce the data that is most analogous to the reality. Thus, people claim that the obtained parameters are the most likely parameters to reassemble the truth. \n\nIn my project of studying trait-population model, I modified this method a bit and successfully recovered the model parameters from simulation experiments. Later on, I applied our model to baleen whales and found the baleen whales are undergoing a weak environmental adaptation and a relatively strong competition (paper submitted soon).\n\nHere, I would like to show you an animation of how ABC algorithm works.  \n \n<video width=\"100%\" height=\"800\" src=\"MCMC3chains_test3.mp4\" controls=\"controls\">\nThe `<video>` tag is not supported by your browser.\n</video>\n\nIn the video, three Monte Carlo chains are deployed. They are independent of each other, exploring the parameter space from different initial conditions. Normally, it is hard to judge when the chain converges. So, you need to run a long time to check. One alternative way is to calculate the autocorrelation of the chain. You can google that if you want to know more details. \n\n# ABC-SMC for parameter estimation and model selection\n\nApproximate Bayesian Computation-Sequential Monte Carlo algorithm is an parallelized extension to the standard ABC algorithm. The basic idea is to do many sample simulations at one generation. Then, the best fit parameters are chosen to gain higher weight to be sampled in the next generation. With the generation proceeds, the best fit parameters converge to the \"true values\". To do model selection, the models are simply treated as one parameter but at a sequential level advanced to the other parameters. That means you need to chose models first and then sample parameters to complete the sampling process.\n\nThe following figure shows how the samples converges to the true value for parameter estimation. It started with a uniform prior information of &alpha;.\n![fig](2019-06-21-ABCalgorithm/Rplot2.png) \n\nThe following figure shows how model selection works under ABC-SMC algorithm. The details are coming in the future.\n![fig](2019-06-21-ABCalgorithm/modelseleSMC.png) \n\n","slug":"2019-06-21-ABCalgorithm","published":1,"date":"2020-10-05T11:30:58.306Z","updated":"2020-10-05T11:30:58.307Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy13g0018nj39py0n5ffl","content":"<p>In the last few months of my PhD project, it is unimaginable that how busy it could be. You need to focus on the current work to finish them, analyzing data, writing paper, structuring the final thesis etc. whilst think about the future, going to industry or staying in academia. To me, I enjoy the process of researching, learning new techs, tackling with challenges. This is the essential need that I want. The outer environment is to support decorating the need to make the environment attractive. So it feels like clothes to human, no matter what kind of clothes they are their essential function is to cover the body that human wants, keeping warm and comfortable. But undoubtedly if available human is chasing after the most fancy clothes that meet human’s additional requirements besides the basic need. Think through this, when I know clearly what I want, what kind of job I want to do is clear, meeting my basic need and seeking the most luxury that I fit. </p>\n<p>The words above are more like a conclusion that I came up with in the last few months and an excuse/explanation of no posting. Here, I would like to record some evolutionary algorithms that I used in the projects I have done and the animations of them. I still have no time to go into the details and cannot post at a normal frequency for a while (at least before my graduation). But I promise I will concretize them in the future.     </p>\n<a id=\"more\"></a>\n<h1 id=\"The-standard-Approximate-Bayesian-Computation\"><a href=\"#The-standard-Approximate-Bayesian-Computation\" class=\"headerlink\" title=\"The standard Approximate Bayesian Computation\"></a>The standard Approximate Bayesian Computation</h1><p>With the development of computer science, people start to be able to solve more problems that are high computationally demanding. In 2013, Toni <em>et al</em>. overviewed the standard ABC algorithm and developed a sequential Monte Carlo variant to accelerate the convergence of parameters. This series of ABC-like methods is usually called “likelihood-free” method, or more precisely “analytic likelihood-free method”. The basic idea is that it exploits the computational power of computer to generate large amount of data under the focal model and pick up the parameters that produce the data that is most analogous to the reality. Thus, people claim that the obtained parameters are the most likely parameters to reassemble the truth. </p>\n<p>In my project of studying trait-population model, I modified this method a bit and successfully recovered the model parameters from simulation experiments. Later on, I applied our model to baleen whales and found the baleen whales are undergoing a weak environmental adaptation and a relatively strong competition (paper submitted soon).</p>\n<p>Here, I would like to show you an animation of how ABC algorithm works.  </p>\n<video width=\"100%\" height=\"800\" src=\"MCMC3chains_test3.mp4\" controls=\"controls\"><br>The <code>&lt;video&gt;</code> tag is not supported by your browser.<br></video>\n\n<p>In the video, three Monte Carlo chains are deployed. They are independent of each other, exploring the parameter space from different initial conditions. Normally, it is hard to judge when the chain converges. So, you need to run a long time to check. One alternative way is to calculate the autocorrelation of the chain. You can google that if you want to know more details. </p>\n<h1 id=\"ABC-SMC-for-parameter-estimation-and-model-selection\"><a href=\"#ABC-SMC-for-parameter-estimation-and-model-selection\" class=\"headerlink\" title=\"ABC-SMC for parameter estimation and model selection\"></a>ABC-SMC for parameter estimation and model selection</h1><p>Approximate Bayesian Computation-Sequential Monte Carlo algorithm is an parallelized extension to the standard ABC algorithm. The basic idea is to do many sample simulations at one generation. Then, the best fit parameters are chosen to gain higher weight to be sampled in the next generation. With the generation proceeds, the best fit parameters converge to the “true values”. To do model selection, the models are simply treated as one parameter but at a sequential level advanced to the other parameters. That means you need to chose models first and then sample parameters to complete the sampling process.</p>\n<p>The following figure shows how the samples converges to the true value for parameter estimation. It started with a uniform prior information of &alpha;.<br><img src=\"/2020/10/05/2019-06-21-ABCalgorithm/Rplot2.png\" alt=\"fig\"> </p>\n<p>The following figure shows how model selection works under ABC-SMC algorithm. The details are coming in the future.<br><img src=\"/2020/10/05/2019-06-21-ABCalgorithm/modelseleSMC.png\" alt=\"fig\"> </p>\n","site":{"data":{}},"excerpt":"<p>In the last few months of my PhD project, it is unimaginable that how busy it could be. You need to focus on the current work to finish them, analyzing data, writing paper, structuring the final thesis etc. whilst think about the future, going to industry or staying in academia. To me, I enjoy the process of researching, learning new techs, tackling with challenges. This is the essential need that I want. The outer environment is to support decorating the need to make the environment attractive. So it feels like clothes to human, no matter what kind of clothes they are their essential function is to cover the body that human wants, keeping warm and comfortable. But undoubtedly if available human is chasing after the most fancy clothes that meet human’s additional requirements besides the basic need. Think through this, when I know clearly what I want, what kind of job I want to do is clear, meeting my basic need and seeking the most luxury that I fit. </p>\n<p>The words above are more like a conclusion that I came up with in the last few months and an excuse/explanation of no posting. Here, I would like to record some evolutionary algorithms that I used in the projects I have done and the animations of them. I still have no time to go into the details and cannot post at a normal frequency for a while (at least before my graduation). But I promise I will concretize them in the future.     </p>","more":"<h1 id=\"The-standard-Approximate-Bayesian-Computation\"><a href=\"#The-standard-Approximate-Bayesian-Computation\" class=\"headerlink\" title=\"The standard Approximate Bayesian Computation\"></a>The standard Approximate Bayesian Computation</h1><p>With the development of computer science, people start to be able to solve more problems that are high computationally demanding. In 2013, Toni <em>et al</em>. overviewed the standard ABC algorithm and developed a sequential Monte Carlo variant to accelerate the convergence of parameters. This series of ABC-like methods is usually called “likelihood-free” method, or more precisely “analytic likelihood-free method”. The basic idea is that it exploits the computational power of computer to generate large amount of data under the focal model and pick up the parameters that produce the data that is most analogous to the reality. Thus, people claim that the obtained parameters are the most likely parameters to reassemble the truth. </p>\n<p>In my project of studying trait-population model, I modified this method a bit and successfully recovered the model parameters from simulation experiments. Later on, I applied our model to baleen whales and found the baleen whales are undergoing a weak environmental adaptation and a relatively strong competition (paper submitted soon).</p>\n<p>Here, I would like to show you an animation of how ABC algorithm works.  </p>\n<video width=\"100%\" height=\"800\" src=\"MCMC3chains_test3.mp4\" controls=\"controls\"><br>The <code>&lt;video&gt;</code> tag is not supported by your browser.<br></video>\n\n<p>In the video, three Monte Carlo chains are deployed. They are independent of each other, exploring the parameter space from different initial conditions. Normally, it is hard to judge when the chain converges. So, you need to run a long time to check. One alternative way is to calculate the autocorrelation of the chain. You can google that if you want to know more details. </p>\n<h1 id=\"ABC-SMC-for-parameter-estimation-and-model-selection\"><a href=\"#ABC-SMC-for-parameter-estimation-and-model-selection\" class=\"headerlink\" title=\"ABC-SMC for parameter estimation and model selection\"></a>ABC-SMC for parameter estimation and model selection</h1><p>Approximate Bayesian Computation-Sequential Monte Carlo algorithm is an parallelized extension to the standard ABC algorithm. The basic idea is to do many sample simulations at one generation. Then, the best fit parameters are chosen to gain higher weight to be sampled in the next generation. With the generation proceeds, the best fit parameters converge to the “true values”. To do model selection, the models are simply treated as one parameter but at a sequential level advanced to the other parameters. That means you need to chose models first and then sample parameters to complete the sampling process.</p>\n<p>The following figure shows how the samples converges to the true value for parameter estimation. It started with a uniform prior information of &alpha;.<br><img src=\"/2020/10/05/2019-06-21-ABCalgorithm/Rplot2.png\" alt=\"fig\"> </p>\n<p>The following figure shows how model selection works under ABC-SMC algorithm. The details are coming in the future.<br><img src=\"/2020/10/05/2019-06-21-ABCalgorithm/modelseleSMC.png\" alt=\"fig\"> </p>"},{"title":"GUI your model - a way to sell theoretical models to empiricists ","_content":"\nThesis submitted! Finally, I could have a bit of time to update this blog before getting feedback from reviewers of two submitted papers. So, I will dig several pits as what I did before and see if in the future I could fill them : - ) The first pit stems from my thinking of that how we, biological theoritians, are able to sell  our work to empiricists. Our work and interest are to stablish complex mathematical models to mimic biological processes to reveal the underlying mechanisms, which form the observed phenomenon. We could code the processes, do statistic analysis to select the most prominent mechanism, infer the likely generating parameters. However, empiricists may have problems even setting up an environment for a programming language in which we used to deploy our models as their focuses are on field or lab experiments which I have not a clue of what they are (exaggerated).  Therefore, to bridge the worlds of empiricists and theoritians, a concrete tool is imperative. GUI (Graphical User Interface) of an abstract model is the right way. (This is the way!  -- Mandalorian).  In this blog, I introduce one GUI example of my model and show how it works within a few clicks. \n\n<!--more-->\n\n# A brief introduction of the trait evolution model\n\nDiversity in traits of species has long been intriguing to biologists. One of the most famous examples is the Galápagos finches, which is also known as Darwin finches. Darwin's Finches comprise a group of about 15 species. Their beak sizes are evidenced to associate with the size of seeds that are available in environment, showing apparent evolutionary evidence responding to ecological changes. In the species *Geospiza fortis*, people found that the beak size of the birds is larger when large seeds are more available. Conversely, when small seeds become more available for some years, small beaks are pervasive in the population.\n\nIn our model of trait evolution with population dynamics, we incorporate the phylogenetic information into consideration. Furthermore, abundance of species plays a role in species interaction, hence,  tunes the trait evolution pace. An animation has been posted before [here](https://xl0418.github.io/2019/06/21/2019-06-21-TPmodel/), which illustrates how the traits of species evolve along a given phylogeny. The manuscript is under review. Details can be found there once it gets published.  \n\nWith this model, empiricists can input the phylogenies and simulate the traits of interest. Using approximate Bayesian computation methods and comparing the simulated traits with the observed traits, one can infer the strength of the environment stabilizing selection and competition. The purpose of designing a GUI is to free empiricists hands from implementing the code. Instead, only a few clicks can do all the work.\n\n# GUI \n\nI use `tkinter` to develop the GUI. It has an exhaustive [document](https://docs.python.org/3/library/tkinter.html) to help you get familiar with it. So, I am not ganna show you how to code it. You can find the code [here](https://github.com/xl0418/Trait_pop_model_sim/tree/UI-branch). But I would like to show you why empiricists would love it!\n\nThe whole model and nearly all its functions are wrapped into this interface.  \n\n<img src=\"2020-03-09-GUI-traitevolution/p1.png\" width=\"100%\" height=\"100%\">\n\nIn the page of Parameter Inference, users can specify their own phylogenies with observed traits data and also assign a folder to store the output. The structure of the algorithm (ABC) can be set by giving the number of iterations and particles for each iteration. The larger the values are, the accurate the inference is. But it also takes longer time for computation. Parallel computation has been implemented with threads being indicating how many threads you want to exploit. We have three summary statistics to compare the similarity between the simulated traits and the empirical traits. Details can be found in the paper. \n\nAs usually we don't know how many iterations are sufficient to get a good enough result, we may start with a small number of iterations to check if the results converge. If not, we can continue with this Continue Parameter Inference page.   <img src=\"2020-03-09-GUI-traitevolution/p2.png\" width=\"100%\" height=\"100%\">\n\nIn this page, one additional setting appears. You can specify the previous result on which you want to continue. Then, set a continue number of the iterations while the number of the particles inherits from the previous result. This page offers a choice of cutting jobs into pieces and running them at different time. \n\nAlthough ABC approach is a nice likelihood-free method for parameter inference, high computational demanding limits its applicability. Normally, I need to work with a cluster to free my desktop for other works. So in the page of Generate Cluster Scripts, one can set all parameters aforementioned and click one button to generate a script to submit jobs on the cluster. This is one example of the peregrine cluster of the University of Groningen. You can modify it accordingly.   <img src=\"2020-03-09-GUI-traitevolution/p3.png\" width=\"100%\" height=\"100%\">\n\nFurthermore, one can even do data analysis on the results by using GUI. Here, I set a plotting function to investigate the distributions of the estimated parameters. \n\n  <img src=\"2020-03-09-GUI-traitevolution/p4.png\" width=\"100%\" height=\"100%\">\n\nMore functions can be developed. With GUI, empiricists would love to try theoritian's models. One may argue that it is convenient but the model is hidden behind the interface so that it lost the model's plasticity. Well, when you succeed to attract empiricists' attention, GUI's job is done. Further model adjustification relies on collaboration. You have already sold your work!  \n\n ","source":"_posts/2020-03-09-GUI-traitevolution.md","raw":"---\ntitle: \"GUI your model - a way to sell theoretical models to empiricists \"\ncategories: [Research,Python, GUI]\ntags: [Python,GUI, models]\n\n---\n\nThesis submitted! Finally, I could have a bit of time to update this blog before getting feedback from reviewers of two submitted papers. So, I will dig several pits as what I did before and see if in the future I could fill them : - ) The first pit stems from my thinking of that how we, biological theoritians, are able to sell  our work to empiricists. Our work and interest are to stablish complex mathematical models to mimic biological processes to reveal the underlying mechanisms, which form the observed phenomenon. We could code the processes, do statistic analysis to select the most prominent mechanism, infer the likely generating parameters. However, empiricists may have problems even setting up an environment for a programming language in which we used to deploy our models as their focuses are on field or lab experiments which I have not a clue of what they are (exaggerated).  Therefore, to bridge the worlds of empiricists and theoritians, a concrete tool is imperative. GUI (Graphical User Interface) of an abstract model is the right way. (This is the way!  -- Mandalorian).  In this blog, I introduce one GUI example of my model and show how it works within a few clicks. \n\n<!--more-->\n\n# A brief introduction of the trait evolution model\n\nDiversity in traits of species has long been intriguing to biologists. One of the most famous examples is the Galápagos finches, which is also known as Darwin finches. Darwin's Finches comprise a group of about 15 species. Their beak sizes are evidenced to associate with the size of seeds that are available in environment, showing apparent evolutionary evidence responding to ecological changes. In the species *Geospiza fortis*, people found that the beak size of the birds is larger when large seeds are more available. Conversely, when small seeds become more available for some years, small beaks are pervasive in the population.\n\nIn our model of trait evolution with population dynamics, we incorporate the phylogenetic information into consideration. Furthermore, abundance of species plays a role in species interaction, hence,  tunes the trait evolution pace. An animation has been posted before [here](https://xl0418.github.io/2019/06/21/2019-06-21-TPmodel/), which illustrates how the traits of species evolve along a given phylogeny. The manuscript is under review. Details can be found there once it gets published.  \n\nWith this model, empiricists can input the phylogenies and simulate the traits of interest. Using approximate Bayesian computation methods and comparing the simulated traits with the observed traits, one can infer the strength of the environment stabilizing selection and competition. The purpose of designing a GUI is to free empiricists hands from implementing the code. Instead, only a few clicks can do all the work.\n\n# GUI \n\nI use `tkinter` to develop the GUI. It has an exhaustive [document](https://docs.python.org/3/library/tkinter.html) to help you get familiar with it. So, I am not ganna show you how to code it. You can find the code [here](https://github.com/xl0418/Trait_pop_model_sim/tree/UI-branch). But I would like to show you why empiricists would love it!\n\nThe whole model and nearly all its functions are wrapped into this interface.  \n\n<img src=\"2020-03-09-GUI-traitevolution/p1.png\" width=\"100%\" height=\"100%\">\n\nIn the page of Parameter Inference, users can specify their own phylogenies with observed traits data and also assign a folder to store the output. The structure of the algorithm (ABC) can be set by giving the number of iterations and particles for each iteration. The larger the values are, the accurate the inference is. But it also takes longer time for computation. Parallel computation has been implemented with threads being indicating how many threads you want to exploit. We have three summary statistics to compare the similarity between the simulated traits and the empirical traits. Details can be found in the paper. \n\nAs usually we don't know how many iterations are sufficient to get a good enough result, we may start with a small number of iterations to check if the results converge. If not, we can continue with this Continue Parameter Inference page.   <img src=\"2020-03-09-GUI-traitevolution/p2.png\" width=\"100%\" height=\"100%\">\n\nIn this page, one additional setting appears. You can specify the previous result on which you want to continue. Then, set a continue number of the iterations while the number of the particles inherits from the previous result. This page offers a choice of cutting jobs into pieces and running them at different time. \n\nAlthough ABC approach is a nice likelihood-free method for parameter inference, high computational demanding limits its applicability. Normally, I need to work with a cluster to free my desktop for other works. So in the page of Generate Cluster Scripts, one can set all parameters aforementioned and click one button to generate a script to submit jobs on the cluster. This is one example of the peregrine cluster of the University of Groningen. You can modify it accordingly.   <img src=\"2020-03-09-GUI-traitevolution/p3.png\" width=\"100%\" height=\"100%\">\n\nFurthermore, one can even do data analysis on the results by using GUI. Here, I set a plotting function to investigate the distributions of the estimated parameters. \n\n  <img src=\"2020-03-09-GUI-traitevolution/p4.png\" width=\"100%\" height=\"100%\">\n\nMore functions can be developed. With GUI, empiricists would love to try theoritian's models. One may argue that it is convenient but the model is hidden behind the interface so that it lost the model's plasticity. Well, when you succeed to attract empiricists' attention, GUI's job is done. Further model adjustification relies on collaboration. You have already sold your work!  \n\n ","slug":"2020-03-09-GUI-traitevolution","published":1,"date":"2020-10-05T11:30:59.184Z","updated":"2020-10-05T11:30:59.185Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy13j001cnj398rj2m8mh","content":"<p>Thesis submitted! Finally, I could have a bit of time to update this blog before getting feedback from reviewers of two submitted papers. So, I will dig several pits as what I did before and see if in the future I could fill them : - ) The first pit stems from my thinking of that how we, biological theoritians, are able to sell  our work to empiricists. Our work and interest are to stablish complex mathematical models to mimic biological processes to reveal the underlying mechanisms, which form the observed phenomenon. We could code the processes, do statistic analysis to select the most prominent mechanism, infer the likely generating parameters. However, empiricists may have problems even setting up an environment for a programming language in which we used to deploy our models as their focuses are on field or lab experiments which I have not a clue of what they are (exaggerated).  Therefore, to bridge the worlds of empiricists and theoritians, a concrete tool is imperative. GUI (Graphical User Interface) of an abstract model is the right way. (This is the way!  – Mandalorian).  In this blog, I introduce one GUI example of my model and show how it works within a few clicks. </p>\n<a id=\"more\"></a>\n<h1 id=\"A-brief-introduction-of-the-trait-evolution-model\"><a href=\"#A-brief-introduction-of-the-trait-evolution-model\" class=\"headerlink\" title=\"A brief introduction of the trait evolution model\"></a>A brief introduction of the trait evolution model</h1><p>Diversity in traits of species has long been intriguing to biologists. One of the most famous examples is the Galápagos finches, which is also known as Darwin finches. Darwin’s Finches comprise a group of about 15 species. Their beak sizes are evidenced to associate with the size of seeds that are available in environment, showing apparent evolutionary evidence responding to ecological changes. In the species <em>Geospiza fortis</em>, people found that the beak size of the birds is larger when large seeds are more available. Conversely, when small seeds become more available for some years, small beaks are pervasive in the population.</p>\n<p>In our model of trait evolution with population dynamics, we incorporate the phylogenetic information into consideration. Furthermore, abundance of species plays a role in species interaction, hence,  tunes the trait evolution pace. An animation has been posted before <a href=\"https://xl0418.github.io/2019/06/21/2019-06-21-TPmodel/\">here</a>, which illustrates how the traits of species evolve along a given phylogeny. The manuscript is under review. Details can be found there once it gets published.  </p>\n<p>With this model, empiricists can input the phylogenies and simulate the traits of interest. Using approximate Bayesian computation methods and comparing the simulated traits with the observed traits, one can infer the strength of the environment stabilizing selection and competition. The purpose of designing a GUI is to free empiricists hands from implementing the code. Instead, only a few clicks can do all the work.</p>\n<h1 id=\"GUI\"><a href=\"#GUI\" class=\"headerlink\" title=\"GUI\"></a>GUI</h1><p>I use <code>tkinter</code> to develop the GUI. It has an exhaustive <a href=\"https://docs.python.org/3/library/tkinter.html\" target=\"_blank\" rel=\"noopener\">document</a> to help you get familiar with it. So, I am not ganna show you how to code it. You can find the code <a href=\"https://github.com/xl0418/Trait_pop_model_sim/tree/UI-branch\" target=\"_blank\" rel=\"noopener\">here</a>. But I would like to show you why empiricists would love it!</p>\n<p>The whole model and nearly all its functions are wrapped into this interface.  </p>\n<p><img src=\"/2020/10/05/2020-03-09-GUI-traitevolution/p1.png\" width=\"100%\" height=\"100%\"></p>\n<p>In the page of Parameter Inference, users can specify their own phylogenies with observed traits data and also assign a folder to store the output. The structure of the algorithm (ABC) can be set by giving the number of iterations and particles for each iteration. The larger the values are, the accurate the inference is. But it also takes longer time for computation. Parallel computation has been implemented with threads being indicating how many threads you want to exploit. We have three summary statistics to compare the similarity between the simulated traits and the empirical traits. Details can be found in the paper. </p>\n<p>As usually we don’t know how many iterations are sufficient to get a good enough result, we may start with a small number of iterations to check if the results converge. If not, we can continue with this Continue Parameter Inference page.   <img src=\"/2020/10/05/2020-03-09-GUI-traitevolution/p2.png\" width=\"100%\" height=\"100%\"></p>\n<p>In this page, one additional setting appears. You can specify the previous result on which you want to continue. Then, set a continue number of the iterations while the number of the particles inherits from the previous result. This page offers a choice of cutting jobs into pieces and running them at different time. </p>\n<p>Although ABC approach is a nice likelihood-free method for parameter inference, high computational demanding limits its applicability. Normally, I need to work with a cluster to free my desktop for other works. So in the page of Generate Cluster Scripts, one can set all parameters aforementioned and click one button to generate a script to submit jobs on the cluster. This is one example of the peregrine cluster of the University of Groningen. You can modify it accordingly.   <img src=\"/2020/10/05/2020-03-09-GUI-traitevolution/p3.png\" width=\"100%\" height=\"100%\"></p>\n<p>Furthermore, one can even do data analysis on the results by using GUI. Here, I set a plotting function to investigate the distributions of the estimated parameters. </p>\n<p>  <img src=\"/2020/10/05/2020-03-09-GUI-traitevolution/p4.png\" width=\"100%\" height=\"100%\"></p>\n<p>More functions can be developed. With GUI, empiricists would love to try theoritian’s models. One may argue that it is convenient but the model is hidden behind the interface so that it lost the model’s plasticity. Well, when you succeed to attract empiricists’ attention, GUI’s job is done. Further model adjustification relies on collaboration. You have already sold your work!  </p>\n","site":{"data":{}},"excerpt":"<p>Thesis submitted! Finally, I could have a bit of time to update this blog before getting feedback from reviewers of two submitted papers. So, I will dig several pits as what I did before and see if in the future I could fill them : - ) The first pit stems from my thinking of that how we, biological theoritians, are able to sell  our work to empiricists. Our work and interest are to stablish complex mathematical models to mimic biological processes to reveal the underlying mechanisms, which form the observed phenomenon. We could code the processes, do statistic analysis to select the most prominent mechanism, infer the likely generating parameters. However, empiricists may have problems even setting up an environment for a programming language in which we used to deploy our models as their focuses are on field or lab experiments which I have not a clue of what they are (exaggerated).  Therefore, to bridge the worlds of empiricists and theoritians, a concrete tool is imperative. GUI (Graphical User Interface) of an abstract model is the right way. (This is the way!  – Mandalorian).  In this blog, I introduce one GUI example of my model and show how it works within a few clicks. </p>","more":"<h1 id=\"A-brief-introduction-of-the-trait-evolution-model\"><a href=\"#A-brief-introduction-of-the-trait-evolution-model\" class=\"headerlink\" title=\"A brief introduction of the trait evolution model\"></a>A brief introduction of the trait evolution model</h1><p>Diversity in traits of species has long been intriguing to biologists. One of the most famous examples is the Galápagos finches, which is also known as Darwin finches. Darwin’s Finches comprise a group of about 15 species. Their beak sizes are evidenced to associate with the size of seeds that are available in environment, showing apparent evolutionary evidence responding to ecological changes. In the species <em>Geospiza fortis</em>, people found that the beak size of the birds is larger when large seeds are more available. Conversely, when small seeds become more available for some years, small beaks are pervasive in the population.</p>\n<p>In our model of trait evolution with population dynamics, we incorporate the phylogenetic information into consideration. Furthermore, abundance of species plays a role in species interaction, hence,  tunes the trait evolution pace. An animation has been posted before <a href=\"https://xl0418.github.io/2019/06/21/2019-06-21-TPmodel/\">here</a>, which illustrates how the traits of species evolve along a given phylogeny. The manuscript is under review. Details can be found there once it gets published.  </p>\n<p>With this model, empiricists can input the phylogenies and simulate the traits of interest. Using approximate Bayesian computation methods and comparing the simulated traits with the observed traits, one can infer the strength of the environment stabilizing selection and competition. The purpose of designing a GUI is to free empiricists hands from implementing the code. Instead, only a few clicks can do all the work.</p>\n<h1 id=\"GUI\"><a href=\"#GUI\" class=\"headerlink\" title=\"GUI\"></a>GUI</h1><p>I use <code>tkinter</code> to develop the GUI. It has an exhaustive <a href=\"https://docs.python.org/3/library/tkinter.html\" target=\"_blank\" rel=\"noopener\">document</a> to help you get familiar with it. So, I am not ganna show you how to code it. You can find the code <a href=\"https://github.com/xl0418/Trait_pop_model_sim/tree/UI-branch\" target=\"_blank\" rel=\"noopener\">here</a>. But I would like to show you why empiricists would love it!</p>\n<p>The whole model and nearly all its functions are wrapped into this interface.  </p>\n<p><img src=\"/2020/10/05/2020-03-09-GUI-traitevolution/p1.png\" width=\"100%\" height=\"100%\"></p>\n<p>In the page of Parameter Inference, users can specify their own phylogenies with observed traits data and also assign a folder to store the output. The structure of the algorithm (ABC) can be set by giving the number of iterations and particles for each iteration. The larger the values are, the accurate the inference is. But it also takes longer time for computation. Parallel computation has been implemented with threads being indicating how many threads you want to exploit. We have three summary statistics to compare the similarity between the simulated traits and the empirical traits. Details can be found in the paper. </p>\n<p>As usually we don’t know how many iterations are sufficient to get a good enough result, we may start with a small number of iterations to check if the results converge. If not, we can continue with this Continue Parameter Inference page.   <img src=\"/2020/10/05/2020-03-09-GUI-traitevolution/p2.png\" width=\"100%\" height=\"100%\"></p>\n<p>In this page, one additional setting appears. You can specify the previous result on which you want to continue. Then, set a continue number of the iterations while the number of the particles inherits from the previous result. This page offers a choice of cutting jobs into pieces and running them at different time. </p>\n<p>Although ABC approach is a nice likelihood-free method for parameter inference, high computational demanding limits its applicability. Normally, I need to work with a cluster to free my desktop for other works. So in the page of Generate Cluster Scripts, one can set all parameters aforementioned and click one button to generate a script to submit jobs on the cluster. This is one example of the peregrine cluster of the University of Groningen. You can modify it accordingly.   <img src=\"/2020/10/05/2020-03-09-GUI-traitevolution/p3.png\" width=\"100%\" height=\"100%\"></p>\n<p>Furthermore, one can even do data analysis on the results by using GUI. Here, I set a plotting function to investigate the distributions of the estimated parameters. </p>\n<p>  <img src=\"/2020/10/05/2020-03-09-GUI-traitevolution/p4.png\" width=\"100%\" height=\"100%\"></p>\n<p>More functions can be developed. With GUI, empiricists would love to try theoritian’s models. One may argue that it is convenient but the model is hidden behind the interface so that it lost the model’s plasticity. Well, when you succeed to attract empiricists’ attention, GUI’s job is done. Further model adjustification relies on collaboration. You have already sold your work!  </p>"},{"title":"An animation to show how traits of species evolve with their abundance","_content":"\nDo you like comics? I am a comics fan, majorly favoring Japanese comics and some Chinese comics with the rise of Chinese comics industry in recent years. Why do I mention this? I mean to say drawings are usually the most straight way to express abstract ideas. This is why I want to show an animation of our model to explain such complex mechanism instead of using tons of papers filled with equations.   \n\n<!--more-->\n\n# An animation of trait-population coevolution for a single attraction\n\nWe have constructed a mathematical model to describe how traits of species interplay with their abundance under environmental adaptation and competition along a phylogenetic tree. For simplicity, we assume one attraction in the community that pull traits evolve toward one single optimum trait that better utilize natural resource. In this model, population dynamics weight the species' competitive power, i.e. the species with large population size gains more competitive power. We would like to see what kind of trait pattern under such mechanism. The animation below shows how it works along a given phylogenetic tree.  \n \n<video width=\"100%\" height=\"800\" src=\"singlespecies5.mp4\" controls=\"controls\">\nThe `<video>` tag is not supported by your browser.\n</video>\n\nThe video is consisting of two models showing different magnitude of attraction towards the optimum for 5 species. The area of red dots denote the population size of that species.\\\\(x\\\\) axis denotes the evolutionary time while \\\\(y\\\\) denotes the trait values. The optimum trait is assumed as 0 without loss of generality. The highlighted by green lineage is the destined species giving birth and going extinct. More details are coming after the paper submission.  \n\n# An animation of trait-population coevolution for multiple attractions\n\nIn the following video, 15 species are evolving under 5 attractions. You can see how they compete with each other and explore the new niches. \n\n<video width=\"100%\" height=\"800\" src=\"multi6species15.mp4\" controls=\"controls\">\nThe `<video>` tag is not supported by your browser.\n</video>\n\nHave fun!\n\n<img src=\"2019-06-21-TPmodel/guixianren_s.png\" width=\"50%\" height=\"50%\">\n","source":"_posts/2019-06-21-TPmodel.md","raw":"---\ntitle: \"An animation to show how traits of species evolve with their abundance\"\ncategories: [Research,Phd projects,trait-population model]\ntags: [Animation,ecology, evolution]\n---\n\nDo you like comics? I am a comics fan, majorly favoring Japanese comics and some Chinese comics with the rise of Chinese comics industry in recent years. Why do I mention this? I mean to say drawings are usually the most straight way to express abstract ideas. This is why I want to show an animation of our model to explain such complex mechanism instead of using tons of papers filled with equations.   \n\n<!--more-->\n\n# An animation of trait-population coevolution for a single attraction\n\nWe have constructed a mathematical model to describe how traits of species interplay with their abundance under environmental adaptation and competition along a phylogenetic tree. For simplicity, we assume one attraction in the community that pull traits evolve toward one single optimum trait that better utilize natural resource. In this model, population dynamics weight the species' competitive power, i.e. the species with large population size gains more competitive power. We would like to see what kind of trait pattern under such mechanism. The animation below shows how it works along a given phylogenetic tree.  \n \n<video width=\"100%\" height=\"800\" src=\"singlespecies5.mp4\" controls=\"controls\">\nThe `<video>` tag is not supported by your browser.\n</video>\n\nThe video is consisting of two models showing different magnitude of attraction towards the optimum for 5 species. The area of red dots denote the population size of that species.\\\\(x\\\\) axis denotes the evolutionary time while \\\\(y\\\\) denotes the trait values. The optimum trait is assumed as 0 without loss of generality. The highlighted by green lineage is the destined species giving birth and going extinct. More details are coming after the paper submission.  \n\n# An animation of trait-population coevolution for multiple attractions\n\nIn the following video, 15 species are evolving under 5 attractions. You can see how they compete with each other and explore the new niches. \n\n<video width=\"100%\" height=\"800\" src=\"multi6species15.mp4\" controls=\"controls\">\nThe `<video>` tag is not supported by your browser.\n</video>\n\nHave fun!\n\n<img src=\"2019-06-21-TPmodel/guixianren_s.png\" width=\"50%\" height=\"50%\">\n","slug":"2019-06-21-TPmodel","published":1,"date":"2020-10-05T11:30:58.478Z","updated":"2020-10-05T11:30:58.481Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy13k001fnj39gsoz8gkt","content":"<p>Do you like comics? I am a comics fan, majorly favoring Japanese comics and some Chinese comics with the rise of Chinese comics industry in recent years. Why do I mention this? I mean to say drawings are usually the most straight way to express abstract ideas. This is why I want to show an animation of our model to explain such complex mechanism instead of using tons of papers filled with equations.   </p>\n<a id=\"more\"></a>\n<h1 id=\"An-animation-of-trait-population-coevolution-for-a-single-attraction\"><a href=\"#An-animation-of-trait-population-coevolution-for-a-single-attraction\" class=\"headerlink\" title=\"An animation of trait-population coevolution for a single attraction\"></a>An animation of trait-population coevolution for a single attraction</h1><p>We have constructed a mathematical model to describe how traits of species interplay with their abundance under environmental adaptation and competition along a phylogenetic tree. For simplicity, we assume one attraction in the community that pull traits evolve toward one single optimum trait that better utilize natural resource. In this model, population dynamics weight the species’ competitive power, i.e. the species with large population size gains more competitive power. We would like to see what kind of trait pattern under such mechanism. The animation below shows how it works along a given phylogenetic tree.  </p>\n<video width=\"100%\" height=\"800\" src=\"singlespecies5.mp4\" controls=\"controls\"><br>The <code>&lt;video&gt;</code> tag is not supported by your browser.<br></video>\n\n<p>The video is consisting of two models showing different magnitude of attraction towards the optimum for 5 species. The area of red dots denote the population size of that species.\\(x\\) axis denotes the evolutionary time while \\(y\\) denotes the trait values. The optimum trait is assumed as 0 without loss of generality. The highlighted by green lineage is the destined species giving birth and going extinct. More details are coming after the paper submission.  </p>\n<h1 id=\"An-animation-of-trait-population-coevolution-for-multiple-attractions\"><a href=\"#An-animation-of-trait-population-coevolution-for-multiple-attractions\" class=\"headerlink\" title=\"An animation of trait-population coevolution for multiple attractions\"></a>An animation of trait-population coevolution for multiple attractions</h1><p>In the following video, 15 species are evolving under 5 attractions. You can see how they compete with each other and explore the new niches. </p>\n<video width=\"100%\" height=\"800\" src=\"multi6species15.mp4\" controls=\"controls\"><br>The <code>&lt;video&gt;</code> tag is not supported by your browser.<br></video>\n\n<p>Have fun!</p>\n<p><img src=\"/2020/10/05/2019-06-21-TPmodel/guixianren_s.png\" width=\"50%\" height=\"50%\"></p>\n","site":{"data":{}},"excerpt":"<p>Do you like comics? I am a comics fan, majorly favoring Japanese comics and some Chinese comics with the rise of Chinese comics industry in recent years. Why do I mention this? I mean to say drawings are usually the most straight way to express abstract ideas. This is why I want to show an animation of our model to explain such complex mechanism instead of using tons of papers filled with equations.   </p>","more":"<h1 id=\"An-animation-of-trait-population-coevolution-for-a-single-attraction\"><a href=\"#An-animation-of-trait-population-coevolution-for-a-single-attraction\" class=\"headerlink\" title=\"An animation of trait-population coevolution for a single attraction\"></a>An animation of trait-population coevolution for a single attraction</h1><p>We have constructed a mathematical model to describe how traits of species interplay with their abundance under environmental adaptation and competition along a phylogenetic tree. For simplicity, we assume one attraction in the community that pull traits evolve toward one single optimum trait that better utilize natural resource. In this model, population dynamics weight the species’ competitive power, i.e. the species with large population size gains more competitive power. We would like to see what kind of trait pattern under such mechanism. The animation below shows how it works along a given phylogenetic tree.  </p>\n<video width=\"100%\" height=\"800\" src=\"singlespecies5.mp4\" controls=\"controls\"><br>The <code>&lt;video&gt;</code> tag is not supported by your browser.<br></video>\n\n<p>The video is consisting of two models showing different magnitude of attraction towards the optimum for 5 species. The area of red dots denote the population size of that species.\\(x\\) axis denotes the evolutionary time while \\(y\\) denotes the trait values. The optimum trait is assumed as 0 without loss of generality. The highlighted by green lineage is the destined species giving birth and going extinct. More details are coming after the paper submission.  </p>\n<h1 id=\"An-animation-of-trait-population-coevolution-for-multiple-attractions\"><a href=\"#An-animation-of-trait-population-coevolution-for-multiple-attractions\" class=\"headerlink\" title=\"An animation of trait-population coevolution for multiple attractions\"></a>An animation of trait-population coevolution for multiple attractions</h1><p>In the following video, 15 species are evolving under 5 attractions. You can see how they compete with each other and explore the new niches. </p>\n<video width=\"100%\" height=\"800\" src=\"multi6species15.mp4\" controls=\"controls\"><br>The <code>&lt;video&gt;</code> tag is not supported by your browser.<br></video>\n\n<p>Have fun!</p>\n<p><img src=\"/2020/10/05/2019-06-21-TPmodel/guixianren_s.png\" width=\"50%\" height=\"50%\"></p>"},{"title":"A user-friendly approximate Bayesian computation package in Python with an application on the coronavirus outbreak in the Netherlands","top":1,"_content":"\nApproximate Bayesian computation (ABC) is a kind of likelihood-free method that utilizes computational power to generate a huge amount of simulations with randomly chosen parameters to hit the target - the observations. The generating parameters that produce the results that are most analogous to the observations are recognized as the best inference of the truth. This is analogous to the idea of neural networks of machine learning. The difference is that ABC requests process-based modeling, which bases on knowledge of the underlying mechanism behind the data. In contrast, neural networks neglect such process-based modeling but only needs to construct a neural net structure to train for fitting data. Thus, the advantage of neural networks is that it provides a general inference approach without need of knowing the mechanisms. Nevertheless, the disadvantage is also on it. The interpretation of the processes behind the data is lacking.  This is why it is widely used in pattern recognition but in biological processes interpretation. Both methods have limited structure types although the combination of them can be infinite. Therefore, coding a general framework to fit any type of models is feasible. While Tensorflow of Google and Pytoch of Facebook stand out in machine learning as the examples of such general frameworks, not many groups are focusing on ABC development. Only a recent [package](https://arxiv.org/pdf/1711.04694.pdf) by Dutta et al. in Python is released setting a good example. Here I would like to share a simple version with an application to the recent coronavirus outbreak in the Netherlands. Just for fun and keeping my coding skills warm in the period of quarantine regulation. \n\n<!--more-->\n\n# A brief introduction of the ABC\n\nApproximate Bayesian computation constitutes a family of computational methods with different algorithms that base on Bayesian statistics. An overview of this approach can be found [here](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002803). The following figure from Sunnaker et al. summarizes the idea accurately. <img src=\"2020-03-18-generalABC/abc_scheme.png\" width=\"100%\" height=\"100%\">\n\nNormally, one iteration that constitutes the above mentioned steps is not enough to get a convergent estimation. Hence, taking the posterior distribution of the estimates as the prior distribution in the next iteration to presume the algorithm learning is usually used. This is why ABC like approaches are computational demanding. However, parallel computation can ameliorate it a bit. \n\nIn my toy package, users can prescribe the number of the iterations and the simulations in each iteration. After initializing the model and parameters for the first iteration, parameter inference can be done automatically. The code is [here](https://github.com/xl0418/ABCer). Welcome for comments.\n\n# The coronavirus outbreak in the Netherlands \n\nSince 27th February 2020 the first infected was reported in NL, the coronavirus pandemic has caused over 1700 infections so far.  \n\n<img src=\"2020-03-18-generalABC/data.png\" width=\"70%\" height=\"70%\">\n\nAs an illustration of how this general ABC approach works, I would like to estimate parameters of 2 models and make inference on the future growth of the infected cases. Note that this is only a toy example. Models are very simple and may have analytic solutions. But my purpose here is to show how this ABC package (ABCer) works for different models in a likelihood-free way. \n\n# The usage of the package ABCer \n\nAs aforementioned, with this package you only need to provide the observation data and specify your model, the parameters to be estimated, the structure of the algorithm, i.e. the number of the iterations and the simulations per iteration, and the prior distribution of the parameters. \n\nFor example, from the observation of the coronavirus outbreak and previous experience of pandemic, an exponential growth model is expected to describe the virus infection. \n\n```Python\nfrom ABCer import ABCer\nimport numpy as np\n\n# Model 1\ndef model1(para, time_survey=np.arange(18)):\n    y = para[0] * np.exp(para[1] * time_survey)\n    return y\n```\n\nSo far, I obtained the infection data for 18 days. Hence, I am ganna use these data to compute the similarity between simulations and the observations in ABC algorithm. \n\nNow, we can specify the structure of the ABC, provide the observed data and initialize the model and the parameters in the algorithm. For example, here we chose 50 iterations and 10000 simulations for each iteration.  \n\n```Python\n# The data of the coronavirus outbreak in NL from 27-2-2020 to 17-03-2020\nobservations = np.array([\n    1.0, 7.0, 10.0, 24.0, 38.0, 82.0, 128.0, 188.0, 265.0, 321.0, 382.0, 503.0,\n    614.0, 804.0, 959.0, 1135.0, 1413.0, 1705.0\n])\ntime = np.arange(len(observations))\n\n# Initialize the ABC approach\ntest_ABC1 = ABCer(iterations=50, particles=10000, observations=observations)\ntest_ABC1.initialize_model(model1)\ntest_ABC1.initialize_parameters([0.0, 1.0])\n\n# Launch...\ntest_list1 = test_ABC1.ABC(prior_paras=[0.0, 1.0, 1.0, 2.0])\n```\n\nThe mean of the estimates for each iteration will be printed out. In this example, the values tend to stabilize with the proceeding of the algorithm, indicating convergence achieved. \n\n<img src=\"2020-03-18-generalABC/setup_model1.gif\" width=\"100%\" height=\"100%\">\n\nNow, we can use the inferred parameters to make prediction of the infection.\n\n```python\nimport matplotlib.pyplot as plt\n# The true data\nplt.plot(time, observations, 'o')\n\n# Collect the inferred parameters\npara_inferred = []\npara_inferred.append(np.mean(test_list1[0][20, :]))\npara_inferred.append(np.mean(test_list1[1][20, :]))\n\n# Predict the infection till 21 days\nextend_time = np.arange(21)\ny_inferred = model1(para_inferred, np.arange(21))\n\n# Plot the prediction\nplt.plot(extend_time, y_inferred, 'x', color='r')\nplt.xlabel(\"Days\")\nplt.ylabel('Number of infected cases')\n```\n\nThe red cross denotes the estimate infection. Well, it seems that this model has a lower increase than the observation at the beginning stage but a higher increase than expected later on. This is probably due to the government didn't take any measures at the beginning so that the increase is big. But when it realized how serious the situation is the government started to take action so that the spread of the virus is limited. \n\n<img src=\"2020-03-18-generalABC/Predict1.png\" width=\"70%\" height=\"70%\">\n\n# Another candidate model\n\nNow, we can test any models and estimate their parameters in the same way. Here I tried an additional simple model - a polynomial model - to see if we get a nicer regression.\n\n <img src=\"2020-03-18-generalABC/setup_model2.gif\" width=\"100%\" height=\"100%\">\n\nThe prediction fits the observed data better at the beginning phase. But in the latter phase the infection is underestimated. This means a polynomial model with up to second order cannot fit the data very well. \n\n<img src=\"2020-03-18-generalABC/Predict2.png\" width=\"70%\" height=\"70%\">\n\n# Conclusion\n\nOk, as I said the purpose of this post is to show you how the general ABC approach works with different models in a user-friendly manner. To investigate the best fitting model, a model should consider how the society and the government act accordingly like the quarantine, self-protection behavior, etc. This ABCer package is just a simplest example. More complex structure of the algorithm, distinct summary statistics, the perturbation kernel across iterations can be implemented in the package. Feel free to explore!  \n\n ","source":"_posts/2020-03-18-generalABC.md","raw":"---\ntitle: \"A user-friendly approximate Bayesian computation package in Python with an application on the coronavirus outbreak in the Netherlands\"\ntop: 1\ncategories: [Research,Algorithm, ABC]\ntags: [Python, ABC, algorithm]\n\n---\n\nApproximate Bayesian computation (ABC) is a kind of likelihood-free method that utilizes computational power to generate a huge amount of simulations with randomly chosen parameters to hit the target - the observations. The generating parameters that produce the results that are most analogous to the observations are recognized as the best inference of the truth. This is analogous to the idea of neural networks of machine learning. The difference is that ABC requests process-based modeling, which bases on knowledge of the underlying mechanism behind the data. In contrast, neural networks neglect such process-based modeling but only needs to construct a neural net structure to train for fitting data. Thus, the advantage of neural networks is that it provides a general inference approach without need of knowing the mechanisms. Nevertheless, the disadvantage is also on it. The interpretation of the processes behind the data is lacking.  This is why it is widely used in pattern recognition but in biological processes interpretation. Both methods have limited structure types although the combination of them can be infinite. Therefore, coding a general framework to fit any type of models is feasible. While Tensorflow of Google and Pytoch of Facebook stand out in machine learning as the examples of such general frameworks, not many groups are focusing on ABC development. Only a recent [package](https://arxiv.org/pdf/1711.04694.pdf) by Dutta et al. in Python is released setting a good example. Here I would like to share a simple version with an application to the recent coronavirus outbreak in the Netherlands. Just for fun and keeping my coding skills warm in the period of quarantine regulation. \n\n<!--more-->\n\n# A brief introduction of the ABC\n\nApproximate Bayesian computation constitutes a family of computational methods with different algorithms that base on Bayesian statistics. An overview of this approach can be found [here](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002803). The following figure from Sunnaker et al. summarizes the idea accurately. <img src=\"2020-03-18-generalABC/abc_scheme.png\" width=\"100%\" height=\"100%\">\n\nNormally, one iteration that constitutes the above mentioned steps is not enough to get a convergent estimation. Hence, taking the posterior distribution of the estimates as the prior distribution in the next iteration to presume the algorithm learning is usually used. This is why ABC like approaches are computational demanding. However, parallel computation can ameliorate it a bit. \n\nIn my toy package, users can prescribe the number of the iterations and the simulations in each iteration. After initializing the model and parameters for the first iteration, parameter inference can be done automatically. The code is [here](https://github.com/xl0418/ABCer). Welcome for comments.\n\n# The coronavirus outbreak in the Netherlands \n\nSince 27th February 2020 the first infected was reported in NL, the coronavirus pandemic has caused over 1700 infections so far.  \n\n<img src=\"2020-03-18-generalABC/data.png\" width=\"70%\" height=\"70%\">\n\nAs an illustration of how this general ABC approach works, I would like to estimate parameters of 2 models and make inference on the future growth of the infected cases. Note that this is only a toy example. Models are very simple and may have analytic solutions. But my purpose here is to show how this ABC package (ABCer) works for different models in a likelihood-free way. \n\n# The usage of the package ABCer \n\nAs aforementioned, with this package you only need to provide the observation data and specify your model, the parameters to be estimated, the structure of the algorithm, i.e. the number of the iterations and the simulations per iteration, and the prior distribution of the parameters. \n\nFor example, from the observation of the coronavirus outbreak and previous experience of pandemic, an exponential growth model is expected to describe the virus infection. \n\n```Python\nfrom ABCer import ABCer\nimport numpy as np\n\n# Model 1\ndef model1(para, time_survey=np.arange(18)):\n    y = para[0] * np.exp(para[1] * time_survey)\n    return y\n```\n\nSo far, I obtained the infection data for 18 days. Hence, I am ganna use these data to compute the similarity between simulations and the observations in ABC algorithm. \n\nNow, we can specify the structure of the ABC, provide the observed data and initialize the model and the parameters in the algorithm. For example, here we chose 50 iterations and 10000 simulations for each iteration.  \n\n```Python\n# The data of the coronavirus outbreak in NL from 27-2-2020 to 17-03-2020\nobservations = np.array([\n    1.0, 7.0, 10.0, 24.0, 38.0, 82.0, 128.0, 188.0, 265.0, 321.0, 382.0, 503.0,\n    614.0, 804.0, 959.0, 1135.0, 1413.0, 1705.0\n])\ntime = np.arange(len(observations))\n\n# Initialize the ABC approach\ntest_ABC1 = ABCer(iterations=50, particles=10000, observations=observations)\ntest_ABC1.initialize_model(model1)\ntest_ABC1.initialize_parameters([0.0, 1.0])\n\n# Launch...\ntest_list1 = test_ABC1.ABC(prior_paras=[0.0, 1.0, 1.0, 2.0])\n```\n\nThe mean of the estimates for each iteration will be printed out. In this example, the values tend to stabilize with the proceeding of the algorithm, indicating convergence achieved. \n\n<img src=\"2020-03-18-generalABC/setup_model1.gif\" width=\"100%\" height=\"100%\">\n\nNow, we can use the inferred parameters to make prediction of the infection.\n\n```python\nimport matplotlib.pyplot as plt\n# The true data\nplt.plot(time, observations, 'o')\n\n# Collect the inferred parameters\npara_inferred = []\npara_inferred.append(np.mean(test_list1[0][20, :]))\npara_inferred.append(np.mean(test_list1[1][20, :]))\n\n# Predict the infection till 21 days\nextend_time = np.arange(21)\ny_inferred = model1(para_inferred, np.arange(21))\n\n# Plot the prediction\nplt.plot(extend_time, y_inferred, 'x', color='r')\nplt.xlabel(\"Days\")\nplt.ylabel('Number of infected cases')\n```\n\nThe red cross denotes the estimate infection. Well, it seems that this model has a lower increase than the observation at the beginning stage but a higher increase than expected later on. This is probably due to the government didn't take any measures at the beginning so that the increase is big. But when it realized how serious the situation is the government started to take action so that the spread of the virus is limited. \n\n<img src=\"2020-03-18-generalABC/Predict1.png\" width=\"70%\" height=\"70%\">\n\n# Another candidate model\n\nNow, we can test any models and estimate their parameters in the same way. Here I tried an additional simple model - a polynomial model - to see if we get a nicer regression.\n\n <img src=\"2020-03-18-generalABC/setup_model2.gif\" width=\"100%\" height=\"100%\">\n\nThe prediction fits the observed data better at the beginning phase. But in the latter phase the infection is underestimated. This means a polynomial model with up to second order cannot fit the data very well. \n\n<img src=\"2020-03-18-generalABC/Predict2.png\" width=\"70%\" height=\"70%\">\n\n# Conclusion\n\nOk, as I said the purpose of this post is to show you how the general ABC approach works with different models in a user-friendly manner. To investigate the best fitting model, a model should consider how the society and the government act accordingly like the quarantine, self-protection behavior, etc. This ABCer package is just a simplest example. More complex structure of the algorithm, distinct summary statistics, the perturbation kernel across iterations can be implemented in the package. Feel free to explore!  \n\n ","slug":"2020-03-18-generalABC","published":1,"date":"2020-10-05T11:30:59.188Z","updated":"2020-10-05T11:30:59.189Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy13m001knj393x83akkb","content":"<p>Approximate Bayesian computation (ABC) is a kind of likelihood-free method that utilizes computational power to generate a huge amount of simulations with randomly chosen parameters to hit the target - the observations. The generating parameters that produce the results that are most analogous to the observations are recognized as the best inference of the truth. This is analogous to the idea of neural networks of machine learning. The difference is that ABC requests process-based modeling, which bases on knowledge of the underlying mechanism behind the data. In contrast, neural networks neglect such process-based modeling but only needs to construct a neural net structure to train for fitting data. Thus, the advantage of neural networks is that it provides a general inference approach without need of knowing the mechanisms. Nevertheless, the disadvantage is also on it. The interpretation of the processes behind the data is lacking.  This is why it is widely used in pattern recognition but in biological processes interpretation. Both methods have limited structure types although the combination of them can be infinite. Therefore, coding a general framework to fit any type of models is feasible. While Tensorflow of Google and Pytoch of Facebook stand out in machine learning as the examples of such general frameworks, not many groups are focusing on ABC development. Only a recent <a href=\"https://arxiv.org/pdf/1711.04694.pdf\" target=\"_blank\" rel=\"noopener\">package</a> by Dutta et al. in Python is released setting a good example. Here I would like to share a simple version with an application to the recent coronavirus outbreak in the Netherlands. Just for fun and keeping my coding skills warm in the period of quarantine regulation. </p>\n<a id=\"more\"></a>\n<h1 id=\"A-brief-introduction-of-the-ABC\"><a href=\"#A-brief-introduction-of-the-ABC\" class=\"headerlink\" title=\"A brief introduction of the ABC\"></a>A brief introduction of the ABC</h1><p>Approximate Bayesian computation constitutes a family of computational methods with different algorithms that base on Bayesian statistics. An overview of this approach can be found <a href=\"https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002803\" target=\"_blank\" rel=\"noopener\">here</a>. The following figure from Sunnaker et al. summarizes the idea accurately. <img src=\"/2020/10/05/2020-03-18-generalABC/abc_scheme.png\" width=\"100%\" height=\"100%\"></p>\n<p>Normally, one iteration that constitutes the above mentioned steps is not enough to get a convergent estimation. Hence, taking the posterior distribution of the estimates as the prior distribution in the next iteration to presume the algorithm learning is usually used. This is why ABC like approaches are computational demanding. However, parallel computation can ameliorate it a bit. </p>\n<p>In my toy package, users can prescribe the number of the iterations and the simulations in each iteration. After initializing the model and parameters for the first iteration, parameter inference can be done automatically. The code is <a href=\"https://github.com/xl0418/ABCer\" target=\"_blank\" rel=\"noopener\">here</a>. Welcome for comments.</p>\n<h1 id=\"The-coronavirus-outbreak-in-the-Netherlands\"><a href=\"#The-coronavirus-outbreak-in-the-Netherlands\" class=\"headerlink\" title=\"The coronavirus outbreak in the Netherlands\"></a>The coronavirus outbreak in the Netherlands</h1><p>Since 27th February 2020 the first infected was reported in NL, the coronavirus pandemic has caused over 1700 infections so far.  </p>\n<p><img src=\"/2020/10/05/2020-03-18-generalABC/data.png\" width=\"70%\" height=\"70%\"></p>\n<p>As an illustration of how this general ABC approach works, I would like to estimate parameters of 2 models and make inference on the future growth of the infected cases. Note that this is only a toy example. Models are very simple and may have analytic solutions. But my purpose here is to show how this ABC package (ABCer) works for different models in a likelihood-free way. </p>\n<h1 id=\"The-usage-of-the-package-ABCer\"><a href=\"#The-usage-of-the-package-ABCer\" class=\"headerlink\" title=\"The usage of the package ABCer\"></a>The usage of the package ABCer</h1><p>As aforementioned, with this package you only need to provide the observation data and specify your model, the parameters to be estimated, the structure of the algorithm, i.e. the number of the iterations and the simulations per iteration, and the prior distribution of the parameters. </p>\n<p>For example, from the observation of the coronavirus outbreak and previous experience of pandemic, an exponential growth model is expected to describe the virus infection. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> ABCer <span class=\"keyword\">import</span> ABCer</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Model 1</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">model1</span><span class=\"params\">(para, time_survey=np.arange<span class=\"params\">(<span class=\"number\">18</span>)</span>)</span>:</span></span><br><span class=\"line\">    y = para[<span class=\"number\">0</span>] * np.exp(para[<span class=\"number\">1</span>] * time_survey)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> y</span><br></pre></td></tr></table></figure>\n<p>So far, I obtained the infection data for 18 days. Hence, I am ganna use these data to compute the similarity between simulations and the observations in ABC algorithm. </p>\n<p>Now, we can specify the structure of the ABC, provide the observed data and initialize the model and the parameters in the algorithm. For example, here we chose 50 iterations and 10000 simulations for each iteration.  </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># The data of the coronavirus outbreak in NL from 27-2-2020 to 17-03-2020</span></span><br><span class=\"line\">observations = np.array([</span><br><span class=\"line\">    <span class=\"number\">1.0</span>, <span class=\"number\">7.0</span>, <span class=\"number\">10.0</span>, <span class=\"number\">24.0</span>, <span class=\"number\">38.0</span>, <span class=\"number\">82.0</span>, <span class=\"number\">128.0</span>, <span class=\"number\">188.0</span>, <span class=\"number\">265.0</span>, <span class=\"number\">321.0</span>, <span class=\"number\">382.0</span>, <span class=\"number\">503.0</span>,</span><br><span class=\"line\">    <span class=\"number\">614.0</span>, <span class=\"number\">804.0</span>, <span class=\"number\">959.0</span>, <span class=\"number\">1135.0</span>, <span class=\"number\">1413.0</span>, <span class=\"number\">1705.0</span></span><br><span class=\"line\">])</span><br><span class=\"line\">time = np.arange(len(observations))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Initialize the ABC approach</span></span><br><span class=\"line\">test_ABC1 = ABCer(iterations=<span class=\"number\">50</span>, particles=<span class=\"number\">10000</span>, observations=observations)</span><br><span class=\"line\">test_ABC1.initialize_model(model1)</span><br><span class=\"line\">test_ABC1.initialize_parameters([<span class=\"number\">0.0</span>, <span class=\"number\">1.0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Launch...</span></span><br><span class=\"line\">test_list1 = test_ABC1.ABC(prior_paras=[<span class=\"number\">0.0</span>, <span class=\"number\">1.0</span>, <span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>])</span><br></pre></td></tr></table></figure>\n<p>The mean of the estimates for each iteration will be printed out. In this example, the values tend to stabilize with the proceeding of the algorithm, indicating convergence achieved. </p>\n<p><img src=\"/2020/10/05/2020-03-18-generalABC/setup_model1.gif\" width=\"100%\" height=\"100%\"></p>\n<p>Now, we can use the inferred parameters to make prediction of the infection.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"comment\"># The true data</span></span><br><span class=\"line\">plt.plot(time, observations, <span class=\"string\">'o'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Collect the inferred parameters</span></span><br><span class=\"line\">para_inferred = []</span><br><span class=\"line\">para_inferred.append(np.mean(test_list1[<span class=\"number\">0</span>][<span class=\"number\">20</span>, :]))</span><br><span class=\"line\">para_inferred.append(np.mean(test_list1[<span class=\"number\">1</span>][<span class=\"number\">20</span>, :]))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Predict the infection till 21 days</span></span><br><span class=\"line\">extend_time = np.arange(<span class=\"number\">21</span>)</span><br><span class=\"line\">y_inferred = model1(para_inferred, np.arange(<span class=\"number\">21</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Plot the prediction</span></span><br><span class=\"line\">plt.plot(extend_time, y_inferred, <span class=\"string\">'x'</span>, color=<span class=\"string\">'r'</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">\"Days\"</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'Number of infected cases'</span>)</span><br></pre></td></tr></table></figure>\n<p>The red cross denotes the estimate infection. Well, it seems that this model has a lower increase than the observation at the beginning stage but a higher increase than expected later on. This is probably due to the government didn’t take any measures at the beginning so that the increase is big. But when it realized how serious the situation is the government started to take action so that the spread of the virus is limited. </p>\n<p><img src=\"/2020/10/05/2020-03-18-generalABC/Predict1.png\" width=\"70%\" height=\"70%\"></p>\n<h1 id=\"Another-candidate-model\"><a href=\"#Another-candidate-model\" class=\"headerlink\" title=\"Another candidate model\"></a>Another candidate model</h1><p>Now, we can test any models and estimate their parameters in the same way. Here I tried an additional simple model - a polynomial model - to see if we get a nicer regression.</p>\n<p> <img src=\"/2020/10/05/2020-03-18-generalABC/setup_model2.gif\" width=\"100%\" height=\"100%\"></p>\n<p>The prediction fits the observed data better at the beginning phase. But in the latter phase the infection is underestimated. This means a polynomial model with up to second order cannot fit the data very well. </p>\n<p><img src=\"/2020/10/05/2020-03-18-generalABC/Predict2.png\" width=\"70%\" height=\"70%\"></p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>Ok, as I said the purpose of this post is to show you how the general ABC approach works with different models in a user-friendly manner. To investigate the best fitting model, a model should consider how the society and the government act accordingly like the quarantine, self-protection behavior, etc. This ABCer package is just a simplest example. More complex structure of the algorithm, distinct summary statistics, the perturbation kernel across iterations can be implemented in the package. Feel free to explore!  </p>\n","site":{"data":{}},"excerpt":"<p>Approximate Bayesian computation (ABC) is a kind of likelihood-free method that utilizes computational power to generate a huge amount of simulations with randomly chosen parameters to hit the target - the observations. The generating parameters that produce the results that are most analogous to the observations are recognized as the best inference of the truth. This is analogous to the idea of neural networks of machine learning. The difference is that ABC requests process-based modeling, which bases on knowledge of the underlying mechanism behind the data. In contrast, neural networks neglect such process-based modeling but only needs to construct a neural net structure to train for fitting data. Thus, the advantage of neural networks is that it provides a general inference approach without need of knowing the mechanisms. Nevertheless, the disadvantage is also on it. The interpretation of the processes behind the data is lacking.  This is why it is widely used in pattern recognition but in biological processes interpretation. Both methods have limited structure types although the combination of them can be infinite. Therefore, coding a general framework to fit any type of models is feasible. While Tensorflow of Google and Pytoch of Facebook stand out in machine learning as the examples of such general frameworks, not many groups are focusing on ABC development. Only a recent <a href=\"https://arxiv.org/pdf/1711.04694.pdf\" target=\"_blank\" rel=\"noopener\">package</a> by Dutta et al. in Python is released setting a good example. Here I would like to share a simple version with an application to the recent coronavirus outbreak in the Netherlands. Just for fun and keeping my coding skills warm in the period of quarantine regulation. </p>","more":"<h1 id=\"A-brief-introduction-of-the-ABC\"><a href=\"#A-brief-introduction-of-the-ABC\" class=\"headerlink\" title=\"A brief introduction of the ABC\"></a>A brief introduction of the ABC</h1><p>Approximate Bayesian computation constitutes a family of computational methods with different algorithms that base on Bayesian statistics. An overview of this approach can be found <a href=\"https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002803\" target=\"_blank\" rel=\"noopener\">here</a>. The following figure from Sunnaker et al. summarizes the idea accurately. <img src=\"/2020/10/05/2020-03-18-generalABC/abc_scheme.png\" width=\"100%\" height=\"100%\"></p>\n<p>Normally, one iteration that constitutes the above mentioned steps is not enough to get a convergent estimation. Hence, taking the posterior distribution of the estimates as the prior distribution in the next iteration to presume the algorithm learning is usually used. This is why ABC like approaches are computational demanding. However, parallel computation can ameliorate it a bit. </p>\n<p>In my toy package, users can prescribe the number of the iterations and the simulations in each iteration. After initializing the model and parameters for the first iteration, parameter inference can be done automatically. The code is <a href=\"https://github.com/xl0418/ABCer\" target=\"_blank\" rel=\"noopener\">here</a>. Welcome for comments.</p>\n<h1 id=\"The-coronavirus-outbreak-in-the-Netherlands\"><a href=\"#The-coronavirus-outbreak-in-the-Netherlands\" class=\"headerlink\" title=\"The coronavirus outbreak in the Netherlands\"></a>The coronavirus outbreak in the Netherlands</h1><p>Since 27th February 2020 the first infected was reported in NL, the coronavirus pandemic has caused over 1700 infections so far.  </p>\n<p><img src=\"/2020/10/05/2020-03-18-generalABC/data.png\" width=\"70%\" height=\"70%\"></p>\n<p>As an illustration of how this general ABC approach works, I would like to estimate parameters of 2 models and make inference on the future growth of the infected cases. Note that this is only a toy example. Models are very simple and may have analytic solutions. But my purpose here is to show how this ABC package (ABCer) works for different models in a likelihood-free way. </p>\n<h1 id=\"The-usage-of-the-package-ABCer\"><a href=\"#The-usage-of-the-package-ABCer\" class=\"headerlink\" title=\"The usage of the package ABCer\"></a>The usage of the package ABCer</h1><p>As aforementioned, with this package you only need to provide the observation data and specify your model, the parameters to be estimated, the structure of the algorithm, i.e. the number of the iterations and the simulations per iteration, and the prior distribution of the parameters. </p>\n<p>For example, from the observation of the coronavirus outbreak and previous experience of pandemic, an exponential growth model is expected to describe the virus infection. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> ABCer <span class=\"keyword\">import</span> ABCer</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Model 1</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">model1</span><span class=\"params\">(para, time_survey=np.arange<span class=\"params\">(<span class=\"number\">18</span>)</span>)</span>:</span></span><br><span class=\"line\">    y = para[<span class=\"number\">0</span>] * np.exp(para[<span class=\"number\">1</span>] * time_survey)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> y</span><br></pre></td></tr></table></figure>\n<p>So far, I obtained the infection data for 18 days. Hence, I am ganna use these data to compute the similarity between simulations and the observations in ABC algorithm. </p>\n<p>Now, we can specify the structure of the ABC, provide the observed data and initialize the model and the parameters in the algorithm. For example, here we chose 50 iterations and 10000 simulations for each iteration.  </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># The data of the coronavirus outbreak in NL from 27-2-2020 to 17-03-2020</span></span><br><span class=\"line\">observations = np.array([</span><br><span class=\"line\">    <span class=\"number\">1.0</span>, <span class=\"number\">7.0</span>, <span class=\"number\">10.0</span>, <span class=\"number\">24.0</span>, <span class=\"number\">38.0</span>, <span class=\"number\">82.0</span>, <span class=\"number\">128.0</span>, <span class=\"number\">188.0</span>, <span class=\"number\">265.0</span>, <span class=\"number\">321.0</span>, <span class=\"number\">382.0</span>, <span class=\"number\">503.0</span>,</span><br><span class=\"line\">    <span class=\"number\">614.0</span>, <span class=\"number\">804.0</span>, <span class=\"number\">959.0</span>, <span class=\"number\">1135.0</span>, <span class=\"number\">1413.0</span>, <span class=\"number\">1705.0</span></span><br><span class=\"line\">])</span><br><span class=\"line\">time = np.arange(len(observations))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Initialize the ABC approach</span></span><br><span class=\"line\">test_ABC1 = ABCer(iterations=<span class=\"number\">50</span>, particles=<span class=\"number\">10000</span>, observations=observations)</span><br><span class=\"line\">test_ABC1.initialize_model(model1)</span><br><span class=\"line\">test_ABC1.initialize_parameters([<span class=\"number\">0.0</span>, <span class=\"number\">1.0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Launch...</span></span><br><span class=\"line\">test_list1 = test_ABC1.ABC(prior_paras=[<span class=\"number\">0.0</span>, <span class=\"number\">1.0</span>, <span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>])</span><br></pre></td></tr></table></figure>\n<p>The mean of the estimates for each iteration will be printed out. In this example, the values tend to stabilize with the proceeding of the algorithm, indicating convergence achieved. </p>\n<p><img src=\"/2020/10/05/2020-03-18-generalABC/setup_model1.gif\" width=\"100%\" height=\"100%\"></p>\n<p>Now, we can use the inferred parameters to make prediction of the infection.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"comment\"># The true data</span></span><br><span class=\"line\">plt.plot(time, observations, <span class=\"string\">'o'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Collect the inferred parameters</span></span><br><span class=\"line\">para_inferred = []</span><br><span class=\"line\">para_inferred.append(np.mean(test_list1[<span class=\"number\">0</span>][<span class=\"number\">20</span>, :]))</span><br><span class=\"line\">para_inferred.append(np.mean(test_list1[<span class=\"number\">1</span>][<span class=\"number\">20</span>, :]))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Predict the infection till 21 days</span></span><br><span class=\"line\">extend_time = np.arange(<span class=\"number\">21</span>)</span><br><span class=\"line\">y_inferred = model1(para_inferred, np.arange(<span class=\"number\">21</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Plot the prediction</span></span><br><span class=\"line\">plt.plot(extend_time, y_inferred, <span class=\"string\">'x'</span>, color=<span class=\"string\">'r'</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">\"Days\"</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'Number of infected cases'</span>)</span><br></pre></td></tr></table></figure>\n<p>The red cross denotes the estimate infection. Well, it seems that this model has a lower increase than the observation at the beginning stage but a higher increase than expected later on. This is probably due to the government didn’t take any measures at the beginning so that the increase is big. But when it realized how serious the situation is the government started to take action so that the spread of the virus is limited. </p>\n<p><img src=\"/2020/10/05/2020-03-18-generalABC/Predict1.png\" width=\"70%\" height=\"70%\"></p>\n<h1 id=\"Another-candidate-model\"><a href=\"#Another-candidate-model\" class=\"headerlink\" title=\"Another candidate model\"></a>Another candidate model</h1><p>Now, we can test any models and estimate their parameters in the same way. Here I tried an additional simple model - a polynomial model - to see if we get a nicer regression.</p>\n<p> <img src=\"/2020/10/05/2020-03-18-generalABC/setup_model2.gif\" width=\"100%\" height=\"100%\"></p>\n<p>The prediction fits the observed data better at the beginning phase. But in the latter phase the infection is underestimated. This means a polynomial model with up to second order cannot fit the data very well. </p>\n<p><img src=\"/2020/10/05/2020-03-18-generalABC/Predict2.png\" width=\"70%\" height=\"70%\"></p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>Ok, as I said the purpose of this post is to show you how the general ABC approach works with different models in a user-friendly manner. To investigate the best fitting model, a model should consider how the society and the government act accordingly like the quarantine, self-protection behavior, etc. This ABCer package is just a simplest example. More complex structure of the algorithm, distinct summary statistics, the perturbation kernel across iterations can be implemented in the package. Feel free to explore!  </p>"},{"title":"The government should take a fast reaction to prevent COVID-19 development","_content":"\nThe outbreak of the COVID-19 virus has given the world a heavy punch in 2020. The growth infection over 30 countries in the world has seen the overwhelming power of the exponential spread of COVID-19 at the early stage. The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle) to prepare the [COVID-19 Open Research Dataset (CORD-19)](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) to attempt to address [key open scientific questions on COVID-19](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks). Those questions are drawn from [National Academies of Sciences, Engineering, and Medicine’s (NASEM)](https://www.nationalacademies.org/event/03-11-2020/standing-committee-on-emerging-infectious-diseases-and-21st-century-health-threats-virtual-meeting-1) and the [World Health Organization (WHO)](https://www.who.int/blueprint/priority-diseases/key-action/Global_Research_Forum_FINAL_VERSION_for_web_14_feb_2020.pdf?ua=1). Flooded by the daily news of how serious the pandemic becomes, I am wondering whether and to what extent the government can stop the pandemic. Specifically, if the government takes a fast response to restrict social activities, will the pandemic be stopped at an early stage? Thus, I took part in one of the competition and built an individual-based virus spread model in which I considered how the speed of the government reaction affects pandemic development. It is an one-week competition that finished in this morning. I create this post by attaching my work [**here**](https://xl0418.github.io/Kaggle_corona/). Enjoy!\n\n","source":"_posts/2020-04-02-government&pandemic.md","raw":"---\ntitle: \"The government should take a fast reaction to prevent COVID-19 development\"\ncategories: [Research,Modeling]\ntags: [Python, simulation, coronavirus, pandemic, government measure]\n\n---\n\nThe outbreak of the COVID-19 virus has given the world a heavy punch in 2020. The growth infection over 30 countries in the world has seen the overwhelming power of the exponential spread of COVID-19 at the early stage. The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle) to prepare the [COVID-19 Open Research Dataset (CORD-19)](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) to attempt to address [key open scientific questions on COVID-19](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks). Those questions are drawn from [National Academies of Sciences, Engineering, and Medicine’s (NASEM)](https://www.nationalacademies.org/event/03-11-2020/standing-committee-on-emerging-infectious-diseases-and-21st-century-health-threats-virtual-meeting-1) and the [World Health Organization (WHO)](https://www.who.int/blueprint/priority-diseases/key-action/Global_Research_Forum_FINAL_VERSION_for_web_14_feb_2020.pdf?ua=1). Flooded by the daily news of how serious the pandemic becomes, I am wondering whether and to what extent the government can stop the pandemic. Specifically, if the government takes a fast response to restrict social activities, will the pandemic be stopped at an early stage? Thus, I took part in one of the competition and built an individual-based virus spread model in which I considered how the speed of the government reaction affects pandemic development. It is an one-week competition that finished in this morning. I create this post by attaching my work [**here**](https://xl0418.github.io/Kaggle_corona/). Enjoy!\n\n","slug":"2020-04-02-government&pandemic","published":1,"date":"2020-10-05T11:30:59.199Z","updated":"2020-10-05T11:30:59.283Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy13n001nnj39fv3hbbuo","content":"<p>The outbreak of the COVID-19 virus has given the world a heavy punch in 2020. The growth infection over 30 countries in the world has seen the overwhelming power of the exponential spread of COVID-19 at the early stage. The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle) to prepare the <a href=\"https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge\" target=\"_blank\" rel=\"noopener\">COVID-19 Open Research Dataset (CORD-19)</a> to attempt to address <a href=\"https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks\" target=\"_blank\" rel=\"noopener\">key open scientific questions on COVID-19</a>. Those questions are drawn from <a href=\"https://www.nationalacademies.org/event/03-11-2020/standing-committee-on-emerging-infectious-diseases-and-21st-century-health-threats-virtual-meeting-1\" target=\"_blank\" rel=\"noopener\">National Academies of Sciences, Engineering, and Medicine’s (NASEM)</a> and the <a href=\"https://www.who.int/blueprint/priority-diseases/key-action/Global_Research_Forum_FINAL_VERSION_for_web_14_feb_2020.pdf?ua=1\" target=\"_blank\" rel=\"noopener\">World Health Organization (WHO)</a>. Flooded by the daily news of how serious the pandemic becomes, I am wondering whether and to what extent the government can stop the pandemic. Specifically, if the government takes a fast response to restrict social activities, will the pandemic be stopped at an early stage? Thus, I took part in one of the competition and built an individual-based virus spread model in which I considered how the speed of the government reaction affects pandemic development. It is an one-week competition that finished in this morning. I create this post by attaching my work <a href=\"https://xl0418.github.io/Kaggle_corona/\"><strong>here</strong></a>. Enjoy!</p>\n","site":{"data":{}},"excerpt":"","more":"<p>The outbreak of the COVID-19 virus has given the world a heavy punch in 2020. The growth infection over 30 countries in the world has seen the overwhelming power of the exponential spread of COVID-19 at the early stage. The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle) to prepare the <a href=\"https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge\" target=\"_blank\" rel=\"noopener\">COVID-19 Open Research Dataset (CORD-19)</a> to attempt to address <a href=\"https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks\" target=\"_blank\" rel=\"noopener\">key open scientific questions on COVID-19</a>. Those questions are drawn from <a href=\"https://www.nationalacademies.org/event/03-11-2020/standing-committee-on-emerging-infectious-diseases-and-21st-century-health-threats-virtual-meeting-1\" target=\"_blank\" rel=\"noopener\">National Academies of Sciences, Engineering, and Medicine’s (NASEM)</a> and the <a href=\"https://www.who.int/blueprint/priority-diseases/key-action/Global_Research_Forum_FINAL_VERSION_for_web_14_feb_2020.pdf?ua=1\" target=\"_blank\" rel=\"noopener\">World Health Organization (WHO)</a>. Flooded by the daily news of how serious the pandemic becomes, I am wondering whether and to what extent the government can stop the pandemic. Specifically, if the government takes a fast response to restrict social activities, will the pandemic be stopped at an early stage? Thus, I took part in one of the competition and built an individual-based virus spread model in which I considered how the speed of the government reaction affects pandemic development. It is an one-week competition that finished in this morning. I create this post by attaching my work <a href=\"https://xl0418.github.io/Kaggle_corona/\"><strong>here</strong></a>. Enjoy!</p>\n"},{"title":"Data visualization: develop a Shiny app to track COVID-19 spread.","_content":"\nThe outbreak of the COVID-19 virus has given me a heavy punch. I have been isolated at home for almost two months with one trip to the nearest trash bin every week. What is even worse is that I cannot get any reply from job hunting. Being so stressed, I decided to develop a Shiny app to track the COVID-19 spread. Hope this may give me a sign of recovery of the world by examining daily data. More information can be found [**here**](https://liangxu-groningen.shinyapps.io/corona_shiny/). A differential equation model is still under construction. Coming soon. Enjoy!\n\n","source":"_posts/2020-04-25-datavisulization_shinyapp.md","raw":"---\ntitle: \"Data visualization: develop a Shiny app to track COVID-19 spread.\"\ncategories: [Research, Modeling]\ntags: [R, simulation, coronavirus, pandemic, data visualization]\n\n---\n\nThe outbreak of the COVID-19 virus has given me a heavy punch. I have been isolated at home for almost two months with one trip to the nearest trash bin every week. What is even worse is that I cannot get any reply from job hunting. Being so stressed, I decided to develop a Shiny app to track the COVID-19 spread. Hope this may give me a sign of recovery of the world by examining daily data. More information can be found [**here**](https://liangxu-groningen.shinyapps.io/corona_shiny/). A differential equation model is still under construction. Coming soon. Enjoy!\n\n","slug":"2020-04-25-datavisulization_shinyapp","published":1,"date":"2020-10-05T11:30:59.301Z","updated":"2020-10-05T11:30:59.301Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy13o001qnj3908vgmv9l","content":"<p>The outbreak of the COVID-19 virus has given me a heavy punch. I have been isolated at home for almost two months with one trip to the nearest trash bin every week. What is even worse is that I cannot get any reply from job hunting. Being so stressed, I decided to develop a Shiny app to track the COVID-19 spread. Hope this may give me a sign of recovery of the world by examining daily data. More information can be found <a href=\"https://liangxu-groningen.shinyapps.io/corona_shiny/\" target=\"_blank\" rel=\"noopener\"><strong>here</strong></a>. A differential equation model is still under construction. Coming soon. Enjoy!</p>\n","site":{"data":{}},"excerpt":"","more":"<p>The outbreak of the COVID-19 virus has given me a heavy punch. I have been isolated at home for almost two months with one trip to the nearest trash bin every week. What is even worse is that I cannot get any reply from job hunting. Being so stressed, I decided to develop a Shiny app to track the COVID-19 spread. Hope this may give me a sign of recovery of the world by examining daily data. More information can be found <a href=\"https://liangxu-groningen.shinyapps.io/corona_shiny/\" target=\"_blank\" rel=\"noopener\"><strong>here</strong></a>. A differential equation model is still under construction. Coming soon. Enjoy!</p>\n"},{"title":"Hello World","_content":"Welcome to [Liang&apos;s blog](https://xl0418.github.io/)! This site is my first blog for sharing my work experience plus a little bit life. \n\n## Categories\n\n### Research\nThis category mainly contains my PhD projects so far. More details can be found on my [Research Gate](https://www.researchgate.net/profile/Liang_Xu50). \n\n### BB life\nSome interesting stuff about life. **BB**, in Chinese, means **talk about**, **chat**. \n\n### Work\nNot on work yet, although 3 years ago I was a lecturer in China. \n\n### Blogging\nInteresting technologies about blogging. \n\nAt last, enjoy the blog. Hope you can find something useful. Or at least have fun :-)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\ncategories: [BB life]\ntags: [info]\n---\nWelcome to [Liang&apos;s blog](https://xl0418.github.io/)! This site is my first blog for sharing my work experience plus a little bit life. \n\n## Categories\n\n### Research\nThis category mainly contains my PhD projects so far. More details can be found on my [Research Gate](https://www.researchgate.net/profile/Liang_Xu50). \n\n### BB life\nSome interesting stuff about life. **BB**, in Chinese, means **talk about**, **chat**. \n\n### Work\nNot on work yet, although 3 years ago I was a lecturer in China. \n\n### Blogging\nInteresting technologies about blogging. \n\nAt last, enjoy the blog. Hope you can find something useful. Or at least have fun :-)\n","slug":"hello-world","published":1,"date":"2020-10-05T11:30:59.301Z","updated":"2020-10-05T11:30:59.302Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckfwgy13p001snj39osf3vifl","content":"<p>Welcome to <a href=\"https://xl0418.github.io/\">Liang&apos;s blog</a>! This site is my first blog for sharing my work experience plus a little bit life. </p>\n<h2 id=\"Categories\"><a href=\"#Categories\" class=\"headerlink\" title=\"Categories\"></a>Categories</h2><h3 id=\"Research\"><a href=\"#Research\" class=\"headerlink\" title=\"Research\"></a>Research</h3><p>This category mainly contains my PhD projects so far. More details can be found on my <a href=\"https://www.researchgate.net/profile/Liang_Xu50\" target=\"_blank\" rel=\"noopener\">Research Gate</a>. </p>\n<h3 id=\"BB-life\"><a href=\"#BB-life\" class=\"headerlink\" title=\"BB life\"></a>BB life</h3><p>Some interesting stuff about life. <strong>BB</strong>, in Chinese, means <strong>talk about</strong>, <strong>chat</strong>. </p>\n<h3 id=\"Work\"><a href=\"#Work\" class=\"headerlink\" title=\"Work\"></a>Work</h3><p>Not on work yet, although 3 years ago I was a lecturer in China. </p>\n<h3 id=\"Blogging\"><a href=\"#Blogging\" class=\"headerlink\" title=\"Blogging\"></a>Blogging</h3><p>Interesting technologies about blogging. </p>\n<p>At last, enjoy the blog. Hope you can find something useful. Or at least have fun :-)</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Welcome to <a href=\"https://xl0418.github.io/\">Liang&apos;s blog</a>! This site is my first blog for sharing my work experience plus a little bit life. </p>\n<h2 id=\"Categories\"><a href=\"#Categories\" class=\"headerlink\" title=\"Categories\"></a>Categories</h2><h3 id=\"Research\"><a href=\"#Research\" class=\"headerlink\" title=\"Research\"></a>Research</h3><p>This category mainly contains my PhD projects so far. More details can be found on my <a href=\"https://www.researchgate.net/profile/Liang_Xu50\" target=\"_blank\" rel=\"noopener\">Research Gate</a>. </p>\n<h3 id=\"BB-life\"><a href=\"#BB-life\" class=\"headerlink\" title=\"BB life\"></a>BB life</h3><p>Some interesting stuff about life. <strong>BB</strong>, in Chinese, means <strong>talk about</strong>, <strong>chat</strong>. </p>\n<h3 id=\"Work\"><a href=\"#Work\" class=\"headerlink\" title=\"Work\"></a>Work</h3><p>Not on work yet, although 3 years ago I was a lecturer in China. </p>\n<h3 id=\"Blogging\"><a href=\"#Blogging\" class=\"headerlink\" title=\"Blogging\"></a>Blogging</h3><p>Interesting technologies about blogging. </p>\n<p>At last, enjoy the blog. Hope you can find something useful. Or at least have fun :-)</p>\n"}],"PostAsset":[{"_id":"source/_posts/2018-10-22-IntrotoPro1/Est_S2VS.jpg","slug":"Est_S2VS.jpg","post":"ckfwgy10z0001nj397pmdm7sw","modified":0,"renderable":0},{"_id":"source/_posts/2019-06-21-ABCalgorithm/modelseleSMC.png","slug":"modelseleSMC.png","post":"ckfwgy13g0018nj39py0n5ffl","modified":0,"renderable":0},{"_id":"source/_posts/2018-11-30-SMCplots/Rplot1.png","slug":"Rplot1.png","post":"ckfwgy12j000knj39y3w1kxyt","modified":0,"renderable":0},{"_id":"source/_posts/2018-11-30-SMCplots/Rplot2.png","slug":"Rplot2.png","post":"ckfwgy12j000knj39y3w1kxyt","modified":0,"renderable":0},{"_id":"source/_posts/2019-01-04-PyCUDAseries1/vsinstaller.png","slug":"vsinstaller.png","post":"ckfwgy12q000snj39ka9xwjdx","modified":0,"renderable":0},{"_id":"source/_posts/2019-01-04-PyCUDAseries1/win10SDK.png","slug":"win10SDK.png","post":"ckfwgy12q000snj39ka9xwjdx","modified":0,"renderable":0},{"_id":"source/_posts/2019-01-10sed2/mfiles.png","slug":"mfiles.png","post":"ckfwgy12s000vnj39wvrb1si9","modified":0,"renderable":0},{"_id":"source/_posts/2019-01-10sed2/rdatafiles.png","slug":"rdatafiles.png","post":"ckfwgy12s000vnj39wvrb1si9","modified":0,"renderable":0},{"_id":"source/_posts/2019-01-14-PyCUDAseries2/gridblockthread.png","slug":"gridblockthread.png","post":"ckfwgy12u000xnj390mtnx2b7","modified":0,"renderable":0},{"_id":"source/_posts/2019-01-14-PyCUDAseries2/speedtest.png","slug":"speedtest.png","post":"ckfwgy12u000xnj390mtnx2b7","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-01-Machinelearningseries1/modelselection1.png","slug":"modelselection1.png","post":"ckfwgy13c0014nj39kc30bjoo","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-01-Machinelearningseries1/prediction_sample.png","slug":"prediction_sample.png","post":"ckfwgy13c0014nj39kc30bjoo","modified":0,"renderable":0},{"_id":"source/_posts/2019-03-20-readdatadon/d.png","slug":"d.png","post":"ckfwgy13d0017nj3905vi5u7n","modified":0,"renderable":0},{"_id":"source/_posts/2019-03-20-readdatadon/rewrittendata.png","slug":"rewrittendata.png","post":"ckfwgy13d0017nj3905vi5u7n","modified":0,"renderable":0},{"_id":"source/_posts/2018-10-22-IntrotoPro1/Powertable.jpg","slug":"Powertable.jpg","post":"ckfwgy10z0001nj397pmdm7sw","modified":0,"renderable":0},{"_id":"source/_posts/2018-10-22-IntrotoPro1/Trees_S2.jpg","slug":"Trees_S2.jpg","post":"ckfwgy10z0001nj397pmdm7sw","modified":0,"renderable":0},{"_id":"source/_posts/2018-10-29-sed/d.png","slug":"d.png","post":"ckfwgy129000dnj39b7c8c9kf","modified":0,"renderable":0},{"_id":"source/_posts/2018-10-29-sed/ds.png","slug":"ds.png","post":"ckfwgy129000dnj39b7c8c9kf","modified":0,"renderable":0},{"_id":"source/_posts/2018-10-29-sed/rawdata.png","slug":"rawdata.png","post":"ckfwgy129000dnj39b7c8c9kf","modified":0,"renderable":0},{"_id":"source/_posts/2019-06-21-ABCalgorithm/MCMC3chains_test3.mp4","slug":"MCMC3chains_test3.mp4","post":"ckfwgy13g0018nj39py0n5ffl","modified":0,"renderable":0},{"_id":"source/_posts/2019-06-21-ABCalgorithm/Rplot2.png","slug":"Rplot2.png","post":"ckfwgy13g0018nj39py0n5ffl","modified":0,"renderable":0},{"_id":"source/_posts/2018-11-29-color/col1.png","slug":"col1.png","post":"ckfwgy12g000hnj39lo0atwq7","modified":0,"renderable":0},{"_id":"source/_posts/2018-11-29-color/col2.png","slug":"col2.png","post":"ckfwgy12g000hnj39lo0atwq7","modified":0,"renderable":0},{"_id":"source/_posts/2018-11-29-color/col3.png","slug":"col3.png","post":"ckfwgy12g000hnj39lo0atwq7","modified":0,"renderable":0},{"_id":"source/_posts/2018-11-29-color/col4.png","slug":"col4.png","post":"ckfwgy12g000hnj39lo0atwq7","modified":0,"renderable":0},{"_id":"source/_posts/2020-03-09-GUI-traitevolution/p1.png","slug":"p1.png","post":"ckfwgy13j001cnj398rj2m8mh","modified":0,"renderable":0},{"_id":"source/_posts/2020-03-09-GUI-traitevolution/p2.png","slug":"p2.png","post":"ckfwgy13j001cnj398rj2m8mh","modified":0,"renderable":0},{"_id":"source/_posts/2020-03-09-GUI-traitevolution/p3.png","slug":"p3.png","post":"ckfwgy13j001cnj398rj2m8mh","modified":0,"renderable":0},{"_id":"source/_posts/2020-03-09-GUI-traitevolution/p4.png","slug":"p4.png","post":"ckfwgy13j001cnj398rj2m8mh","modified":0,"renderable":0},{"_id":"source/_posts/2019-06-21-TPmodel/guixianren1.jpg","slug":"guixianren1.jpg","post":"ckfwgy13k001fnj39gsoz8gkt","modified":0,"renderable":0},{"_id":"source/_posts/2019-06-21-TPmodel/guixianren_s.png","slug":"guixianren_s.png","post":"ckfwgy13k001fnj39gsoz8gkt","modified":0,"renderable":0},{"_id":"source/_posts/2019-06-21-TPmodel/multi6species15.mp4","slug":"multi6species15.mp4","post":"ckfwgy13k001fnj39gsoz8gkt","modified":0,"renderable":0},{"_id":"source/_posts/2019-06-21-TPmodel/singlespecies5.mp4","slug":"singlespecies5.mp4","post":"ckfwgy13k001fnj39gsoz8gkt","modified":0,"renderable":0},{"_id":"source/_posts/2020-03-18-generalABC/Predict1.png","slug":"Predict1.png","post":"ckfwgy13m001knj393x83akkb","modified":0,"renderable":0},{"_id":"source/_posts/2020-03-18-generalABC/Predict2.png","slug":"Predict2.png","post":"ckfwgy13m001knj393x83akkb","modified":0,"renderable":0},{"_id":"source/_posts/2020-03-18-generalABC/abc_scheme.png","slug":"abc_scheme.png","post":"ckfwgy13m001knj393x83akkb","modified":0,"renderable":0},{"_id":"source/_posts/2020-03-18-generalABC/data.png","slug":"data.png","post":"ckfwgy13m001knj393x83akkb","modified":0,"renderable":0},{"_id":"source/_posts/2020-03-18-generalABC/setup_model1.gif","slug":"setup_model1.gif","post":"ckfwgy13m001knj393x83akkb","modified":0,"renderable":0},{"_id":"source/_posts/2020-03-18-generalABC/setup_model2.gif","slug":"setup_model2.gif","post":"ckfwgy13m001knj393x83akkb","modified":0,"renderable":0},{"_id":"source/_posts/2018-12-05-ggradar2/fullscore.png","slug":"fullscore.png","post":"ckfwgy12p000qnj39hhnq6saz","modified":0,"renderable":0},{"_id":"source/_posts/2018-12-05-ggradar2/lux.png","slug":"lux.png","post":"ckfwgy12p000qnj39hhnq6saz","modified":0,"renderable":0},{"_id":"source/_posts/2018-12-05-ggradar2/mini.png","slug":"mini.png","post":"ckfwgy12p000qnj39hhnq6saz","modified":0,"renderable":0},{"_id":"source/_posts/2018-12-05-ggradar2/multipleplots.png","slug":"multipleplots.png","post":"ckfwgy12p000qnj39hhnq6saz","modified":0,"renderable":0},{"_id":"source/_posts/2018-12-05-ggradar2/multipleplotsbig.png","slug":"multipleplotsbig.png","post":"ckfwgy12p000qnj39hhnq6saz","modified":0,"renderable":0},{"_id":"source/_posts/2018-12-05-ggradar2/nolegend.png","slug":"nolegend.png","post":"ckfwgy12p000qnj39hhnq6saz","modified":0,"renderable":0},{"_id":"source/_posts/2018-12-05-ggradar2/roundfill.png","slug":"roundfill.png","post":"ckfwgy12p000qnj39hhnq6saz","modified":0,"renderable":0},{"_id":"source/_posts/2018-12-05-ggradar2/roundnofill.png","slug":"roundnofill.png","post":"ckfwgy12p000qnj39hhnq6saz","modified":0,"renderable":0},{"_id":"source/_posts/2018-12-05-ggradar2/straightfill.png","slug":"straightfill.png","post":"ckfwgy12p000qnj39hhnq6saz","modified":0,"renderable":0},{"_id":"source/_posts/2018-12-05-ggradar2/straightnofill.png","slug":"straightnofill.png","post":"ckfwgy12p000qnj39hhnq6saz","modified":0,"renderable":0},{"_id":"source/_posts/2018-12-05-ggradar2/trend.png","slug":"trend.png","post":"ckfwgy12p000qnj39hhnq6saz","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/1.png","slug":"1.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/10.png","slug":"10.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/11.png","slug":"11.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/12.png","slug":"12.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/13-15.png","slug":"13-15.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/16.png","slug":"16.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/17.png","slug":"17.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/18.png","slug":"18.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/19-1.png","slug":"19-1.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/19.png","slug":"19.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/2.png","slug":"2.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/20.png","slug":"20.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/21.png","slug":"21.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/22.png","slug":"22.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/23.png","slug":"23.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/24.png","slug":"24.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/25.png","slug":"25.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/3.png","slug":"3.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/4.png","slug":"4.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/5.png","slug":"5.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/6.png","slug":"6.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/78.png","slug":"78.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/9-1.png","slug":"9-1.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/9.png","slug":"9.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/step1.png","slug":"step1.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/step2.png","slug":"step2.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/step3.png","slug":"step3.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/step4.png","slug":"step4.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/step5.png","slug":"step5.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/step6.png","slug":"step6.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/step7.png","slug":"step7.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0},{"_id":"source/_posts/2019-02-08-Machinelearningseries2/step8.png","slug":"step8.png","post":"ckfwgy1380013nj399avzbwoy","modified":0,"renderable":0}],"PostCategory":[{"post_id":"ckfwgy10z0001nj397pmdm7sw","category_id":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy130000ynj39wijvh074"},{"post_id":"ckfwgy10z0001nj397pmdm7sw","category_id":"ckfwgy12p000onj39zidcws5k","_id":"ckfwgy1360012nj392xa6w0zy"},{"post_id":"ckfwgy127000cnj391jb65uit","category_id":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy13i0019nj39qg5l7pdy"},{"post_id":"ckfwgy127000cnj391jb65uit","category_id":"ckfwgy1320010nj39bqek7v5a","_id":"ckfwgy13k001dnj39d3cilqn8"},{"post_id":"ckfwgy1150003nj399b58fphg","category_id":"ckfwgy126000anj39l4uxd8x9","_id":"ckfwgy13l001hnj39g3urlkrs"},{"post_id":"ckfwgy1150003nj399b58fphg","category_id":"ckfwgy13d0015nj39hkl6e4nu","_id":"ckfwgy13n001mnj39ruaamgb1"},{"post_id":"ckfwgy12f000gnj39rfo06fgd","category_id":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy13q001tnj3936xmcrsu"},{"post_id":"ckfwgy12f000gnj39rfo06fgd","category_id":"ckfwgy12s000unj39ro1mki1q","_id":"ckfwgy13s001wnj39pdy96q74"},{"post_id":"ckfwgy11c0007nj39jit1ttbs","category_id":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy13s001xnj392jlurmg8"},{"post_id":"ckfwgy11c0007nj39jit1ttbs","category_id":"ckfwgy13o001pnj39ikcpum8n","_id":"ckfwgy13t0020nj39x4z9dd3s"},{"post_id":"ckfwgy11f0008nj39m0ge4p77","category_id":"ckfwgy12h000inj39o8utf2iw","_id":"ckfwgy13v0027nj39rr51jw7i"},{"post_id":"ckfwgy11f0008nj39m0ge4p77","category_id":"ckfwgy13t0021nj39co4qiqpm","_id":"ckfwgy13x002anj39fzvqtxgb"},{"post_id":"ckfwgy11h0009nj39efn5zhxl","category_id":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy144002snj39fayk3fyq"},{"post_id":"ckfwgy11h0009nj39efn5zhxl","category_id":"ckfwgy12s000unj39ro1mki1q","_id":"ckfwgy144002vnj39qovl3ey7"},{"post_id":"ckfwgy11h0009nj39efn5zhxl","category_id":"ckfwgy142002lnj39gnga7fmz","_id":"ckfwgy144002xnj3964q1hwrb"},{"post_id":"ckfwgy129000dnj39b7c8c9kf","category_id":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy14i003hnj39tiqaejja"},{"post_id":"ckfwgy129000dnj39b7c8c9kf","category_id":"ckfwgy13i001anj39x8mbvsho","_id":"ckfwgy14j003knj3942i8dru5"},{"post_id":"ckfwgy129000dnj39b7c8c9kf","category_id":"ckfwgy14a0039nj39pc8en4p6","_id":"ckfwgy14l003lnj39iisoo8xc"},{"post_id":"ckfwgy13k001fnj39gsoz8gkt","category_id":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy15i003nnj39vft39o0z"},{"post_id":"ckfwgy13k001fnj39gsoz8gkt","category_id":"ckfwgy12p000onj39zidcws5k","_id":"ckfwgy15n003qnj39k8alg8xr"},{"post_id":"ckfwgy13k001fnj39gsoz8gkt","category_id":"ckfwgy14i003enj39dn74je9i","_id":"ckfwgy15o003tnj39kk918uh3"},{"post_id":"ckfwgy13n001nnj39fv3hbbuo","category_id":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy15p003xnj396c71wnln"},{"post_id":"ckfwgy13n001nnj39fv3hbbuo","category_id":"ckfwgy15h003mnj39jg0i8309","_id":"ckfwgy15p003znj39p6u5ruhx"},{"post_id":"ckfwgy13o001qnj3908vgmv9l","category_id":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy15p0041nj39pwnjcwmz"},{"post_id":"ckfwgy13o001qnj3908vgmv9l","category_id":"ckfwgy15h003mnj39jg0i8309","_id":"ckfwgy17x0043nj397hqeu9ls"},{"post_id":"ckfwgy13p001snj39osf3vifl","category_id":"ckfwgy15o003wnj39xvk5gpdg","_id":"ckfwgy18x0046nj39bx28bsj9"},{"post_id":"ckfwgy12g000hnj39lo0atwq7","category_id":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy18z004cnj395erpyrh5"},{"post_id":"ckfwgy12g000hnj39lo0atwq7","category_id":"ckfwgy13q001unj39lo7rxww5","_id":"ckfwgy18z004dnj39benggqup"},{"post_id":"ckfwgy12g000hnj39lo0atwq7","category_id":"ckfwgy15p0042nj393daq1n7k","_id":"ckfwgy190004gnj39c7arz1fy"},{"post_id":"ckfwgy12j000knj39y3w1kxyt","category_id":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy191004hnj39v5zq5ls8"},{"post_id":"ckfwgy12j000knj39y3w1kxyt","category_id":"ckfwgy13q001unj39lo7rxww5","_id":"ckfwgy192004knj39974nb9ub"},{"post_id":"ckfwgy12j000knj39y3w1kxyt","category_id":"ckfwgy15p0042nj393daq1n7k","_id":"ckfwgy193004lnj39gskonszc"},{"post_id":"ckfwgy12q000snj39ka9xwjdx","category_id":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy197004snj39h5v39r7g"},{"post_id":"ckfwgy12q000snj39ka9xwjdx","category_id":"ckfwgy13x002cnj39q8p82qxm","_id":"ckfwgy197004unj39b6kvn3c0"},{"post_id":"ckfwgy12q000snj39ka9xwjdx","category_id":"ckfwgy191004jnj39fgeibe1h","_id":"ckfwgy197004ynj39kzo8i2ul"},{"post_id":"ckfwgy12s000vnj39wvrb1si9","category_id":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy198004znj394pagyggp"},{"post_id":"ckfwgy12s000vnj39wvrb1si9","category_id":"ckfwgy13i001anj39x8mbvsho","_id":"ckfwgy1980052nj39s38jfjgp"},{"post_id":"ckfwgy12s000vnj39wvrb1si9","category_id":"ckfwgy14a0039nj39pc8en4p6","_id":"ckfwgy1980053nj39mxi9y0w9"},{"post_id":"ckfwgy12u000xnj390mtnx2b7","category_id":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy1990054nj39toq16lgy"},{"post_id":"ckfwgy12u000xnj390mtnx2b7","category_id":"ckfwgy13x002cnj39q8p82qxm","_id":"ckfwgy19c0058nj39yw0gcwgl"},{"post_id":"ckfwgy12u000xnj390mtnx2b7","category_id":"ckfwgy191004jnj39fgeibe1h","_id":"ckfwgy19c005anj39ra26331c"},{"post_id":"ckfwgy131000znj39y9u9ccvc","category_id":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy19d005enj39mp650z5p"},{"post_id":"ckfwgy131000znj39y9u9ccvc","category_id":"ckfwgy13x002cnj39q8p82qxm","_id":"ckfwgy19e005gnj39fgnuipk6"},{"post_id":"ckfwgy131000znj39y9u9ccvc","category_id":"ckfwgy191004jnj39fgeibe1h","_id":"ckfwgy19e005jnj39xwt4legk"},{"post_id":"ckfwgy1380013nj399avzbwoy","category_id":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy19e005knj392o34unkn"},{"post_id":"ckfwgy1380013nj399avzbwoy","category_id":"ckfwgy144002rnj39buy4e0ew","_id":"ckfwgy19f005nnj39dfu0vsrl"},{"post_id":"ckfwgy1380013nj399avzbwoy","category_id":"ckfwgy1980051nj39p5r87mkh","_id":"ckfwgy19f005onj39yxkk95op"},{"post_id":"ckfwgy13c0014nj39kc30bjoo","category_id":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy19h005rnj39vpehegxi"},{"post_id":"ckfwgy13c0014nj39kc30bjoo","category_id":"ckfwgy144002rnj39buy4e0ew","_id":"ckfwgy19h005snj39hpnqq0ii"},{"post_id":"ckfwgy13c0014nj39kc30bjoo","category_id":"ckfwgy1980051nj39p5r87mkh","_id":"ckfwgy19i005vnj392tfzc89k"},{"post_id":"ckfwgy13d0017nj3905vi5u7n","category_id":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy19j005xnj39ii90vfed"},{"post_id":"ckfwgy13d0017nj3905vi5u7n","category_id":"ckfwgy13i001anj39x8mbvsho","_id":"ckfwgy19o0061nj399czk9qaa"},{"post_id":"ckfwgy13d0017nj3905vi5u7n","category_id":"ckfwgy14a0039nj39pc8en4p6","_id":"ckfwgy19o0063nj398yxkfdtx"},{"post_id":"ckfwgy13g0018nj39py0n5ffl","category_id":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy19p0066nj39ixgffinl"},{"post_id":"ckfwgy13g0018nj39py0n5ffl","category_id":"ckfwgy1460034nj3935gafyil","_id":"ckfwgy19p0068nj3989sx0c0l"},{"post_id":"ckfwgy13g0018nj39py0n5ffl","category_id":"ckfwgy19e005inj39fcdzwrg3","_id":"ckfwgy19q006anj39nkukfv2m"},{"post_id":"ckfwgy13j001cnj398rj2m8mh","category_id":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy19q006bnj39q8qu6rvq"},{"post_id":"ckfwgy13j001cnj398rj2m8mh","category_id":"ckfwgy1470036nj39sed0irko","_id":"ckfwgy1ac006dnj39ptcgjfxe"},{"post_id":"ckfwgy13j001cnj398rj2m8mh","category_id":"ckfwgy19f005mnj394qtqgcvt","_id":"ckfwgy1ad006fnj39opb57ogq"},{"post_id":"ckfwgy13m001knj393x83akkb","category_id":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy1ag006inj39b98sm333"},{"post_id":"ckfwgy13m001knj393x83akkb","category_id":"ckfwgy1460034nj3935gafyil","_id":"ckfwgy1ai006knj39k17vx3u9"},{"post_id":"ckfwgy13m001knj393x83akkb","category_id":"ckfwgy19e005inj39fcdzwrg3","_id":"ckfwgy1ai006mnj39ujtywkd5"},{"post_id":"ckfwgy12n000mnj39nfs62fxf","category_id":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy1ai006onj399950wkdl"},{"post_id":"ckfwgy12n000mnj39nfs62fxf","category_id":"ckfwgy13q001unj39lo7rxww5","_id":"ckfwgy1ai006pnj396kfypcgs"},{"post_id":"ckfwgy12n000mnj39nfs62fxf","category_id":"ckfwgy15p0042nj393daq1n7k","_id":"ckfwgy1ai006rnj390r8t741z"},{"post_id":"ckfwgy12n000mnj39nfs62fxf","category_id":"ckfwgy19i005unj390s2ncq21","_id":"ckfwgy1ai006snj39w2tnp92k"},{"post_id":"ckfwgy12p000qnj39hhnq6saz","category_id":"ckfwgy1170004nj392l3if9wh","_id":"ckfwgy1aj006unj39uph4nw1b"},{"post_id":"ckfwgy12p000qnj39hhnq6saz","category_id":"ckfwgy13q001unj39lo7rxww5","_id":"ckfwgy1aj006wnj390l40x23v"},{"post_id":"ckfwgy12p000qnj39hhnq6saz","category_id":"ckfwgy15p0042nj393daq1n7k","_id":"ckfwgy1aj006znj39ombrr0qt"},{"post_id":"ckfwgy12p000qnj39hhnq6saz","category_id":"ckfwgy19i005unj390s2ncq21","_id":"ckfwgy1ak0071nj39vhvy3d20"}],"PostTag":[{"post_id":"ckfwgy10z0001nj397pmdm7sw","tag_id":"ckfwgy11a0005nj39ov7im3am","_id":"ckfwgy12k000lnj39bfiquvwz"},{"post_id":"ckfwgy10z0001nj397pmdm7sw","tag_id":"ckfwgy127000bnj391603lcl3","_id":"ckfwgy12p000nnj3902qirnhz"},{"post_id":"ckfwgy10z0001nj397pmdm7sw","tag_id":"ckfwgy12a000fnj398n9qa1ea","_id":"ckfwgy12q000rnj393q30qhgt"},{"post_id":"ckfwgy1150003nj399b58fphg","tag_id":"ckfwgy12i000jnj39far5kr2c","_id":"ckfwgy12s000tnj39xm49h4jc"},{"post_id":"ckfwgy11c0007nj39jit1ttbs","tag_id":"ckfwgy12p000pnj39ilwvpsj3","_id":"ckfwgy13k001enj39o0cdskpn"},{"post_id":"ckfwgy11c0007nj39jit1ttbs","tag_id":"ckfwgy12t000wnj392odbx6zs","_id":"ckfwgy13l001gnj3968h74vf9"},{"post_id":"ckfwgy11c0007nj39jit1ttbs","tag_id":"ckfwgy127000bnj391603lcl3","_id":"ckfwgy13n001lnj39ut4hfzgr"},{"post_id":"ckfwgy11c0007nj39jit1ttbs","tag_id":"ckfwgy13d0016nj39sc8932ni","_id":"ckfwgy13o001onj39owh37btt"},{"post_id":"ckfwgy11f0008nj39m0ge4p77","tag_id":"ckfwgy13j001bnj39ze7n3xua","_id":"ckfwgy13v0025nj39ecfst59h"},{"post_id":"ckfwgy11f0008nj39m0ge4p77","tag_id":"ckfwgy13l001jnj39ti1ceczr","_id":"ckfwgy13w0028nj3918ycqlhc"},{"post_id":"ckfwgy11f0008nj39m0ge4p77","tag_id":"ckfwgy13p001rnj39xf5y7kpa","_id":"ckfwgy13x002bnj39reh9jvq8"},{"post_id":"ckfwgy11f0008nj39m0ge4p77","tag_id":"ckfwgy13q001vnj39y9lwkldv","_id":"ckfwgy13x002dnj39ku6nnkt7"},{"post_id":"ckfwgy11f0008nj39m0ge4p77","tag_id":"ckfwgy13t001znj39xarsp7iu","_id":"ckfwgy13y002fnj39wzcl5yun"},{"post_id":"ckfwgy11f0008nj39m0ge4p77","tag_id":"ckfwgy13t0022nj39425ayk99","_id":"ckfwgy13y002hnj39dstmv9at"},{"post_id":"ckfwgy11h0009nj39efn5zhxl","tag_id":"ckfwgy13j001bnj39ze7n3xua","_id":"ckfwgy143002pnj39s8oe3l3m"},{"post_id":"ckfwgy11h0009nj39efn5zhxl","tag_id":"ckfwgy13l001jnj39ti1ceczr","_id":"ckfwgy144002qnj399hcf1wd8"},{"post_id":"ckfwgy11h0009nj39efn5zhxl","tag_id":"ckfwgy13p001rnj39xf5y7kpa","_id":"ckfwgy144002unj398y0snlmw"},{"post_id":"ckfwgy11h0009nj39efn5zhxl","tag_id":"ckfwgy13q001vnj39y9lwkldv","_id":"ckfwgy144002wnj39nxfzfw57"},{"post_id":"ckfwgy11h0009nj39efn5zhxl","tag_id":"ckfwgy13t001znj39xarsp7iu","_id":"ckfwgy1450030nj39hf5qja8j"},{"post_id":"ckfwgy11h0009nj39efn5zhxl","tag_id":"ckfwgy13t0022nj39425ayk99","_id":"ckfwgy1450031nj39zb4aa37e"},{"post_id":"ckfwgy127000cnj391jb65uit","tag_id":"ckfwgy143002onj39m6ajy1cd","_id":"ckfwgy14a0038nj39w93z6sfn"},{"post_id":"ckfwgy127000cnj391jb65uit","tag_id":"ckfwgy144002tnj392gdcxhy3","_id":"ckfwgy14f003anj3948mnavqx"},{"post_id":"ckfwgy127000cnj391jb65uit","tag_id":"ckfwgy145002znj39csj5t5on","_id":"ckfwgy14i003cnj39ha8avmr2"},{"post_id":"ckfwgy127000cnj391jb65uit","tag_id":"ckfwgy13d0016nj39sc8932ni","_id":"ckfwgy14i003dnj39wmqaa5b1"},{"post_id":"ckfwgy127000cnj391jb65uit","tag_id":"ckfwgy1470035nj3936n7dr9m","_id":"ckfwgy14i003gnj39bqmpp2fa"},{"post_id":"ckfwgy129000dnj39b7c8c9kf","tag_id":"ckfwgy14a0037nj39nhdqnyhb","_id":"ckfwgy15n003pnj39b2i5mnm2"},{"post_id":"ckfwgy129000dnj39b7c8c9kf","tag_id":"ckfwgy14h003bnj391adm72ga","_id":"ckfwgy15o003snj397euo1eb7"},{"post_id":"ckfwgy129000dnj39b7c8c9kf","tag_id":"ckfwgy14i003fnj399sk3hkvd","_id":"ckfwgy15o003vnj39hd19sne4"},{"post_id":"ckfwgy129000dnj39b7c8c9kf","tag_id":"ckfwgy14j003jnj39ow7245d2","_id":"ckfwgy15p003ynj399p8vdrk8"},{"post_id":"ckfwgy12f000gnj39rfo06fgd","tag_id":"ckfwgy15i003onj39y0xzdfge","_id":"ckfwgy18x0045nj39pbprr8wo"},{"post_id":"ckfwgy12f000gnj39rfo06fgd","tag_id":"ckfwgy15o003unj39ulhukg33","_id":"ckfwgy18y0048nj39q3sye70g"},{"post_id":"ckfwgy12f000gnj39rfo06fgd","tag_id":"ckfwgy15p0040nj39mdnguiw1","_id":"ckfwgy18y004anj398fekiqm0"},{"post_id":"ckfwgy12g000hnj39lo0atwq7","tag_id":"ckfwgy15i003onj39y0xzdfge","_id":"ckfwgy196004onj39follp0hl"},{"post_id":"ckfwgy12g000hnj39lo0atwq7","tag_id":"ckfwgy18y0049nj39mybj87lr","_id":"ckfwgy196004pnj39c7uk6e5o"},{"post_id":"ckfwgy12g000hnj39lo0atwq7","tag_id":"ckfwgy190004enj39dslm8izt","_id":"ckfwgy197004tnj39f4wqnp78"},{"post_id":"ckfwgy12g000hnj39lo0atwq7","tag_id":"ckfwgy191004inj39gv89027r","_id":"ckfwgy197004vnj39r19snumn"},{"post_id":"ckfwgy12j000knj39y3w1kxyt","tag_id":"ckfwgy15i003onj39y0xzdfge","_id":"ckfwgy19c0057nj39b5xav03g"},{"post_id":"ckfwgy12j000knj39y3w1kxyt","tag_id":"ckfwgy196004qnj39aa0y95xb","_id":"ckfwgy19c0059nj39soi9c5io"},{"post_id":"ckfwgy12j000knj39y3w1kxyt","tag_id":"ckfwgy197004wnj392zmow8i0","_id":"ckfwgy19d005dnj39iftjeqt7"},{"post_id":"ckfwgy12j000knj39y3w1kxyt","tag_id":"ckfwgy1980050nj39ksfokyij","_id":"ckfwgy19e005fnj39fgt8a0x7"},{"post_id":"ckfwgy12n000mnj39nfs62fxf","tag_id":"ckfwgy15i003onj39y0xzdfge","_id":"ckfwgy19i005wnj39fzx9vo9s"},{"post_id":"ckfwgy12n000mnj39nfs62fxf","tag_id":"ckfwgy196004qnj39aa0y95xb","_id":"ckfwgy19j005ynj3968ryx6ud"},{"post_id":"ckfwgy12n000mnj39nfs62fxf","tag_id":"ckfwgy19e005hnj3970fv4sg5","_id":"ckfwgy19o0062nj39ci6omll2"},{"post_id":"ckfwgy12n000mnj39nfs62fxf","tag_id":"ckfwgy1980050nj39ksfokyij","_id":"ckfwgy19o0064nj39vt9ypu69"},{"post_id":"ckfwgy12n000mnj39nfs62fxf","tag_id":"ckfwgy19f005pnj39k30xx0e9","_id":"ckfwgy19p0067nj39eowmnq1g"},{"post_id":"ckfwgy12p000qnj39hhnq6saz","tag_id":"ckfwgy15i003onj39y0xzdfge","_id":"ckfwgy1ad006enj399nabo86l"},{"post_id":"ckfwgy12p000qnj39hhnq6saz","tag_id":"ckfwgy196004qnj39aa0y95xb","_id":"ckfwgy1ad006gnj39qjaq1a2u"},{"post_id":"ckfwgy12p000qnj39hhnq6saz","tag_id":"ckfwgy19e005hnj3970fv4sg5","_id":"ckfwgy1ah006jnj39rglj9mgb"},{"post_id":"ckfwgy12p000qnj39hhnq6saz","tag_id":"ckfwgy1980050nj39ksfokyij","_id":"ckfwgy1ai006lnj39uzsydam0"},{"post_id":"ckfwgy12q000snj39ka9xwjdx","tag_id":"ckfwgy19q006cnj39f2jsgzn4","_id":"ckfwgy1aj006vnj39v8nwhvuj"},{"post_id":"ckfwgy12q000snj39ka9xwjdx","tag_id":"ckfwgy1ad006hnj3923hshmbx","_id":"ckfwgy1aj006xnj39ghw0ejxe"},{"post_id":"ckfwgy12q000snj39ka9xwjdx","tag_id":"ckfwgy1ai006nnj390dmvngz0","_id":"ckfwgy1aj0070nj39wxheo7od"},{"post_id":"ckfwgy12q000snj39ka9xwjdx","tag_id":"ckfwgy1ai006qnj39ogdv01gu","_id":"ckfwgy1ak0072nj39aoe0ir0q"},{"post_id":"ckfwgy12s000vnj39wvrb1si9","tag_id":"ckfwgy14a0037nj39nhdqnyhb","_id":"ckfwgy1al0076nj395uzffkba"},{"post_id":"ckfwgy12s000vnj39wvrb1si9","tag_id":"ckfwgy14h003bnj391adm72ga","_id":"ckfwgy1al0077nj39zi5i3ezu"},{"post_id":"ckfwgy12s000vnj39wvrb1si9","tag_id":"ckfwgy14i003fnj399sk3hkvd","_id":"ckfwgy1al0079nj39jnywn1ky"},{"post_id":"ckfwgy12s000vnj39wvrb1si9","tag_id":"ckfwgy14j003jnj39ow7245d2","_id":"ckfwgy1al007anj39s1wiippr"},{"post_id":"ckfwgy12u000xnj390mtnx2b7","tag_id":"ckfwgy19q006cnj39f2jsgzn4","_id":"ckfwgy1an007enj39hw1yiual"},{"post_id":"ckfwgy12u000xnj390mtnx2b7","tag_id":"ckfwgy1ad006hnj3923hshmbx","_id":"ckfwgy1an007fnj391ev8xrcy"},{"post_id":"ckfwgy12u000xnj390mtnx2b7","tag_id":"ckfwgy1ai006nnj390dmvngz0","_id":"ckfwgy1an007hnj39u5uf6r3k"},{"post_id":"ckfwgy12u000xnj390mtnx2b7","tag_id":"ckfwgy1ai006qnj39ogdv01gu","_id":"ckfwgy1an007inj39i3icnbfa"},{"post_id":"ckfwgy131000znj39y9u9ccvc","tag_id":"ckfwgy19q006cnj39f2jsgzn4","_id":"ckfwgy1ap007mnj39dgxkgav0"},{"post_id":"ckfwgy131000znj39y9u9ccvc","tag_id":"ckfwgy1ad006hnj3923hshmbx","_id":"ckfwgy1ap007nnj39tjrz3rc9"},{"post_id":"ckfwgy131000znj39y9u9ccvc","tag_id":"ckfwgy1ai006nnj390dmvngz0","_id":"ckfwgy1aq007pnj39v4d8nqlp"},{"post_id":"ckfwgy131000znj39y9u9ccvc","tag_id":"ckfwgy1ai006qnj39ogdv01gu","_id":"ckfwgy1aq007qnj39akdm7y3o"},{"post_id":"ckfwgy1380013nj399avzbwoy","tag_id":"ckfwgy19q006cnj39f2jsgzn4","_id":"ckfwgy1b3007vnj397itrf49p"},{"post_id":"ckfwgy1380013nj399avzbwoy","tag_id":"ckfwgy1ap007onj39tqopp7hx","_id":"ckfwgy1b3007wnj39rkf1nyye"},{"post_id":"ckfwgy1380013nj399avzbwoy","tag_id":"ckfwgy1aq007rnj39mkla1qrh","_id":"ckfwgy1b4007ynj39rpefyy0h"},{"post_id":"ckfwgy1380013nj399avzbwoy","tag_id":"ckfwgy1aq007snj39o3cdysb1","_id":"ckfwgy1b4007znj39x26dp1sa"},{"post_id":"ckfwgy1380013nj399avzbwoy","tag_id":"ckfwgy1b2007tnj39rn12p7yk","_id":"ckfwgy1b40081nj39af1u5o3a"},{"post_id":"ckfwgy13c0014nj39kc30bjoo","tag_id":"ckfwgy19q006cnj39f2jsgzn4","_id":"ckfwgy1b50084nj39if467ca8"},{"post_id":"ckfwgy13c0014nj39kc30bjoo","tag_id":"ckfwgy1ap007onj39tqopp7hx","_id":"ckfwgy1b50085nj39tnz6e62u"},{"post_id":"ckfwgy13c0014nj39kc30bjoo","tag_id":"ckfwgy1aq007rnj39mkla1qrh","_id":"ckfwgy1b70087nj3994g6nbl0"},{"post_id":"ckfwgy13c0014nj39kc30bjoo","tag_id":"ckfwgy1ai006nnj390dmvngz0","_id":"ckfwgy1b70088nj39obm8vlk6"},{"post_id":"ckfwgy13d0017nj3905vi5u7n","tag_id":"ckfwgy1b50083nj393udwc2k0","_id":"ckfwgy1b8008bnj39w2z5n9c2"},{"post_id":"ckfwgy13d0017nj3905vi5u7n","tag_id":"ckfwgy14h003bnj391adm72ga","_id":"ckfwgy1b8008cnj398syg8cqk"},{"post_id":"ckfwgy13d0017nj3905vi5u7n","tag_id":"ckfwgy15i003onj39y0xzdfge","_id":"ckfwgy1b8008enj39d9vs2jku"},{"post_id":"ckfwgy13g0018nj39py0n5ffl","tag_id":"ckfwgy1b50083nj393udwc2k0","_id":"ckfwgy1b9008hnj393x9h6g56"},{"post_id":"ckfwgy13g0018nj39py0n5ffl","tag_id":"ckfwgy1b8008dnj39ri0un9vg","_id":"ckfwgy1b9008inj39gr2vxzxj"},{"post_id":"ckfwgy13g0018nj39py0n5ffl","tag_id":"ckfwgy1b8008fnj39lb7w6g63","_id":"ckfwgy1b9008knj39gk4ugb4p"},{"post_id":"ckfwgy13j001cnj398rj2m8mh","tag_id":"ckfwgy19q006cnj39f2jsgzn4","_id":"ckfwgy1bc008nnj39j9fwhy37"},{"post_id":"ckfwgy13j001cnj398rj2m8mh","tag_id":"ckfwgy1b9008jnj39v8c1ijtb","_id":"ckfwgy1bc008onj39yrklsky0"},{"post_id":"ckfwgy13j001cnj398rj2m8mh","tag_id":"ckfwgy1b9008lnj39muio0ait","_id":"ckfwgy1bd008qnj39tdbfpea7"},{"post_id":"ckfwgy13k001fnj39gsoz8gkt","tag_id":"ckfwgy1b8008fnj39lb7w6g63","_id":"ckfwgy1be008snj39fys43055"},{"post_id":"ckfwgy13k001fnj39gsoz8gkt","tag_id":"ckfwgy1bd008pnj39ts6j1qss","_id":"ckfwgy1be008tnj39p3binp3d"},{"post_id":"ckfwgy13k001fnj39gsoz8gkt","tag_id":"ckfwgy11a0005nj39ov7im3am","_id":"ckfwgy1bf008vnj39dq36mnu8"},{"post_id":"ckfwgy13m001knj393x83akkb","tag_id":"ckfwgy19q006cnj39f2jsgzn4","_id":"ckfwgy1c0008ynj39n7mo2h3t"},{"post_id":"ckfwgy13m001knj393x83akkb","tag_id":"ckfwgy1b8008dnj39ri0un9vg","_id":"ckfwgy1c0008znj39fkv04yy9"},{"post_id":"ckfwgy13m001knj393x83akkb","tag_id":"ckfwgy1bz008wnj39hy8itl1j","_id":"ckfwgy1c10091nj39csw0joqn"},{"post_id":"ckfwgy13n001nnj39fv3hbbuo","tag_id":"ckfwgy19q006cnj39f2jsgzn4","_id":"ckfwgy1c20096nj3917crkysw"},{"post_id":"ckfwgy13n001nnj39fv3hbbuo","tag_id":"ckfwgy1c00090nj39svfan9yk","_id":"ckfwgy1c20097nj39qmncj00c"},{"post_id":"ckfwgy13n001nnj39fv3hbbuo","tag_id":"ckfwgy1c10092nj394pmogyke","_id":"ckfwgy1c20099nj39enodpmyd"},{"post_id":"ckfwgy13n001nnj39fv3hbbuo","tag_id":"ckfwgy1c10093nj39tgvmxuao","_id":"ckfwgy1c4009anj39tnp1wtt6"},{"post_id":"ckfwgy13n001nnj39fv3hbbuo","tag_id":"ckfwgy1c10094nj392gs0zsmr","_id":"ckfwgy1c4009cnj39dvbvy6tt"},{"post_id":"ckfwgy13o001qnj3908vgmv9l","tag_id":"ckfwgy15i003onj39y0xzdfge","_id":"ckfwgy1c6009gnj39hocyjbhb"},{"post_id":"ckfwgy13o001qnj3908vgmv9l","tag_id":"ckfwgy1c00090nj39svfan9yk","_id":"ckfwgy1c6009hnj39mh1r9ogu"},{"post_id":"ckfwgy13o001qnj3908vgmv9l","tag_id":"ckfwgy1c10092nj394pmogyke","_id":"ckfwgy1c6009inj39chajga3j"},{"post_id":"ckfwgy13o001qnj3908vgmv9l","tag_id":"ckfwgy1c10093nj39tgvmxuao","_id":"ckfwgy1c7009jnj393w4ff23k"},{"post_id":"ckfwgy13o001qnj3908vgmv9l","tag_id":"ckfwgy1c5009enj39c2y71wyl","_id":"ckfwgy1c7009knj39t58pw4dk"},{"post_id":"ckfwgy13p001snj39osf3vifl","tag_id":"ckfwgy1c5009fnj39ux9c9m7l","_id":"ckfwgy1c7009lnj39i8q1r55v"}],"Tag":[{"name":"evolution","_id":"ckfwgy11a0005nj39ov7im3am"},{"name":"mathematical modeling","_id":"ckfwgy127000bnj391603lcl3"},{"name":"diversity-dependence","_id":"ckfwgy12a000fnj398n9qa1ea"},{"name":"Tech","_id":"ckfwgy12i000jnj39far5kr2c"},{"name":"the Fokker-Planck equation","_id":"ckfwgy12p000pnj39ilwvpsj3"},{"name":"stochastic differential equation","_id":"ckfwgy12t000wnj392odbx6zs"},{"name":"math","_id":"ckfwgy13d0016nj39sc8932ni"},{"name":"phylogeny","_id":"ckfwgy13j001bnj39ze7n3xua"},{"name":"phylo class","_id":"ckfwgy13l001jnj39ti1ceczr"},{"name":"L table","_id":"ckfwgy13p001rnj39xf5y7kpa"},{"name":"DDD package","_id":"ckfwgy13q001vnj39y9lwkldv"},{"name":"pruneL","_id":"ckfwgy13t001znj39xarsp7iu"},{"name":"phylo2L","_id":"ckfwgy13t0022nj39425ayk99"},{"name":"regression","_id":"ckfwgy143002onj39m6ajy1cd"},{"name":"least square method","_id":"ckfwgy144002tnj392gdcxhy3"},{"name":"partial least square","_id":"ckfwgy145002znj39csj5t5on"},{"name":"machine learning","_id":"ckfwgy1470035nj3936n7dr9m"},{"name":"project 3","_id":"ckfwgy14a0037nj39nhdqnyhb"},{"name":"bash","_id":"ckfwgy14h003bnj391adm72ga"},{"name":"mega data","_id":"ckfwgy14i003fnj399sk3hkvd"},{"name":"extract information","_id":"ckfwgy14j003jnj39ow7245d2"},{"name":"R","_id":"ckfwgy15i003onj39y0xzdfge"},{"name":"update","_id":"ckfwgy15o003unj39ulhukg33"},{"name":"packages","_id":"ckfwgy15p0040nj39mdnguiw1"},{"name":"color","_id":"ckfwgy18y0049nj39mybj87lr"},{"name":"python","_id":"ckfwgy190004enj39dslm8izt"},{"name":"plot","_id":"ckfwgy191004inj39gv89027r"},{"name":"ggplot","_id":"ckfwgy196004qnj39aa0y95xb"},{"name":"SMC","_id":"ckfwgy197004wnj392zmow8i0"},{"name":"Data visualization","_id":"ckfwgy1980050nj39ksfokyij"},{"name":"ggradar2","_id":"ckfwgy19e005hnj3970fv4sg5"},{"name":"help document","_id":"ckfwgy19f005pnj39k30xx0e9"},{"name":"Python","_id":"ckfwgy19q006cnj39f2jsgzn4"},{"name":"CUDA","_id":"ckfwgy1ad006hnj3923hshmbx"},{"name":"GPU programming","_id":"ckfwgy1ai006nnj390dmvngz0"},{"name":"Parallel computation","_id":"ckfwgy1ai006qnj39ogdv01gu"},{"name":"Machine learning","_id":"ckfwgy1ap007onj39tqopp7hx"},{"name":"neural networks","_id":"ckfwgy1aq007rnj39mkla1qrh"},{"name":"Backward propagation","_id":"ckfwgy1aq007snj39o3cdysb1"},{"name":"gradient descent","_id":"ckfwgy1b2007tnj39rn12p7yk"},{"name":"data analysis","_id":"ckfwgy1b50083nj393udwc2k0"},{"name":"ABC","_id":"ckfwgy1b8008dnj39ri0un9vg"},{"name":"Animation","_id":"ckfwgy1b8008fnj39lb7w6g63"},{"name":"GUI","_id":"ckfwgy1b9008jnj39v8c1ijtb"},{"name":"models","_id":"ckfwgy1b9008lnj39muio0ait"},{"name":"ecology","_id":"ckfwgy1bd008pnj39ts6j1qss"},{"name":"algorithm","_id":"ckfwgy1bz008wnj39hy8itl1j"},{"name":"simulation","_id":"ckfwgy1c00090nj39svfan9yk"},{"name":"coronavirus","_id":"ckfwgy1c10092nj394pmogyke"},{"name":"pandemic","_id":"ckfwgy1c10093nj39tgvmxuao"},{"name":"government measure","_id":"ckfwgy1c10094nj392gs0zsmr"},{"name":"data visualization","_id":"ckfwgy1c5009enj39c2y71wyl"},{"name":"info","_id":"ckfwgy1c5009fnj39ux9c9m7l"}]}}