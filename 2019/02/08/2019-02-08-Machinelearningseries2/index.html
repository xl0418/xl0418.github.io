<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Machine learning on biology S2: how does a neural network work mathematically? | Liang Xu | This is Liang&#39;s blog for life and work archive.</title>

  
  <meta name="author" content="Liang Xu">
  

  
  <meta name="description" content="There are tons of documents out there to explain how neural network works in different angles. So I guess people don’t mind me adding one more from my perspective. Hopefully, someone may get inspired from this post.">
  

  
  
  <meta name="keywords" content="Python,Machine learning,neural networks,Backward propagation,gradient descent">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="Machine learning on biology S2: how does a neural network work mathematically?"/>

  <meta property="og:site_name" content="Liang Xu"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Liang Xu" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Liang Xu</a>
    </h1>
    <p class="site-description">This is Liang&#39;s blog for life and work archive.</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/about">About</a></li>
      
        <li><a href="/categories">Categories</a></li>
      
        <li><a href="/tags">Tags</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>Machine learning on biology S2: how does a neural network work mathematically?</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2019/02/08/2019-02-08-Machinelearningseries2/" rel="bookmark">
        <time class="entry-date published" datetime="2019-02-08T08:00:00.000Z">
          2019-02-08
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>There are tons of documents out there to explain how neural network works in different angles. So I guess people don’t mind me adding one more from my perspective. Hopefully, someone may get inspired from this post.</p>
<span id="more"></span>

<p>In short, the neural network is a kind of system that transforms the input data into its corresponding output (or labels), strictly speaking, for a supervised learning. This system is consisting of three types of layers, i.e. the input layer, the hidden layer and the output layer. Normally, the hidden layer may have multiple layers according to one’s design. Among layers, the data from the upper layer are transformed to the data in the lower layer via a linear combination with an initialized weight matrix. After that, a nonlinear transformation that is generally called the activation function is applied to the data that would be converted to a form for the next round until reaching the output layer. To assure the neural network feedback the correct output, people need to train the neural network by adjusting the weight matrix. They are usually adjusted via minimizing the error between the output from the final layer and the known output from the data used for training. </p>
<p>Without loss of generality, here I consider a simplest neural network which only possesses fully connected layers, meaning that every neuron in each layer connects all neurons of the upper and lower layers. The mathematical logic behind is analogous to other structures of neural network. Firstly, I used a series of graphs to illustrate how the data flows to the final layer and introduce the notations used in the derivation. Then, a general formulation is present subsequently.</p>
<h1 id="A-6-layer-neural-network"><a href="#A-6-layer-neural-network" class="headerlink" title="A 6-layer neural network"></a>A 6-layer neural network</h1><p>The following four graphs illustrate how the data evolves along the neural network and the notations used in the derivation.</p>
<p><img src="/2019-02-08-Machinelearningseries2/step1.png" alt="fig">    </p>
<p><img src="/2019-02-08-Machinelearningseries2/step2.png" alt="fig">    </p>
<p><img src="/2019-02-08-Machinelearningseries2/step3.png" alt="fig">    </p>
<p><img src="/2019-02-08-Machinelearningseries2/step4.png" alt="fig">   </p>
<p><img src="/2019-02-08-Machinelearningseries2/step5.png" alt="fig"> </p>
<p><img src="/2019-02-08-Machinelearningseries2/step8.png" alt="fig">         </p>
<p><img src="/2019-02-08-Machinelearningseries2/step7.png" alt="fig">     </p>
<p><img src="/2019-02-08-Machinelearningseries2/step6.png" alt="fig">     </p>
<h1 id="1-Forward-flow-of-the-neural-network"><a href="#1-Forward-flow-of-the-neural-network" class="headerlink" title="1 Forward flow of the neural network"></a>1 Forward flow of the neural network</h1><p>Consider a neural network with \(k+1\) layers including the input layer and the output layer. The input data is consisting of \(n\) samples with \(L_{1}\) features</p>
<p><img src="/2019-02-08-Machinelearningseries2/1.png" alt="fig">   </p>
<p>The output label is given accordingly</p>
<p><img src="/2019-02-08-Machinelearningseries2/2.png" alt="fig">   </p>
<p>where the label on the top left denotes the index of the sample. Too abstract? Imagine that sample \(^{1}\boldsymbol{x}\) corresponds to model 1 while sample \(^{2}\boldsymbol{x}\) corresponds to model 2. Then the output can be either an array of two vectors indicating the probabilities of model 1 and model 2, for example, \({(1,0),(0,1)}\), or a vector of two elements \({0,1}\) where 0 indicates model 1 and 1 indicates model 2 or whatever you label them. The purpose is to use a huge amount of data sets to train the neural network, more precisely to compute the weight matrix among the adjacent pair of layers, to fit the output layer to the output vectors. Will see it later on.</p>
<p>Now, let’s feed one sample (the first one \(^{1}\boldsymbol{x}\)) to the neural network to see how to compute the output layer. Later, we will see how to train the network with multiple samples. Assume the second layer has \(L_{2}\) neurons</p>
<p><img src="/2019-02-08-Machinelearningseries2/3.png" alt="fig">   </p>
<p>Note that the first layer is the input layer in which we feed one sample with \(L_{1}\) feature to the neural network at first. Then the weight matrix from the first layer to the second layer is defined as </p>
<p><img src="/2019-02-08-Machinelearningseries2/4.png" alt="fig">   </p>
<p>Thus, multiplying the weight matrix Eq.4 with the neurons of the upper layer Eq.3 yields</p>
<p><img src="/2019-02-08-Machinelearningseries2/5.png" alt="fig">   </p>
<p>More generally, a bias term (constant term) is incorporated as follows</p>
<p><img src="/2019-02-08-Machinelearningseries2/6.png" alt="fig"> </p>
<p>which can also be written in the form if we absorb the constant vector \(\boldsymbol{b}_{1}\) into the weight matrix  </p>
<p><img src="/2019-02-08-Machinelearningseries2/78.png" alt="fig"> </p>
<p>After the transformation, an activation function is applied to \(\boldsymbol{z}^{(2)}\) elementwisely. For a model classification problem, the sigmoid function and the hyperbolic tangent function are widely used. Here we use the sigmoid function for instance</p>
<p><img src="/2019-02-08-Machinelearningseries2/9.png" alt="fig"> </p>
<p>Till now, the transformation from the first layer (the input layer) to the second layer (the first hidden layer) is done. This process can be generalized as .</p>
<p><img src="/2019-02-08-Machinelearningseries2/9-1.png" alt="fig"> </p>
<p>where \(\boldsymbol{a}’\) is the vector \(\boldsymbol{a}\) absorbing 1 at the end as what I did in Eq.8. Note that \(\boldsymbol{a}^{(1)}&#x3D;^{1}\boldsymbol{x}\).</p>
<p>Finally, the neural network will return \(\boldsymbol{a}^{(k+1)}\) with \(L_{k+1}\) elements from the output layer. Given the corresponding output label \(\boldsymbol{y}^{(1)}\), we can compute the error between the feedback \(\boldsymbol{a}^{(k+1)}\) and the output label \(\boldsymbol{y}^{(1)}\). This is normally called the loss of the result to the output. There are several candidate loss functions for model classification like the least square, the cross-entropy function. We take the least square function as the example</p>
<p><img src="/2019-02-08-Machinelearningseries2/10.png" alt="fig"> </p>
<p>So far, we have computed out the loss of the neural network with a bunch of randomly initialized weight matrix. No doubt, the loss would be huge. Our aim is to minimized the loss \(J\) by tuning the weight matrix. How? Remember your advanced calculus in the high school or the university? \(J\) can be envisaged as a function of every entry in the weight matrix that we want to adjust. Thus, the derivative of \(J\) with respect to each entries of the weight matrix would tell us how to tune the weight matrix to minimize the loss. This method is called gradient descend. And the loss can also propagate backwards to determine the gradient of the entries of the weight matrix at each layer. The full process to tune the weight matrix is also called Backward Propagation. </p>
<h1 id="2-Backward-propagation"><a href="#2-Backward-propagation" class="headerlink" title="2 Backward propagation"></a>2 Backward propagation</h1><h2 id="From-the-k-1-th-layer-to-the-k-th-layer"><a href="#From-the-k-1-th-layer-to-the-k-th-layer" class="headerlink" title="From the \(k+1\)th layer to the \(k\)th layer"></a>From the \(k+1\)th layer to the \(k\)th layer</h2><p>Let us start from the final layer (the \(k+1\)th layer) to the previous one (the \(k\)th layer). Note that the variable of our concern is the entry of the weight matrix from the \(k\)th layer to the \(k+1\)th layer, \(\theta_{L_{k+1}\times L_{k}}^{(k)}\). Here I only consider tuning \(\theta\) instead of \(\theta’\), meaning that the bias terms are not tuned. Updating the bias terms is similar and you can practice it afterward. The derivative of the loss function with respect to the matrix is defined as </p>
<p><img src="/2019-02-08-Machinelearningseries2/11.png" alt="fig"> </p>
<p>At the mean time, from the loss function Eq.10, we obtain the derivative according to the chain rule</p>
<p><img src="/2019-02-08-Machinelearningseries2/12.png" alt="fig"> </p>
<p>where</p>
<p><img src="/2019-02-08-Machinelearningseries2/13-15.png" alt="fig"> </p>
<p>and</p>
<p><img src="/2019-02-08-Machinelearningseries2/16.png" alt="fig"> </p>
<p>where \(\otimes\) is the outer product. Simplifying the gradient Eq.12 yields</p>
<p><img src="/2019-02-08-Machinelearningseries2/17.png" alt="fig"> </p>
<p>If we define</p>
<p><img src="/2019-02-08-Machinelearningseries2/18.png" alt="fig"> </p>
<p>The gradient Eq.17 can be further simplified as</p>
<p><img src="/2019-02-08-Machinelearningseries2/19.png" alt="fig"> </p>
<p>Note that we define the outer product of two column vectors is the element-wise produce of the corresponding elements</p>
<p><img src="/2019-02-08-Machinelearningseries2/19-1.png" alt="fig"> </p>
<p>Now, as \(\boldsymbol{z}^{(k+1)}\), \(\boldsymbol{a}^{(k+1)}\) and \(^{1}\boldsymbol{y}\) are known, we can update the weight matrix \(\theta^{(k)}\) by</p>
<p><img src="/2019-02-08-Machinelearningseries2/20.png" alt="fig"> </p>
<p>where \(\lambda\) is a constant called the learning rate given in advance. </p>
<h2 id="From-the-k-th-layer-to-the-k-1-th-layer"><a href="#From-the-k-th-layer-to-the-k-1-th-layer" class="headerlink" title="From the \(k\)th layer to the \(k-1\)th layer"></a>From the \(k\)th layer to the \(k-1\)th layer</h2><p>Let’s do the calculation one more time from the \(k\)th layer to the \(k-1\)th layer. At the end of this section, we will get a general formula to update all weight matrix via which we can develop the algorithm for a deep neural network. </p>
<p>Now we consider one more previous weight matrix \(\theta^{(k-1)}\). The derivative of the loss function with respect to that matrix yields </p>
<p><img src="/2019-02-08-Machinelearningseries2/21.png" alt="fig"> </p>
<p>The first two terms on the right hand side are the same as Eq.12. The third term \(\frac{\partial\boldsymbol{z}^{(k+1)}}{\partial\boldsymbol{a}^{(k)}}\) produces the weight matrix from the \(k+1\)th layer to the \(k\)th layer</p>
<p><img src="/2019-02-08-Machinelearningseries2/22.png" alt="fig"> </p>
<p>The last two terms on the right hand side are similar to what we have done above. Finally, we get</p>
<p><img src="/2019-02-08-Machinelearningseries2/23.png" alt="fig"> </p>
<p>Comparing Eq.19 and Eq.23 tells us that we can update the error</p>
<p><img src="/2019-02-08-Machinelearningseries2/24.png" alt="fig"> </p>
<p>for each transition among layers. This is how the error propagates backwards along the neural network and where the name comes from. Note that this expression is a slightly different from the formula in Chapter 9.2 of the <a target="_blank" rel="noopener" href="https://www.coursera.org/course/ml">machine learning course</a> by Andrew Ng of Standford. Do you see why is that? </p>
<h1 id="3-Training-multiple-samples"><a href="#3-Training-multiple-samples" class="headerlink" title="3 Training multiple samples"></a>3 Training multiple samples</h1><p>We have derived mathematically how to train one sample on a neural network. How about multiple samples? Easy. Because all the weight matrix of the neural network are shared for all samples, we can update the weight matrix by a fraction of error of each sample. Normally this fraction is \(\frac{1}{\text{sample size}}\) where in our example the sample size is \(n\)</p>
<p><img src="/2019-02-08-Machinelearningseries2/25.png" alt="fig"> </p>
<p>Then, the weight matrix is tuned to minimize the error of samples. </p>
<h1 id="4-Program-a-neural-network"><a href="#4-Program-a-neural-network" class="headerlink" title="4 Program a neural network"></a>4 Program a neural network</h1><p>Till now, we have derived a general formula Eq.23 to allow us to update all weight matrix. After updating, the loss function is applied again to examine if the error is sufficiently small. If not, update the weight matrix again till meeting our criterion. Understanding the math behind is a huge step towards the expert level but not the final one. Whether you can equip it via code is essential. Here I attached my code in Python from my perspective as a reference. The user can add any number of hidden layers and deploy any number of neurons there. It is a bit like a minimalistic version of tensorflow. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">neuralnetwork</span>:</span><br><span class="line">    <span class="comment"># initialize parameters</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,num_sample,num_hidden_layer_units,num_input_feature,num_output_feature,bias,learningrate</span>):</span><br><span class="line">        self.learningrate=learningrate <span class="comment"># learning rate</span></span><br><span class="line">        self.num_sample = num_sample  <span class="comment"># number of the samples</span></span><br><span class="line">        self.num_hidden_layer_units = num_hidden_layer_units <span class="comment"># a list indicating the number of hidden layers and how many neurons for each layer</span></span><br><span class="line">        self.num_hidden_layer = <span class="built_in">len</span>(num_hidden_layer_units) <span class="comment"># the number of the hidden layers</span></span><br><span class="line">        self.weight_layer = []  <span class="comment"># initialize the weight matrix</span></span><br><span class="line">        self.bias = bias   <span class="comment"># bias for the input layer and for the hidden layers</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(self.bias) == self.num_hidden_layer+<span class="number">1</span>, <span class="string">&quot;The length of the biases should equal the length of the hidden layers plus 1!!!&quot;</span></span><br><span class="line">        self.units = [num_input_feature] + num_hidden_layer_units + [num_output_feature]</span><br><span class="line">        <span class="comment"># randomly initialize the weight matrix</span></span><br><span class="line">        <span class="keyword">for</span> num_layer <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="built_in">len</span>(self.units)-<span class="number">1</span>):</span><br><span class="line">            temp_bias = np.zeros((<span class="number">1</span>,self.units[num_layer+<span class="number">1</span>]))</span><br><span class="line">            temp_bias.fill(self.bias[num_layer])</span><br><span class="line">            self.weight_layer.append(np.concatenate((np.random.randn(self.units[num_layer+<span class="number">1</span>],</span><br><span class="line">                                                self.units[num_layer]),temp_bias.T),axis = <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># the activation function: sigmoid</span></span><br><span class="line">    <span class="comment"># could be replaced by whatever you want</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">self,x</span>):</span><br><span class="line">        y = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">    <span class="comment"># train function requires the training data, accuracy and iteration limit</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, <span class="built_in">input</span>, output,accuracy, iteration_limit</span>):</span><br><span class="line">        go_on = <span class="literal">True</span></span><br><span class="line">        i = <span class="number">0</span> <span class="comment"># iteration indicator</span></span><br><span class="line">        error = []  <span class="comment"># error list recoding errors for all iterations</span></span><br><span class="line">        iteration = []</span><br><span class="line">        <span class="keyword">while</span> go_on:</span><br><span class="line">            iteration.append(i)</span><br><span class="line">            error_acc = <span class="number">0</span>   <span class="comment"># initialize error for every iteration</span></span><br><span class="line">            stderror = []   <span class="comment"># standard error</span></span><br><span class="line">            sample_Z = []   <span class="comment"># Z value of neurons for every sample</span></span><br><span class="line">            sample_a = []   <span class="comment"># sigmoid value of Z for every sample</span></span><br><span class="line">            <span class="comment"># loop all samples</span></span><br><span class="line">            <span class="keyword">for</span> sample_iter <span class="keyword">in</span> <span class="built_in">range</span>(self.num_sample):</span><br><span class="line">                Zlayer = []</span><br><span class="line">                a = []</span><br><span class="line">                a.append(<span class="built_in">input</span>[sample_iter])    <span class="comment"># place the input as the first a value</span></span><br><span class="line">                <span class="comment"># forward computing Z and a values for all layers</span></span><br><span class="line">                <span class="keyword">for</span> forward_layer <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.units)-<span class="number">1</span>):</span><br><span class="line">                    Zlayer.append(np.matmul(self.weight_layer[forward_layer], np.concatenate((a[forward_layer],[<span class="number">1</span>]))))</span><br><span class="line">                    a.append(self.sigmoid(Zlayer[forward_layer]))</span><br><span class="line">                <span class="comment"># standard error by computing the distance between output layer and true output</span></span><br><span class="line">                stderror.append(a[<span class="built_in">len</span>(a)-<span class="number">1</span>]-output[sample_iter])</span><br><span class="line">                <span class="comment"># loss function defined as the sum of the least square</span></span><br><span class="line">                <span class="comment"># can be replaced by other functions like cross-entropy</span></span><br><span class="line">                error_acc += (<span class="number">1</span>/<span class="number">2</span>*np.<span class="built_in">sum</span>((stderror[sample_iter])**<span class="number">2</span>))</span><br><span class="line">                sample_a.append(a)  <span class="comment"># store a values for updating</span></span><br><span class="line">                sample_Z.append(Zlayer) <span class="comment"># store Z values for updating</span></span><br><span class="line">            error.append(error_acc) <span class="comment"># store error for this iteration</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># backward propagate errors to update the weight matrix</span></span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> error[i-<span class="number">1</span>] &lt;accuracy <span class="keyword">or</span> i &gt;iteration_limit:</span><br><span class="line">                go_on = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> sample_iter <span class="keyword">in</span> <span class="built_in">range</span>(self.num_sample):</span><br><span class="line">                    delta = stderror[sample_iter]  <span class="comment"># delta: standard error defined as the derivative of the loss function</span></span><br><span class="line">                    <span class="comment"># initialize g&#x27;(z): the derivative of the activation function at the output layer</span></span><br><span class="line">                    deriv_sigmoid =  sample_a[sample_iter][<span class="built_in">len</span>(sample_a[sample_iter])-<span class="number">1</span>] *\</span><br><span class="line">                                 (<span class="number">1</span> - sample_a[sample_iter][<span class="built_in">len</span>(sample_a[sample_iter])-<span class="number">1</span>])</span><br><span class="line">                    <span class="comment"># backward propagation</span></span><br><span class="line">                    <span class="keyword">for</span> backward_layer <span class="keyword">in</span> <span class="built_in">list</span>(<span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(self.units)-<span class="number">1</span>))):</span><br><span class="line">                        temp_weight = self.weight_layer[backward_layer] <span class="comment"># store the temporary kth matrix</span></span><br><span class="line">                        <span class="comment"># update the kth matrix</span></span><br><span class="line">                        self.weight_layer[backward_layer][:,:-<span class="number">1</span>] += - self.learningrate/self.num_sample *\</span><br><span class="line">                            np.outer(delta * deriv_sigmoid, sample_a[sample_iter][backward_layer])</span><br><span class="line">                        <span class="comment"># update delta and derivative of the activation function</span></span><br><span class="line">                        delta = np.dot(temp_weight[:,:-<span class="number">1</span>].T,delta * deriv_sigmoid)</span><br><span class="line">                        deriv_sigmoid = sample_a[sample_iter][backward_layer] *\</span><br><span class="line">                                 (<span class="number">1</span> - sample_a[sample_iter][backward_layer])</span><br><span class="line">        self.learningspeed = &#123;<span class="string">&#x27;error&#x27;</span>: error, <span class="string">&#x27;iteration&#x27;</span>: iteration&#125;</span><br><span class="line">        self.lsdf = pd.DataFrame(self.learningspeed)</span><br><span class="line">        <span class="keyword">return</span> sample_a, self.weight_layer</span><br><span class="line"></span><br><span class="line">    <span class="comment"># predict</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self,<span class="built_in">input</span>,weight</span>):</span><br><span class="line">        out = <span class="built_in">input</span>+[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(weight)):</span><br><span class="line">            out = np.dot(weight[i],np.array(out))</span><br><span class="line">            out = self.sigmoid(out)</span><br><span class="line">            out = np.concatenate((out,[<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">return</span> out[:-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># test</span></span><br><span class="line">inputx = np.array(([<span class="number">0.2</span>, <span class="number">0.5</span>, <span class="number">0.5</span>,<span class="number">0.3</span>], [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.15</span>,<span class="number">0.1</span>],[<span class="number">0.7</span>,<span class="number">0.9</span>,<span class="number">0.4</span>,<span class="number">0.8</span>]))</span><br><span class="line">outputy = np.array(([<span class="number">0.5</span>, <span class="number">0.4</span>], [<span class="number">0.9</span>, <span class="number">0.1</span>],[<span class="number">0.3</span>,<span class="number">0.5</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># build the neural network</span></span><br><span class="line">nn=neuralnetwork(num_sample=inputx.shape[<span class="number">0</span>],num_hidden_layer_units=[<span class="number">15</span>,<span class="number">14</span>,<span class="number">15</span>,<span class="number">16</span>],num_input_feature=inputx.shape[<span class="number">1</span>],</span><br><span class="line">             num_output_feature=outputy.shape[<span class="number">1</span>], bias=[<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.4</span>,<span class="number">0.4</span>],learningrate=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set the accuracy and the iteration limit</span></span><br><span class="line">acc = <span class="number">1e-7</span></span><br><span class="line"><span class="built_in">iter</span> = <span class="number">20000</span></span><br><span class="line"><span class="comment"># train the neural network</span></span><br><span class="line">out_a,weight = nn.train(inputx,outputy,accuracy = acc,iteration_limit=<span class="built_in">iter</span>)</span><br><span class="line"><span class="comment"># plot the learning process</span></span><br><span class="line">nn.lsdf.plot(x=<span class="string">&#x27;iteration&#x27;</span>,y=<span class="string">&#x27;error&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict</span></span><br><span class="line"><span class="built_in">input</span> = [<span class="number">0.19</span>, <span class="number">0.51</span>, <span class="number">0.49</span>,<span class="number">0.29</span>]</span><br><span class="line">out = nn.predict(<span class="built_in">input</span>,weight)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>You can also clone it <a target="_blank" rel="noopener" href="https://github.com/xl0418/Tensorflow/blob/master/NeuralNetwork.py">on my Github</a> </p>
<h1 id="The-end"><a href="#The-end" class="headerlink" title="The end"></a>The end</h1><p>My code is obviously not the most efficient one as a lot of loops are used. But in another sense it is easy to read for a starter. As you see in the derivation and in the code, there are a huge amount of independent computing that can be parallelized to speed up. So the future plan is to parallelize the code on GPU which may substantially improve the efficiency of the neural network. It would also be a good practice for you to step into the machine learning field. </p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><p>An introduction to the math used in Machine Learning:  <a target="_blank" rel="noopener" href="https://explained.ai/matrix-calculus/index.html">The Matrix Calculus You Need For Deep Learning
</a>.</p>
</li>
<li><p>A concrete derivation of backward propagation for a two-layer neural network in Chinese <a target="_blank" rel="noopener" href="http://www.cnblogs.com/charlotte77/p/5629865.html#!comments">here</a>.</p>
</li>
</ul>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/Research/">Research</a>, <a href="/categories/Research/Machine-learning/">Machine learning</a>, <a href="/categories/Research/Machine-learning/Deep-learning/">Deep learning</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Python/">Python</a><a href="/tags/Machine-learning/">Machine learning</a><a href="/tags/neural-networks/">neural networks</a><a href="/tags/Backward-propagation/">Backward propagation</a><a href="/tags/gradient-descent/">gradient descent</a>
    </span>
    

    </div>

    
  </div>
</article>

  









    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2023 Liang Xu
    
  </p>
</footer>
    
    
  </div>
</div>
</body>
</html>