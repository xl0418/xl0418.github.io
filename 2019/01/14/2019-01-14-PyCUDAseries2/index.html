<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Liang Xu">


    <meta name="subtitle" content="This is Liang's blog for life and work.">




<title>PyCUDA series 2: a simple matrix algebra and speed test | Liang Xu</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    


<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Liang Xu</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/research">Research</a>
                
                    <a class="menu-item" href="/publication">Publications</a>
                
                    <a class="menu-item" href="/packages">Packages</a>
                
                    <a class="menu-item" href="/archives">Blog</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Liang Xu</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/research">Research</a>
                
                    <a class="menu-item" href="/publication">Publications</a>
                
                    <a class="menu-item" href="/packages">Packages</a>
                
                    <a class="menu-item" href="/archives">Blog</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6; // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function() {
            tocbot.refresh(obj_merge(tocbot_default_config, {
                hasInnerContainers: true
            }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function() {
        tocbot.init(obj_merge(tocbot_default_config, {
            collapseDepth: 1
        }));
    });

    function expand_toc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, {
            collapseDepth: expanded ? 1 : DEPTH_MAX
        }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">PyCUDA series 2: a simple matrix algebra and speed test</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Liang Xu</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">January 14, 2019&nbsp;&nbsp;0:00:00</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/Research/">Research</a>
                            
                                <a href="/categories/Research/GPU-programming/">GPU programming</a>
                            
                                <a href="/categories/Research/GPU-programming/pyCUDA/">pyCUDA</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <p>In this post, you will find a simple matrix algebra done by gpu parallelization and a straightforward result of the speed contest between cpu and gpu.</p>
<span id="more"></span>
<h1 id="A-simple-matrix-algebra-to-be-parallelized"><a href="#A-simple-matrix-algebra-to-be-parallelized" class="headerlink" title="A simple matrix algebra to be parallelized"></a>A simple matrix algebra to be parallelized</h1><p>As a first try, I intended to parallleize a simple matrix algebra in my Project 2 that can be computed as follows on Python:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">c = a_cpu[:,np.newaxis]-a_cpu</span><br></pre></td></tr></table></figure>

<p>This is a formula to compute the discrepancy of any pair of elements given by a vector. For example, input </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<p>the formula should return </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">c = array([[<span class="number">0</span>,-<span class="number">1</span>,-<span class="number">2</span>,],</span><br><span class="line">	[<span class="number">1</span>,<span class="number">0</span>,-<span class="number">1</span>],</span><br><span class="line">	[<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>]])</span><br></pre></td></tr></table></figure>

<p>A standard python code like the formula above by using <code>numpy</code> package, every element in <code>c_cpu</code> is computed in a serial order. </p>
<p>In terms of the power of current cpu, for a lower dimensional matrix, you can hardly feel the elapsed time to compute out the equation. However, if you have a huge matrix of 10000X10000 size and want to loop it 100 times, it costs half a minute on an i7 4790k cpu, although you may ignore the little time consumed. But it is not rare to have a large matrix bigger than 1 million by 1 million and loop it billion times at least in an evolutionary problem. In this case, you are saving your life if the compulation time can be reduced.</p>
<h1 id="A-shortcut-to-understand-GPU-parallel-computation"><a href="#A-shortcut-to-understand-GPU-parallel-computation" class="headerlink" title="A shortcut to understand GPU parallel computation"></a>A shortcut to understand GPU parallel computation</h1><p>Now we have a good way to save our life instead of having a weird pill. CUDA is a parallel computing platform and application programming interface (API) model created by <a target="_blank" rel="noopener" href="https://developer.nvidia.com/nvidia-developer-zone">Nvidia</a>. <a target="_blank" rel="noopener" href="https://documen.tician.de/pycuda/index.html">PyCUDA</a> provides a python package to allow people to parallelize their computation on a Graphic Processing Unit (GPU) by Python. For sure, I didn’t say everything is parallelizable. Only the procedures that are consisting of independent calculation paths can be parallelized, which means you work on different paths at the same time instead of doing them one by one. Hence, the amount of time that can be saved relies on the percentage of work that is parallelizable in your work. In this sense, matrix algebra is a good representative. Other applications of parallelism involves rendering graphics to a screen, running a Monte Carlo Simulation, multiplying matrices for a machine learning algorithm, or powering a database. Google them yourself. </p>
<p>Why does GPU possess such powerful ability? It owes to its design purpose. A graphic card is designed to process graphics and render them to a screen. What is a graphic? It is just like a matrix. So to process a matrix, the ability to compute multiple entries of the matrix is more important than the ability to deal with just one entry smartly. Hence, a gpu is designed to own a huge amount of threads that can process multiple calculations at one time although they are slow while a cpu only contains few (like an i7 4790k has 4 physical cores which means 4  physical threads, at most 8 threads if including hyperthreads tech) but are faster. A good analogy is that a gpu is like a cluster of students at primary school while a cpu is like a professor. When calculating a complex problem, the professor is definitely faster and smarter. But when computing lots of simple algebra, the cluster of primary students is faster. In summary, CPUs are designed for running a small number of potentially quite complex tasks. GPUs are designed for running a large number of quite<br>simple tasks.</p>
<h1 id="Parallelize-the-matrix-algebra"><a href="#Parallelize-the-matrix-algebra" class="headerlink" title="Parallelize the matrix algebra"></a>Parallelize the matrix algebra</h1><p>After having a rough idea of parallelism, let’s do it on practice. For a matrix algebra like the above example, what are the independent calculation paths? Apparently, each entry of the output matrix <code>c</code> is computed out independently given the input <code>a</code>. Thus, we could assign each calculation path to a thread on GPU. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">kernel_code_template = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">__global__ void com_t(int matrixsize,float *a, float *c)</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    // 2D Thread ID </span></span><br><span class="line"><span class="string">    int tx = blockDim.x*blockIdx.x + threadIdx.x; // Compute column index</span></span><br><span class="line"><span class="string">    int ty = blockDim.y*blockIdx.y + threadIdx.y; // Compute row index</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    // Pvalue is used to store the element of the matrix</span></span><br><span class="line"><span class="string">    // that is computed by the thread</span></span><br><span class="line"><span class="string">    float Pvalue = 0;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    // Each thread loads one row of M and one column of N, </span></span><br><span class="line"><span class="string">    //   to produce one element of P.</span></span><br><span class="line"><span class="string">    if((ty &lt;matrixsize) &amp;&amp; (tx &lt; matrixsize))</span></span><br><span class="line"><span class="string">    &#123;</span></span><br><span class="line"><span class="string">    float Aelement = a[ty];</span></span><br><span class="line"><span class="string">    float Belement = a[tx];</span></span><br><span class="line"><span class="string">    Pvalue = Aelement - Belement;</span></span><br><span class="line"><span class="string">    // Write the matrix to device memory;</span></span><br><span class="line"><span class="string">    // each thread writes one element</span></span><br><span class="line"><span class="string">    c[ty * matrixsize + tx] = Pvalue;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>


<p>The most important part of parallelism on GPU is the kernel function which is coded for a single calculation path. The only difference among different calculation paths is the locality information of the entries of matrix <code>c</code> in the memory. Normally, we can envisage a memory on GPU as a cluster of grids of blocks of threads. But in fact, it is not this physical structure on the borad of the card. It involves streaming processors (SMs) dealing with wraps. However, here we better use the metaphor.</p>
<p><img src="/gridblockthread.png" alt="default"></p>
<p>Each entry in matrix <code>c</code> corresponds to a specific combination of <code>gridIdx.x,gridIdx.y,blockIdx.x,blockIdx.y,threadIdx.x,threadIdx.y</code>. The value of the indexes also depend on how we allocate the size of grid, block. A regular way for a matrix algebra is to fit the struture to the matrix. For example, if we want to compute a 5X5 matrix, we can allocate the size of block as 5X5 which means we will use a matrix of 5X5 threads for compuatation. But a block contains at most 1024 threads for the current card (GTX 970). Thus for a square matrix we can allocate at most a size of 32X32 threads for one block. If we want to work on a larger matrix, we need more blocks or even more grids. This then causes one issue that some threads may not be used if the matrix size is not a multiplier of the block size. Therefore, we need to constrain our computation within the matrix by using <code>if((ty &lt;matrixsize) &amp;&amp; (tx &lt; matrixsize))</code>. And make sure the evaluation of matrix <code>c</code> is put in the loop. Otherwise, the over requested threads (if the requested threads don’t fit the matrix size) will be invoked and replace the inner results in <code>c</code>.</p>
<p>Once we have correctly allocated the memory and located the entry by these indexes, we can formulate your calculation. After that, the code will be uploaded to GPU and tranferred to the code of GPU (compile). Once all entries have been calculated, the results will be stored in the memory on GPU. Then cpu will pull back the result.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># compile the kernel code</span></span><br><span class="line">mod = compiler.SourceModule(kernel_code_template)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get the kernel function from the compiled module</span></span><br><span class="line">matrixmul = mod.get_function(<span class="string">&quot;com_t&quot;</span>)</span><br><span class="line"></span><br><span class="line">matrixsize = <span class="number">3</span></span><br><span class="line">BLOCK_SIZE = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># call the kernel on the card</span></span><br><span class="line">matrixmul(np.uint32(matrixsize),</span><br><span class="line">    <span class="comment"># inputs</span></span><br><span class="line">    a_gpu,</span><br><span class="line">    <span class="comment"># output</span></span><br><span class="line">    c_gpu,</span><br><span class="line">    <span class="comment"># 4 blocks of BLOCK_SIZE x BLOCK_SIZE threads</span></span><br><span class="line">    grid = (<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>),</span><br><span class="line">    block = (BLOCK_SIZE, BLOCK_SIZE, <span class="number">1</span>),</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<h1 id="Speed-comparison-between-CPU-and-GPU"><a href="#Speed-comparison-between-CPU-and-GPU" class="headerlink" title="Speed comparison between CPU and GPU"></a>Speed comparison between CPU and GPU</h1><p>At last, to show the power of GPU even on such simple matrix algebra, I run calculation for different dimensions and loop each calculation 100 times to enlarge the time consumption.</p>
<p><img src="/speedtest.png" alt="default"></p>
<p>As the figure shows, the time consumption for 100 times calculations starts to significantly split up when the dimension of matrix is over 2000. The time consumption on CPU grows almost exponentially along the dimension while on gpu it only grows a little. With the increase of dimension, you save a huge amount of your time and your life.</p>
<h1 id="End"><a href="#End" class="headerlink" title="End"></a>End</h1><p>This is a quite simplified introduction to a parallelism example. There is a bank of posts explaining parallelism better and more exhaustive than this one. However, my goal here is to use a simple example and a short length of words to have you get an idea of how it works instead of scaring you away from this field. Once you fall into the trap of “it looks as simple as the author said”, I believe you will learn more by yourself :-)</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[CUDA Programming] by Shane Cook is a good start book for learning both the hardware and the language. </p>
<p>This <a target="_blank" rel="noopener" href="https://andreask.cs.illinois.edu/PyCuda">PyCUDA website</a> provides some examples on Python.</p>
<h1 id="Full-code"><a href="#Full-code" class="headerlink" title="Full code"></a>Full code</h1><p>It contains the example code and the speed test. Clone it <a target="_blank" rel="noopener" href="https://github.com/xl0418/GPU_Python/blob/master/gpu_cpu_speedtest.py">here</a></p>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Liang Xu</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://xl0418.github.io/2019/01/14/2019-01-14-PyCUDAseries2/">http://xl0418.github.io/2019/01/14/2019-01-14-PyCUDAseries2/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>Do you believe in <strong>DESTINY</strong>?</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/Python/"># Python</a>
                    
                        <a href="/tags/CUDA/"># CUDA</a>
                    
                        <a href="/tags/GPU-programming/"># GPU programming</a>
                    
                        <a href="/tags/Parallel-computation/"># Parallel computation</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2019/01/21/2019-01-21-PyCUDAseries3/">PyCUDA series 3: matrix multiplication using multiple blocks</a>
            
            
            <a class="next" rel="next" href="/2019/01/10/2019-01-10sed2/">Sed again: run on Ubuntu to avoid weird behavior</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Liang Xu | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>

</html>