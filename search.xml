<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[PyCUDA series 2: a simple matrix algebra and speed test]]></title>
    <url>%2F2019%2F01%2F14%2F2019-01-14-PyCUDAseries2%2F</url>
    <content type="text"><![CDATA[In this post, you will find a simple matrix algebra done by gpu parallelization and a straightforward result of the speed contest between cpu and gpu. A simple matrix algebra to be parallelizedAs a first try, I intended to parallleize a simple matrix algebra in my Project 2 that can be computed as follows on Python: 1c = a_cpu[:,np.newaxis]-a_cpu This is a formula to compute the discrepancy of any pair of elements given by a vector. For example, input 1a = np.array([1,2,3]) the formula should return 123c = array([[0,-1,-2,], [1,0,-1], [2,1,0]]) A standard python code like the formula above by using numpy package, every element in c_cpu is computed in a serial order. In terms of the power of current cpu, for a lower dimensional matrix, you can hardly feel the elapsed time to compute out the equation. However, if you have a huge matrix of 10000X10000 size and want to loop it 100 times, it costs half a minute on an i7 4790k cpu, although you may ignore the little time consumed. But it is not rare to have a large matrix bigger than 1 million by 1 million and loop it billion times at least in an evolutionary problem. In this case, you are saving your life if the compulation time can be reduced. A shortcut to understand GPU parallel computationNow we have a good way to save our life instead of having a weird pill. CUDA is a parallel computing platform and application programming interface (API) model created by Nvidia. PyCUDA provides a python package to allow people to parallelize their computation on a Graphic Processing Unit (GPU) by Python. For sure, I didn’t say everything is parallelizable. Only the procedures that are consisting of independent calculation paths can be parallelized, which means you work on different paths at the same time instead of doing them one by one. Hence, the amount of time that can be saved relies on the percentage of work that is parallelizable in your work. In this sense, matrix algebra is a good representative. Other applications of parallelism involves rendering graphics to a screen, running a Monte Carlo Simulation, multiplying matrices for a machine learning algorithm, or powering a database. Google them yourself. Why does GPU possess such powerful ability? It owes to its design purpose. A graphic card is designed to process graphics and render them to a screen. What is a graphic? It is just like a matrix. So to process a matrix, the ability to compute multiple entries of the matrix is more important than the ability to deal with just one entry smartly. Hence, a gpu is designed to own a huge amount of threads that can process multiple calculations at one time although they are slow while a cpu only contains few (like an i7 4790k has 4 physical cores which means 4 physical threads, at most 8 threads if including hyperthreads tech) but are faster. A good analogy is that a gpu is like a cluster of students at primary school while a cpu is like a professor. When calculating a complex problem, the professor is definitely faster and smarter. But when computing lots of simple algebra, the cluster of primary students is faster. In summary, CPUs are designed for running a small number of potentially quite complex tasks. GPUs are designed for running a large number of quitesimple tasks. Parallelize the matrix algebraAfter having a rough idea of parallelism, let’s do it on practice. For a matrix algebra like the above example, what are the independent calculation paths? Apparently, each entry of the output matrix c is computed out independently given the input a. Thus, we could assign each calculation path to a thread on GPU. 1234567891011121314151617181920212223242526kernel_code_template = """__global__ void com_t(int matrixsize,float *a, float *c)&#123; // 2D Thread ID int tx = blockDim.x*blockIdx.x + threadIdx.x; // Compute column index int ty = blockDim.y*blockIdx.y + threadIdx.y; // Compute row index // Pvalue is used to store the element of the matrix // that is computed by the thread float Pvalue = 0; // Each thread loads one row of M and one column of N, // to produce one element of P. if((ty &lt;matrixsize) &amp;&amp; (tx &lt; matrixsize)) &#123; float Aelement = a[ty]; float Belement = a[tx]; Pvalue = Aelement - Belement; // Write the matrix to device memory; // each thread writes one element c[ty * matrixsize + tx] = Pvalue; &#125;&#125;""" The most important part of parallelism on GPU is the kernel function which is coded for a single calculation path. The only difference among different calculation paths is the locality information of the entries of matrix c in the memory. Normally, we can envisage a memory on GPU as a cluster of grids of blocks of threads. But in fact, it is not this physical structure on the borad of the card. It involves streaming processors (SMs) dealing with wraps. However, here we better use the metaphor. Each entry in matrix c corresponds to a specific combination of gridIdx.x,gridIdx.y,blockIdx.x,blockIdx.y,threadIdx.x,threadIdx.y. The value of the indexes also depend on how we allocate the size of grid, block. A regular way for a matrix algebra is to fit the struture to the matrix. For example, if we want to compute a 5X5 matrix, we can allocate the size of block as 5X5 which means we will use a matrix of 5X5 threads for compuatation. But a block contains at most 1024 threads for the current card (GTX 970). Thus for a square matrix we can allocate at most a size of 32X32 threads for one block. If we want to work on a larger matrix, we need more blocks or even more grids. This then causes one issue that some threads may not be used if the matrix size is not a multiplier of the block size. Therefore, we need to constrain our computation within the matrix by using if((ty &lt;matrixsize) &amp;&amp; (tx &lt; matrixsize)). And make sure the evaluation of matrix c is put in the loop. Otherwise, the over requested threads (if the requested threads don’t fit the matrix size) will be invoked and replace the inner results in c. Once we have correctly allocated the memory and located the entry by these indexes, we can formulate your calculation. After that, the code will be uploaded to GPU and tranferred to the code of GPU (compile). Once all entries have been calculated, the results will be stored in the memory on GPU. Then cpu will pull back the result. 1234567891011121314151617181920# compile the kernel codemod = compiler.SourceModule(kernel_code_template)# get the kernel function from the compiled modulematrixmul = mod.get_function("com_t")matrixsize = 3BLOCK_SIZE = 2# call the kernel on the cardmatrixmul(np.uint32(matrixsize), # inputs a_gpu, # output c_gpu, # 4 blocks of BLOCK_SIZE x BLOCK_SIZE threads grid = (2,2,1), block = (BLOCK_SIZE, BLOCK_SIZE, 1), ) Speed comparison between CPU and GPUAt last, to show the power of GPU even on such simple matrix algebra, I run calculation for different dimensions and loop each calculation 100 times to enlarge the time consumption. As the figure shows, the time consumption for 100 times calculations starts to significantly split up when the dimension of matrix is over 2000. The time consumption on CPU grows almost exponentially along the dimension while on gpu it only grows a little. With the increase of dimension, you save a huge amount of your time and your life. EndThis is a quite simplified introduction to a parallelism example. There is a bank of posts explaining parallelism better and more exhaustive than this one. However, my goal here is to use a simple example and a short length of words to have you get an idea of how it works instead of scaring you away from this field. Once you fall into the trap of “it looks as simple as the author said”, I believe you will learn more by yourself :-) Reference[CUDA Programming] by Shane Cook is a good start book for learning both the hardware and the language. This PyCUDA website provides some examples on Python. Full codeIt contains the example code and the speed test. Clone it here]]></content>
      <categories>
        <category>Research</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>CUDA</tag>
        <tag>GPU programming</tag>
        <tag>Parallel computation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sed again: run on Ubuntu to avoid weird behavior]]></title>
    <url>%2F2019%2F01%2F10%2F2019-01-10sed2%2F</url>
    <content type="text"><![CDATA[As my 2nd project is close to an end, it is right the time to go back to the phylogenetic Janzen-Connell model. Simulations and plotting functions have been done during the gap time squeezed from the 2nd project. The last thing is to analyze the data, in which I bumped into a weird behavior of sed. The data fileIn this project, I have generated in total 108 data file in .m (Matlab) format. Each file contain several distance matrix, abundance vectors, an event table and turnover ticks that I want to extract and analyze exclusively. Extract the matrixThe procedure is simple. I modified the previous experimental bash scripts a bit to fit the file names of data. Then execute them sequentially on Windows prompt. Run extractallDs.sh and extractallRs.sh to extract all D matrix and R matrix from the raw files into LRsij.Rdata,MRsij.Rdata,HRsij.Rdata,LDsij.Rdata,MDsij.Rdata,HDsij.Rdata. Run Dlast.sh to extract the last D and R matrix into LDij.Rdata,LRij.Rdata,MDij.Rdata,MRij.Rdata,HDij.Rdata,HRij.Rdata. Do the same thing to extract event table and turnover ticks by running loopextractEve_Turn.sh like But I obtained nothing in the files. Bug shootingSame simulation code generates the same data structure. But why couldn’t the same bash script extract information as before? At that moment, I kind of had a feeling that it must be that the script cannot recognize the newline sign. like the following, if the computer cannot match \r, which is the code for the newline under windows and DOS, the script extracts nothing. 123456789#!/bin/bashfor j in &#123;0..4&#125;;dofor i in &#123;0..4&#125;;do sed -n '/D'&#123;'length(D)+1'&#125;' = \[\r/,/\];/p' test"$j$i".m &gt; Ds"$j$i".Rdataecho $j$i' done'donedone But after I changed it to \n that is the right code for a linux file, it didn’t work neither. Even after changing the file code via unix2dos, none of the means worked. \(\boldsymbol{Finally}\), after several hours of googling, thinking, patting the computer, I tried out the solution. The scripts need to be executed under the linux system. I launched them on Ubuntu again, the newline sign was recognized. But it is still a mystery to me that why on a windows prompt none of \r and \n can be recognized. If you know it, pls reply this post. Thanks!]]></content>
      <categories>
        <category>Research</category>
      </categories>
      <tags>
        <tag>project 3</tag>
        <tag>bash</tag>
        <tag>mega data</tag>
        <tag>extract information</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyCUDA series 1: Build GPU programming environment]]></title>
    <url>%2F2019%2F01%2F04%2F2019-01-04-PyCUDAseries1%2F</url>
    <content type="text"><![CDATA[As my research always involves a huge amount of computation, parallel technique is a super fancy way to save my time. But the resource of cluster at my university is limited. I recently received a warning that CIT has detected multiple-account usage under my IP. To avoid the risk of being fired, I have to figure out another way to parallelize my job instead of parallelizing accounts. Right on time, GPU programming came out as an ideal option. I hope the GPU resource of my university is not full loaded yet. Due to some incompatibilities of CUDA and Visual Studio 2017, I spent half a day to figure out the solutions and finally succeeded building up GPU programming environment on Windows 10 with Pycuda installed in Python. Here, I post the procedure of the build and some solutions to the incompatibilities. PrerequisiteFor now, I am working on Windows 10 with Python 3.7.0. The CUDA version that is compatibly built up for me is CUDA 10.0. I used Pycharm 2018.3 as the Python IDE. Probably I will try to build on OS 10 soon in the future. Simple procedure Install Python 3.7 and PyCharm 2018.3. Install Visual Studio 2017 as CUDA needs C++ compile. Open the Visual Studio installer under the folder Visual Studio 2017. Select: Modify under Visual Studio 2017 -&gt; Installation details. Install options: select only the Windows 10 SDK. Install CUDA 10.0 and follow the steps here to set up CUDA environment. Tips To make sure that CUDA is successfully installed, check nvcc -v at your terminal. The step to build samples in Nvidia’s guide is not necessary. I couldn’t build the samples due to some incompatibilities of vs c++ but still have it work on Python. Problem shooting CUDA install failed I only came across this issue on my desktop at office that has an old monitor. Then, I resolved it by customizing the installation with uncheck of Visual Studio integration. No idea why but it works. Also check answer here nvcc fatal : Cannot find compiler ‘cl.exe’ in PATH Check here. In principle, you should add cl.exe to the environment variables.]]></content>
      <categories>
        <category>Research</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>CUDA</tag>
        <tag>GPU programming</tag>
        <tag>Parallel computation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ggradar2: Help document]]></title>
    <url>%2F2018%2F12%2F07%2F2018-12-07-ggradar2helpdocument%2F</url>
    <content type="text"><![CDATA[In this blog, the details of the arguments used in ggradar2 are provided. ggradar2: argumentsplot.data :The input data should be in the data.frame format with columns named. The values in the data are supposed to be numeric. If you want to apply scales like ‘high’, ‘middle’, ‘low’. Please evaluate them first and put them as gridline.label . Notice that ‘group’ column is suggested to included in your data. ggradar2 now can smartly detect if ‘group’ is correctly provided. If not, you will be asked if the first column is allowed to defined as the group column. 1WARNING: &apos;group&apos; column is not detected. The first column will be chosen as the group name. Yes/no? (y/n) If you want to plot multiple plots against some subgroups, please specify it in the column data$facet1. base.size: The size of radar chart. The default value is 20. webtype: &quot;mini&quot; set a web type with 3 grid lines while &quot;lux&quot; set a type with 5 grid lines. Default setting is &quot;mini&quot;. axis.labels: The label of each column in your data is plotted around the radar. axis.label.offset: The offset of axis labels. axis.label.size: The size of axis labels. axis.line.colour: The color of axis labels. grid.min, grid.max: Rescale your values in this range. centre.y: The radius of inner circle. label.centre.y: TRUE prints the central value. FALSE turns it off. grid.line.width: The width of grid lines. grid.line.trend: &quot;classic&quot; sets equal width of the grid lines. &quot;increase&quot; sets an outward-increasing width of the grid lines. &quot;decrease&quot; sets an outward-decreasing width of the grid lines. gridline.min.linetype, gridline.mid.linetype, gridline.max.linetype: Set the grid line type for the inner, middle and outer circles. The default setting is &quot;longdash&quot;. gridline.min.colour, gridline.mid.colour, gridline.max.colour: Set the colors for the inner, middle and outer circles. The default settings are &quot;grey&quot;, &quot;#007A87&quot;, &quot;grey&quot;. grid.label.size: Set the size of grid labels. gridline.label.offset: The offset of grid labels. gridline.label: The default setting is the percentage. Replace it with your labels. group.line.width: The width of group lines. group.point.size: The size of the point in each axis for group lines. group.colours: Set colors for the group lines. polygonfill: Turn on/off the polygon fill. polygonfill.transparency: The transparency of polygon fills. group.fill.colours: The colors of polygon fills. background.circle.colour: The background color for the radar. background.circle.transparency: The transparency of the background. radarshape: &quot;round&quot; gives you a round radar. &quot;sharp&quot; gives you a sharp radar. multiplots: Turn on/off multi-plotting function. If on, data$facet1 column should be included in your data. TRUE/FALSE fullscore: Set full scores to your values. stripbackground: Turn on/off the background for the panels of multiple plots. The followings are basic settings. 1234legend.title="",plot.legend=TRUE,plot.title="",legend.text.size=14,]]></content>
      <categories>
        <category>Research</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>ggplot</tag>
        <tag>Data visualization</tag>
        <tag>ggradar2</tag>
        <tag>help document</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ggradar2: deploy a radar to your data]]></title>
    <url>%2F2018%2F12%2F05%2F2018-12-05-ggradar2%2F</url>
    <content type="text"><![CDATA[ggradar2 is now available. A large amount of features have been added to make your radar chart powerful. See ggradar2 on my Github Introductionggradar2 is a gg-function to draw a radar plot for data analysis. It is stem from ggradar but has been extended with more cool features. Install ggradar2Run the code 1devtools::install_github("xl0418/ggradar2",dependencies=TRUE) Use ggradar2Load data.12345678910library(ggradar2)data(mtcars)% Extract the group names. Otherwise, the first column will be chosen as the group names.group = row.names(mtcars)df = cbind(group,mtcars)% The radar chart is not a nice presentation if you want to compare too many groups. Thus here % we only focus on 4 groups.dftest = head(df,4)% To better distinguish two different styles, 6 groups are selected for illustration.dftest = dftest[,1:7] Default styleBy default 1ggradar2(dftest) returns No fill with round gridIf you don’t want to fill the polygon, run 1ggradar2(dftest,polygonfill = FALSE) Web typeA new web type ‘lux’ has been added by webtype. mini type 1ggradar2(dftest,webtype = 'mini') luxurious type 1ggradar2(dftest,webtype = 'lux') Gird line trendUse grid.line.trend = &#39;increase&#39; to plot an outward-increasing grid lines. 123ggradar2(dftest,style = 'sharp',webtype = 'lux', group.line.width = 0.5,grid.line.trend = 'increase',gridline.min.linetype = 'solid', gridline.max.linetype = 'solid',gridline.min.colour = 'black',gridline.max.colour='black') Full scoreUse fullscore = c(...) to set the full score to each variable. 12fullscore &lt;- c(100,10,300,150,10,10)a &lt;- ggradar2(dftest,fullscore = fullscore) Sharp gridA new style has been added. Call out the straight line style by running 1ggradar2(dftest,style = 'sharp') Sharp grid without fillGet rid of the fill 1ggradar2(dftest,style = 'sharp',polygonfill = FALSE) Removing the legend12ggradar2(dftest,style = 'sharp',polygonfill = FALSE,plot.legend = FALSE) Multiple plots by subgroups1234567# Extract 3 brands of cars out of the data framefacettest &lt;- df[c(1,2,4,5,8:14),]# Set the subgroup namesfacet1 &lt;- mapply(rep,c('Mazda','Hornet','Merc'),c(2,2,7))facet1 &lt;- Reduce(c,facet1)facettest &lt;- cbind(facettest,facet1)ggradar2(facettest,multiplots = TRUE) Notice that the column name for the subgroups should be ‘facet1’. Otherwise, ggradar2 could not recognize it.]]></content>
      <categories>
        <category>Research</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>ggplot</tag>
        <tag>Data visualization</tag>
        <tag>ggradar2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Draw the evolution of SMC]]></title>
    <url>%2F2018%2F11%2F30%2F2018-11-30-SMCplots%2F</url>
    <content type="text"><![CDATA[Finally, I have done visualizing the evolution of SMC in R instead of in Python. Here, two illustrations are given. Generating DataThe data I used in the examples is generated under SMC applying on our trait-population coevolution model, a likelihood-free method for searching the true parameters when an analytic formulation of the model is not achievable. The data is consisting of 30 iterations samples. Each iteration contains 10K sampling. The algorithm starts by sampling from a uniform distribution in range (0,1). After each iteration, it computes the fitness of the samples and highly weights the samples with high fitness. Finally, it converges to the true value. Mountain plotMountain plot is recently highly evaluated theme for comparing the distribution of data across groups. The idea is stem from Henrik’s plot Forget to point out the true value is at 1.0, which is the peak of the distribution at 30th iteration. Heatmap plotAnother nice presentation is realized by the heat map. The data was manipulated a bit. The density of the samples was converted to the frequency in tiny cells. Then it can be applied by the heat map plot. Which one do you prefer?]]></content>
      <categories>
        <category>Research</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>ggplot</tag>
        <tag>SMC</tag>
        <tag>Data visualization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Color your plots]]></title>
    <url>%2F2018%2F11%2F29%2F2018-11-29-color%2F</url>
    <content type="text"><![CDATA[Share a fancy website that shows “Curated color palette”. GallerySome examples by using different colors. Plots are generated by ggtree. And the data is simulated under our model. Coming soon! PS: which one do you prefer? Or all suck…]]></content>
      <categories>
        <category>Research</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>color</tag>
        <tag>python</tag>
        <tag>plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R update all packages]]></title>
    <url>%2F2018%2F11%2F26%2F2018-11-26-Rupdateallpackages%2F</url>
    <content type="text"><![CDATA[A convenient way to update all the packages. Update all the packagesVery simple, just one piece of code as follows: 1update.packages(ask = FALSE, dependencies = c('Suggests')) Run it in the console of your Rstudio and take a cup of coffee.]]></content>
      <categories>
        <category>Research</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>update</tag>
        <tag>packages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sed:a fast way to extract information from data]]></title>
    <url>%2F2018%2F10%2F29%2F2018-10-29-sed%2F</url>
    <content type="text"><![CDATA[In the third project, a bunch of mega data are generated for analysis. However, the simulated data is too big to load in memory. Thus, I need to extract some information without opening the file. After quite a few time searching for a solution, sed command in bash language helps me out. More details about the third project is coming soon. The data fileThe data file contains a few snapshot on different time points. Some matrix are written in the file when a snapshot time is hit. These matrix may be very large according to the parameter settings you chose. What I want to extract is the matrix \(\boldsymbol{D}\) and \(\boldsymbol{R}\) at the end. Extracting the matrixFirstly, I extract all the \(\boldsymbol{D}\)s, for example, from the data file. The following code does the thing for me. 123456789#!/bin/bashfor j in &#123;0..4&#125;;dofor i in &#123;0..4&#125;;do sed -n '/D'&#123;'length(D)+1'&#125;' = \[\r/,/\];/p' test"$j$i".m &gt; Ds"$j$i".Rdataecho $j$i' done'donedone As you see, there are in total 25 data files I need to deal with. For each data file, sed finds the line starting with “\(D\left\{length(D)+1\right\} = [\)” till “\(];\)” from the file testji.m and writes/prints (\(p\)) all the matched lines to the file Dsji.Rdata. Replacing the last D matrixThen, I want to extract the last \(\boldsymbol{D}\) matrix. How can I do that? Use the following code: 123456789101112131415161718#!/bin/bashfor j in &#123;0..4&#125;;dofor i in &#123;0..4&#125;;do A=$(grep -c 'D'&#123;'length(D)+1'&#125;' = \[' Ds"$j$i".Rdata)B=$[$A-1]sed '/D'&#123;'length(D)+1'&#125;'/&#123;G;s/\nX\&#123;'$B'\&#125;//;tend;x;s/^/X/;x;P;d&#125;;p;d;:end;s/D'&#123;'length(D)+1'&#125;'/D/;:a;n;ba' Ds"$j$i".Rdata&gt;Dt"$j$i".Rdatased -n '/D = \[\r/,/\];/p' Dt"$j$i".Rdata &gt; D"$j$i".RdataC=$(cat D"$j$i".Rdata |wc -l)C=$[$C-2]D=$[$C+1]sed -i -e 's/D = \[/D = structure(c(/' -e 's/\];/),.Dim=c('$(echo $C)','$(echo $C)'))/' -e 's/;/ /g' -e '2,$&#123;s/ /,/g&#125;' -e '2,$&#123;s/,,/ /g&#125;' -e '3,'$(echo $D)'&#123;s/^/,/g&#125;' D"$j$i".Rdataecho $j$i' done'donedone The command grep returns the number of the replicates of the line “\(D\left\{length(D)+1\right\} = [\)”. Then sed replaces the head of the last matrix with \(D\) and extracts this matrix out and writes into file Dij.Rdata. As I want to work in R with these results, I reformat the structure of the results to fit the matrix format in R which is something like: 1D = structure(c(1,0,0,1),.Dim=c(2,2)) At last, I can directly source the file in R and read the matrix. Details in sedThe first sed in the second script is the key part in this function. 1sed '/D'&#123;'length(D)+1'&#125;'/&#123;G;s/\nX\&#123;'$B'\&#125;//;tend;x;s/^/X/;x;P;d&#125;;p;d;:end;s/D'&#123;'length(D)+1'&#125;'/D/;:a;n;ba' Ds"$j$i".Rdata&gt;Dt"$j$i".Rdata I don’t fully understand what every letter in this command means. In this answer, the author explained the idea is to store a \(X\) at each match in the holdspace, and when all the \(X\)s are there, loop till the end of file. If you know more, please comment after the post. Thanks! The third sed reformats the matrix to fit in R. -e executes several commands in one line. 1sed -i -e 's/D = \[/D = structure(c(/' -e 's/\];/),.Dim=c('$(echo $C)','$(echo $C)'))/' -e 's/;/ /g' -e '2,$&#123;s/ /,/g&#125;' -e '2,$&#123;s/,,/ /g&#125;' -e '3,'$(echo $D)'&#123;s/^/,/g&#125;' D"$j$i".Rdata The first segment replaces “\(D = [ \)” with “\(D = structure(c( \)” while the second replaces the tail “];” with “(,.Dim=c(row,col)”. Then all the signs “;” are substituted by space and the single space is substituted by “,” from the second line to the end. Following the replacement of “,,” by space from the second line and add “,” at the beginning of the line from the third line on. Finally, a standard matrix form is rebuilt. BTW, a good tutorial can be found here. Have fun!]]></content>
      <categories>
        <category>Research</category>
      </categories>
      <tags>
        <tag>project 3</tag>
        <tag>bash</tag>
        <tag>mega data</tag>
        <tag>extract information</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Partial least square regression]]></title>
    <url>%2F2018%2F10%2F24%2F2018-10-25-PLSR%2F</url>
    <content type="text"><![CDATA[I found the following interpretation to partial least square regression is much better than mine. So I cited it as below: “Partial least squares regression (PLS regression) is a statistical method that bears some relation to principal components regression; instead of finding hyperplanes of maximum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the observable variables to a new space. Because both the X and Y data are projected to new spaces, the PLS family of methods are known as bilinear factor models. Partial least squares Discriminant Analysis (PLS-DA) is a variant used when the Y is categorical.” —from Wikipedia PLS algorithmFor record purpose, I am deriving the PLS algorithm in this post. Consider a linear regression problem in a vector format as in Eq.[eq:1] given an observed data, which is denoted by a vector \(\boldsymbol{x}^{T}=(x_{1},x_{2},\cdots,x_{n})\) $$\begin{align}y &amp;=b_{1}x_{1}+\cdots+b_{n}x_{n}+e \\ &amp;=\boldsymbol{x}^{T}\boldsymbol{b}+e \tag{1}\end{align}$$ We are aiming to work out the unknown regression coefficients vector \(\boldsymbol{b}\) for future prediction of \(y\) once we have the observed data \(\boldsymbol{x}\). To keep the prediction as accurate as possible, we will want to make the residual error e as small as possible. This is simply the whole idea of the linear regression method. So one solution is to use training samples, that is given m observations \(\boldsymbol{y}^{T}=(y_{1},\cdots,y_{m}),X=[x_{ij}],i=1,\cdots,m,j=1,\cdots,n \)to solve \(\boldsymbol{b}\) out in terms of keeping the residual error sufficiently small $$\begin{pmatrix}y_{1}\\y_{2}\\\vdots\\y_{m}\end{pmatrix}=\begin{pmatrix}x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1n}\\x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2n}\\\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\x_{m1} &amp; x_{m2} &amp; \cdots &amp; x_{mn}\end{pmatrix}\begin{pmatrix}b_{1}\\b_{2}\\\vdots\\b_{n}\end{pmatrix}+\begin{pmatrix}e_{1}\\e_{2}\\\vdots\\e_{m}\end{pmatrix}\tag{2}$$ For comparability among different data, all the observations are assumed to be centered and normalized in advance. Now the problem is translated to an optimization problem, that is to find a least square solution for the regression coefficients such that the sum of the square errors of the residuals is minimized. For simplicity, I would like to use the matrix form for Eq.[2] $$Y=X\boldsymbol{b}+\boldsymbol{e}. \tag{3}$$ We want to find out \(\boldsymbol{b}\) such that $$min\{\boldsymbol{e}^{T}\boldsymbol{e}\}.$$ The least square solution for \(\boldsymbol{b}\) is given by $$\boldsymbol{b}=(X^{T}X)^{-1}X^{T}Y. \tag{4}$$ The derivation is simply as follows: $$F_{\boldsymbol{b}}=\frac{1}{2}(X\boldsymbol{b}-Y)^{T}(X\boldsymbol{b}-Y) \tag{5}$$ So the derivative of F_{\boldsymbol{b}} yields $$\begin{align}\nabla_{\boldsymbol{b}}F_{\boldsymbol{b}}&amp;=\frac{1}{2}\nabla_{\boldsymbol{b}}(X\boldsymbol{b}-Y)^{T}(X\boldsymbol{b}-Y)\\ &amp;=\frac{1}{2}\nabla_{\boldsymbol{b}}(\boldsymbol{b}^{T}X^{T}-Y^{T})(X\boldsymbol{b}-Y)\\ &amp;=\frac{1}{2}\nabla_{\boldsymbol{b}}(\boldsymbol{b}^{T}X^{T}X\boldsymbol{b}-\boldsymbol{b}^{T}X^{T}Y-Y^{T}X\boldsymbol{b}+Y^{T}Y)\\ &amp;=\frac{1}{2}\nabla_{\boldsymbol{b}}tr(\boldsymbol{b}^{T}X^{T}X\boldsymbol{b}-\boldsymbol{b}^{T}X^{T}Y-Y^{T}X\boldsymbol{b}+Y^{T}Y)\\ &amp;=\frac{1}{2}\nabla_{\boldsymbol{b}}\left(tr(\boldsymbol{b}^{T}X^{T}X\boldsymbol{b})-2tr(Y^{T}X\boldsymbol{b})+tr(Y^{T}Y)\right)\\ &amp;=X^{T}X\boldsymbol{b}-X^{T}Y.\end{align}$$ By setting the derivative to be 0, solution Eq.[4] is obtained.However, this least square solution may have problem when the training sample is not enough, that is m&lt;n. In that case, the matrix \(X^{T}X\) doesn’t have full rank which means it is nonsingular. To solve this problem, we can project each measurement \(\boldsymbol{x_{i}}=(x_{i1},\cdots,x_{in})\) into a lower-dimensional subspace spanned by data \(T=XW\). We can think of this as forming a smaller set of features, each being the linear combination of the original set of features. These new features are also called “latent” variables. Therefore, the linear regression can be written as a linear regression system on the new latent variables $$\begin{align}Y &amp;=TQ^{T}+F \tag{6} \\X &amp;=TP^{T}+E \tag{7}\end{align}$$ where \(P,Q\) are the coefficient matrices and E,F are matrices of errors. As in partial least squares regression (PLSR), the weight matrix \(W\) reflects the covariance structure between the predictor and response variables. Hence, maximizing the covariance of the latent variables and the response variables \(Cov(T,Y)\) gives us the weight matrix \(W\). Once obtaining \(W\) and then constructing \(T\), \(Q^{T}\) is solved by the least squares solution Eq.[6]: $$Q^{T}=(T^{T}T)^{-1}T^{T}Y.$$ Plug Eq.[eq:latent] into the regression equation Eq.[6], we obtain the solution of the matrix \(\boldsymbol{b}\) of the coefficient in the model Eq.[3] $$\boldsymbol{b}=W(T^{T}T)^{-1}T^{T}Y.$$ So this is the whole idea of PLS. A commonly used algorithm to compute PLSR is the nonlinear iterative partial least square (NIPALS) method. The steps are summarized as follows: NIPALS Normalize \(m\) training samples by $$\begin{align}x_{ij} &amp;=\left(x_{ij}-\bar{x} _{\cdot\ j}\right)/ \sigma _{x_{\cdot\ j}} \\y_{i} &amp;=\left(y_{i}-\bar{y}\right)/ \sigma _{y}\ \text{for }i=1,\cdots,m,j=1,\cdots,n\end{align}$$ Compute the scores Compute the dominant eigenvector of \(X_{0}^{T}Y_{0}Y_{0}^{T}X_{0}\) and assign it to \(w_{1}\) (the step of maximizing the covariance). Normalize \(w_{1}\). The scores \(t_{1}\) of \(X_{0}\) yields $$t_{1}=X_{0}w_{1}.$$ Do the same to \(Y_{0}\) and obtain a normalized vector \(c_{1}\). The scores \(v_{1}\) of \(Y_{0}\) yields $$v_{1}=Y_{0}c_{1}.$$ Compute the loadings Based on the regression equations $$\begin{align}X &amp;=t_{1}p_{1}^{T}+E \\Y &amp;=t_{1}q_{1}^{T}+F,\end{align}$$ the least square method gives us $$\begin{align}p_{1} &amp;=\frac{X^{T}t_{1}}{t_{1}t_{1}^{T}} \\q_{1} &amp;=\frac{Y^{T}t_{1}}{t_{1}t_{1}^{T}}.\end{align}$$ Check if \(F\) is sufficiently small. Otherwise, set \(E\) as \(X_{1}\), \(F\) as \(Y_{1}\). Repeat step 2-3. Finally, we obtain $$\begin{align}X &amp;=t_{1}p_{1}^{T}+t_{2}p_{2}^{T}+\cdots+t_{n}p_{n}^{T}+E \\Y &amp;=t_{1}q_{1}^{T}+t_{2}q_{2}^{T}+\cdots+t_{n}q_{n}^{T}+F\end{align}$$ which in a matrix format is as follows: $$\begin{align}X &amp;=TP^{T}+E \\Y &amp;=TQ^{T}+F \\ &amp;=XWQ^{T}+F \\ &amp;:=XB+F.\end{align}$$ So, when we have a new data \(x_{new}\), to define the PLS components in our ABC-MCMC algorithm, we only need to compute $$t_{1}=x_{new}^{T}p_{1},t_{2}=x_{new}^{T}p_{2},\cdots,t_{n}=x_{new}^{T}p_{n}$$]]></content>
      <categories>
        <category>Research</category>
      </categories>
      <tags>
        <tag>regression</tag>
        <tag>least square method</tag>
        <tag>partial least square</tag>
        <tag>math</tag>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deriving the Fokker-Planck equation from a stochastic differential equation]]></title>
    <url>%2F2018%2F10%2F24%2F2018-10-24-derivingFPequ%2F</url>
    <content type="text"><![CDATA[Writing a periodic progress report is a good way to record the process that you gain knowledge. Reviewing my progress reports in the last 3 years, it recovers my memory about the knowledge and technologies that I almost forgot as I rarely use them. From this post on, I would like to give them a review and share on the blog. Hope it can help people who are interest in and one can help me if I am wrong at somewhere. DerivationHere I briefly present the derivation for the Fokker-Planck equation from a stochastic differential equation. Given the stochastic process $$dx=a(x,t)dt+b(x,t)dW_{t}$$ where \(W_{t}\) is a Wiener process. By Ito lemma, for any twice-differentiable scalar function \(f(x)\) we have $$df(x)=\left(a(x,t)f’(x)+\frac{1}{2}b^{2}(x,t)f’’(x)\right)dt+b(x,t)f’(x)dW_{t}$$ The expectation of \(f(x,t)\) yields $$E(f(x))=\int f(x)p(x,t)dx$$ and take the derivative $$\frac{dE(f(x))}{dt}=\frac{d\int f(x)p(x,t)dx}{dt}=\int f(x)\frac{\partial p(x,t)}{\partial t}dx \tag{1}$$ Also, we could plug Eq.[1] in the expectation of \(f(x)\) and take the derivative yields $$\frac{dE(f(x))}{dt}=\frac{E(df(x))}{dt}=E\left(a(x,t)f’(x)+\frac{1}{2}b^{2}(x,t)f’’(x)\right) \tag{2}$$ From that Eq.[1] and Eq.[2] are identical, we have $$\begin{align}\int f(x)\frac{\partial p(x,t)}{\partial t}dx &amp;=\int\left(a(x,t)f’(x)+\frac{1}{2}b^{2}(x,t)f’’(x)\right)p(x,t)dx \\ &amp; =\int a(x,t)f’(x)p(x,t)dx+\frac{1}{2}\int b^{2}(x,t)f’’(x)p(x,t)dx \\ &amp; =-\int f(x)\frac{\partial a(x,t)p(x,t)}{\partial x}dx+\frac{1}{2}\int f(x)\frac{\partial^{2}b^{2}(x,t)p(x,t)}{\partial x^{2}}dx \\ &amp; =\int f(x)\left(-\frac{\partial a(x,t)p(x,t)}{\partial x}+\frac{1}{2}\frac{\partial^{2}b^{2}(x,t)p(x,t)}{\partial x^{2}}\right)dx\end{align}$$ As \(f(x)\) is arbitrary, we obtain the Fokker-Planck equation in one dimension $$\frac{\partial p(x,t)}{\partial t}=-\frac{\partial a(x,t)p(x,t)}{\partial x}+\frac{1}{2}\frac{\partial^{2}b^{2}(x,t)p(x,t)}{\partial x^{2}}$$]]></content>
      <categories>
        <category>Research</category>
      </categories>
      <tags>
        <tag>mathematical modeling</tag>
        <tag>math</tag>
        <tag>the Fokker-Planck equation</tag>
        <tag>stochastic differential equation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pruneL function]]></title>
    <url>%2F2018%2F10%2F24%2F2018-10-24-pruneLfunction%2F</url>
    <content type="text"><![CDATA[I guess this function is specially useful to our group in which we play with L table. But might be less useful than phylo2L function :-) Following last post, this function pruneL prunes an L table by removing the extinct lineages. pruneL function can be found here if you want to improve the function. There is a huge room for improvement. This version is only the first draft of the function. And I guess I should remove the argument dropextinct. It doesn’t make any sense. What do you think?]]></content>
      <categories>
        <category>Research</category>
      </categories>
      <tags>
        <tag>phylogeny</tag>
        <tag>phylo class</tag>
        <tag>L table</tag>
        <tag>DDD package</tag>
        <tag>pruneL</tag>
        <tag>phylo2L</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[phylo2L function]]></title>
    <url>%2F2018%2F10%2F24%2F2018-10-24-phylo2Lfunction%2F</url>
    <content type="text"><![CDATA[I guess this function is specially useful to our group in which we play with L table. L table is an alternative way to a phylo class for phylogenetic information storage. The function L2phylo has been implemented in the DDD package that converts an L table to a phylo class. This function phylo2L does the conversion the other way around. Thus, if you want to apply your model to an empirical data. This may be useful to you. phylo2L function can be found here if you want to improve the function. I have verified it by examining if L=phylo2L(L2phylo(L)) Notice that phylo2L doesn’t have the argument dropextinct as what L2phylo has. Because to my perspective L table should be consistent with the given phylo class. But if you have a full tree on hand and want to prune it, you can do it like this prune_phylo = L2phylo(phylo2L(full tree), dropextinct = TRUE) prune_L = phylo2L(prune_phylo) Or you can use pruneL function that I have developed to prune an L table. More details on pruneL function can be found in this post.]]></content>
      <categories>
        <category>Research</category>
      </categories>
      <tags>
        <tag>phylogeny</tag>
        <tag>phylo class</tag>
        <tag>L table</tag>
        <tag>DDD package</tag>
        <tag>pruneL</tag>
        <tag>phylo2L</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Blogging tech]]></title>
    <url>%2F2018%2F10%2F23%2F2018-10-23-Blogging%20tech%2F</url>
    <content type="text"><![CDATA[I am using hexo for blogging. The blog website is built on my personal Github Page. The theme of the blog is next which is a powerful tool to customize the blog. Here I would like to share/archive some useful tech for future use. Optimizing the themeTwo posts are recommended here on zhihu and CSDN. They almost contain all the custom adjustments for the theme. Of course, the official website next is a good choice as well.Advance tech to make your blog prettier is on this website. Huge amount of tips and tricks are shown. MaterialsTo look for some materials like pictures, icons, you can find a lot form easyicon and fontawesome for free.]]></content>
      <categories>
        <category>Blogging</category>
      </categories>
      <tags>
        <tag>Tech</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PhD Project 1-Detecting local diversity-dependence in diversification]]></title>
    <url>%2F2018%2F10%2F22%2F2018-10-22-IntrotoPro1%2F</url>
    <content type="text"><![CDATA[Welcome to Liang&apos;s blog! This is my first PhD project since 2015 joining Etienne&apos;s lab. You will find a brief introduction to this project. The code for the model can be found on my GitHub. This my first formal research project starting from December of 2015 to February 2018 when the paper gets published. It seems to take quite a long time. Because I spent some time to switch from mathematics to evolutionary science and writing paper costs time as well. Anyway, to commemorate the first paper. AbstractWhether there are ecological limits to species diversification is a hotly debated topic. Molecular phylogenies show slowdowns in lineage accumulation, suggesting that speciation rates decline with increasing diversity. A maximum likelihood method to detect diversity-dependent diversification from phylogenetic branching times exists, but it assumes that diversity-dependence is a global phenomenon and therefore ignores that the underlying species interactions are mostly local, and not all species in the phylogeny co-occur locally. Here, we explore whether this maximum likelihood method based on the non-spatial diversity-dependence model can detect local diversity-dependence, by applying it to phylogenies, simulated with a spatial stochastic model of local-diversity-dependent speciation, extinction and dispersal between two local communities. We find that type I errors (falsely detecting diversity-dependence) are low, and the power to detect diversity-dependence is high when dispersal rates are not too low. Interestingly, when dispersal is high the power to detect diversity-dependence is even higher than in the non-spatial model. Moreover, estimates of intrinsic speciation rate, extinction rate and ecological limit strongly depend on dispersal rate. We conclude that the non-spatial diversity-dependent approach can be used to detect diversity-dependence in clades of species that live in not too disconnected areas, but parameter estimates must be interpreted cautiously. ModelWe have developed a spatial diversity-dependent diversification model to explore if the global version of the diversity-dependence approach could detect the diversity-dependent signal on the spatial scenario. Generating treesThe idea is simple. For simplicity, we build a two-location model and let species evolve (speciate: give birth to a new species/ extinction: one species goes extinct) in the regime. A pack of generated trees under different speciation rates and extinction rates and dispersal rates is like this (Scenario 2, see details in our paper): Applying the bootstrapping analysisWe exploit the bootstrapping analysis to examine the power of the global approah to detect the signal of diversity-dependence on the spatial structered trees. The following tables show the power of the global tool for signal detection under different parameter combinations and scenarios. Parameter inferenceAs due to the complexity of the model, an analytical likelihood function is not possible to achieve. Therefore, we test if the global likelihood approach can recover the parameters on the spatial model or show some patterns. ConclusionFrom the results above, the conclusion is clear. Please see our paper:-) ReferenceXu, L., &amp; Etienne, R. S. (2018). Detecting local diversity-dependence in diversification. Evolution, 72(6), 1294-1305. DOI: 10.1111/evo.13482 Etienne, R. S., Pigot, A. L., &amp; Phillimore, A. B. (2016). How reliably can we infer diversity-dependent diversification from phylogenies? Methods in ecology and evolution, 7(9), 1092-1099. DOI: 10.1111/2041-210X.12565 Etienne, R. S., Haegeman, B., Stadler, T., Aze, T., Pearson, P. N., Purvis, A., &amp; Phillimore, A. B. (2012). Diversity-dependence brings molecular phylogenies closer to agreement with the fossil record. Proceedings of the Royal Society of London. Series B, Biological Sciences, 279(1732), 1300-1309. DOI: 10.1098/rspb.2011.1439]]></content>
      <categories>
        <category>Research</category>
      </categories>
      <tags>
        <tag>evolution</tag>
        <tag>mathematical modeling</tag>
        <tag>diversity-dependence</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F10%2F22%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Liang&apos;s blog! This site is my first blog for sharing my work experience plus a little bit life. CategoriesResearchThis category mainly contains my PhD projects so far. More details can be found on my Research Gate. BB lifeSome interesting stuff about life. BB, in Chinese, means talk about, chat. WorkNot on work yet, although 3 years ago I was a lecturer in China. BloggingInteresting technologies about blogging. At last, enjoy the blog. Hope you can find something useful. Or at least have fun :-)]]></content>
      <categories>
        <category>BB life</category>
      </categories>
      <tags>
        <tag>info</tag>
      </tags>
  </entry>
</search>
